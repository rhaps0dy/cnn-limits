Hello everyone, I'm Adria and today I'll present our paper, Correlated weights in infinite limits of deep convolutional nerual netowrks.
My collaborator is Mark van der Wilk, you can see him on the left.
--
Take a Bayesian nerual netowrk of any architecture, with an independent, zero-mean prior over the weighnts. We now know that, if you take the limit of its width to infinity, it converges in distribution to a corresponding Gaussian process, with a mean and covariance functions that depend on the architecture of the network.

This was first noted by Radford Neal back in 1996, and it is what inspired the paper that made Gaussian processes popular in the machine learning community.
--
The great advantage of a Gaussian process model is that, for regression, its posterior can be written in closed form, so it's much easier to do inference with it than with a Bayesian neural network.

However, Gaussian processes have one conceptual problem:
they do not learn features based on the data, because the kernel function completely determines the nonlinear features they use. This is in contrast to neural networks, which we believe work by extracting good features from the data.

In the late 90s, it seemed that Gaussian processes were taking over: they had much better performance that the multi-layer perceptrons of the time. And still, David MacKay wondered:
--
Can  a Gaussian process, a simple smoothing machine, really replace a neural network?

When taking the infinite limit, we lose the correlations between the hidden units of a Bayesian neural network. Have we lost something that was crucial?
--
Have we thrown the baby out with the bathwater? We still do not have a definitive answer. Neural networks and gaussian processes each do well in the large and small data regimes, respectively. This suggests that NNs can learn useful things from the extra data, by learning representations; but it's not clear that GPs cannot do that.  
--
In this paper, we take a look at what we lose when we take the infinite-width limit of a Bayesian convolutional neural network with an independent prior over the weights.


At each layer, a Bayesian CNN applies the same random function to each patch of the image. Thus, its activations at different spatial points are correlated.

However, its corresponding GP limit is *also* the GP limit for a neural network that applies a *different* random function to each patch. This is called a locally connected network, and that we get it was noted by Novak et al. The resulting Gaussian process has no spatial correlations.
--
Can we avoid throwing out the spatial correlations baby with the finite neural net bathwater?
--
The answer is yes: we can do it by adding spatial correlations in the prior over the weights of the CNN. Note that we do not add correlations between channels, so we still have an infinite-width GP limit, and one that does not throw away the spatial correlation between the activations.

If a neural network layer has a D-dimensional convolution of weights, the way to calculate the corresponding GP kernel is to perform a 2D-dimensional convolution of covariance tensors. That is, if the neural network layer has a 2d convolution, we calculate its kernel by doing 4d convolutions.

This is considerably more expensive, but cannot be avoided if we want to have weight correlations. The algorithm presented in our paper has as special cases faster algorithms that operate only with independent weights.
--
We also generalize priors. Existing work uses independent weights, with identity covariance, and mean pooling,  which is equivalent to weights with all-ones covariance. We are able to interpolate between them by putting a stationary kernel on the pixel positions of the convolutional filter. This way we encode the inductive bias that the convolution filters are smooth.

Our formulation also includes other GPs with convolutional structure from the previous literature.
--
Finally, we have some empirical results

--

for CIFAR-10 classification with the correlated-weight kernels. We specify the correlation structure of the last layer using a Matern kernel with some lengthscale. The other layers have independent weights.

For each data-set size, which is increasing with the lines of darker color, we show the performance of each lengthscale from zero, i.e. independent weights, to infinity, i.e. mean-pooling. We can see that the optimum, the asterisk, is in the middle, and that it is the same for all the larger data sets. This supports the hypothesis that an intermediate amount of smoothness in the weights can be better than independence or averaging.

The optimal lengthscale is the same for both NN architectures, so we speculate that it reflects an architecture-independent property of the data set.

--

We also have an experiment where we add spatial covariance to intermediate layers of the network. We take the Myrtle10 architecture by Shankar et al. and replace the mean-pooling and neighbouring convolution layers with a single convolution layer. Then, we vary the amount of correlation in that layer. Here the correlated-weights GP does not exceed the performance of the hand-tuned architecture, but is able to match it.

--

Alright. Here are the things you should remember from this talk.

--
Thank you for listening. Check out the paper and the references if you're interested.  Thank you for your time and attention.

start at 0:17