@InProceedings{markvdw2017convolutional,
  author    = {van der Wilk, Mark and Rasmussen, Carl Edward and Hensman, James},
  title     = {Convolutional Gaussian Processes},
  booktitle = {{Advances} in {Neural} {Information} {Processing} {Systems}},
  year      = {2017},
  pages     = {2845--2854},
}

@article{dutordoir2019tick,
  author =		 {Dutordoir, Vincent and Wilk, Mark van der and Artemev, Artem
                  and Tomczak, Marcin and Hensman, James},
  title =		 {Translation Insensitivity for Deep Convolutional Gaussian
                  Processes},
  journal =		 {CoRR},
  year =		 2019,
  url =			 {http://arxiv.org/abs/1902.05888v1},
  archivePrefix ={arXiv},
  eprint =		 {1902.05888},
  primaryClass = {stat.ML},
}

@misc{li2019enhanced,
Author = {Zhiyuan Li and Ruosong Wang and Dingli Yu and Simon S. Du and Wei Hu and Ruslan Salakhutdinov and Sanjeev Arora},
Title = {Enhanced Convolutional Neural Tangent Kernels},
Year = {2019},
Eprint = {arXiv:1911.00809},
}

@incollection{arora2019exact,
title = {On Exact Computation with an Infinitely Wide Neural Net},
author = {Arora, Sanjeev and Du, Simon S and Hu, Wei and Li, Zhiyuan and Salakhutdinov, Russ R and Wang, Ruosong},
booktitle = {Advances in Neural Information Processing Systems 32},
editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
pages = {8141--8150},
year = {2019},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/9025-on-exact-computation-with-an-infinitely-wide-neural-net.pdf}
}

@misc{shankar2020,
Author = {Vaishaal Shankar and Alex Fang and Wenshuo Guo and Sara Fridovich-Keil and Ludwig Schmidt and Jonathan Ragan-Kelley and Benjamin Recht},
Title = {Neural Kernels Without Tangents},
Year = {2020},
Eprint = {arXiv:2003.02237},
}

@article{springenberg14allconv,
  author =       {Springenberg, Jost Tobias and Dosovitskiy, Alexey and Brox,
                  Thomas and Riedmiller, Martin},
  title =        {Striving for Simplicity: the All Convolutional Net},
  journal =      {CoRR},
  year =         2014,
  url =          {http://arxiv.org/abs/1412.6806v3},
  archivePrefix ={arXiv},
  eprint =       {1412.6806},
  primaryClass = {cs.LG},
}
@article{tan19efficientnet,
  author =       {Tan, Mingxing and Le, Quoc V.},
  title =        {Efficientnet: Rethinking Model Scaling for Convolutional
                  Neural Networks},
  journal =      {CoRR},
  year =         2019,
  url =          {http://arxiv.org/abs/1905.11946v3},
  archivePrefix ={arXiv},
  eprint =       {1905.11946},
  primaryClass = {cs.LG},
}

@article{wang19_kervol_neural_networ,
  author =       {Wang, Chen and Yang, Jianfei and Xie, Lihua and Yuan, Junsong},
  title =        {Kervolutional Neural Networks},
  journal =      {CoRR},
  year =         2019,
  url =          {http://arxiv.org/abs/1904.03955v2},
  abstract =     {Convolutional neural networks (CNNs) have enabled the
                  state-of-the-art performance in many computer vision tasks.
                  However, little effort has been devoted to establishing
                  convolution in non-linear space. Existing works mainly
                  leverage on the activation layers, which can only provide
                  point-wise non-linearity. To solve this problem, a new
                  operation, kervolution (kernel convolution), is introduced to
                  approximate complex behaviors of human perception systems
                  leveraging on the kernel trick. It generalizes convolution,
                  enhances the model capacity, and captures higher order
                  interactions of features, via patch-wise kernel functions, but
                  without introducing additional parameters. Extensive
                  experiments show that kervolutional neural networks (KNN)
                  achieve higher accuracy and faster convergence than baseline
                  CNN.},
  archivePrefix ={arXiv},
  eprint =       {1904.03955},
  primaryClass = {cs.CV},
}

@inproceedings{cui2017kernel,
  title={Kernel pooling for convolutional neural networks},
  author={Cui, Yin and Zhou, Feng and Wang, Jiang and Liu, Xiao and Lin, Yuanqing and Belongie, Serge},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={2921--2930},
  year={2017}
}
@inproceedings{yang2015deep,
  title={Deep fried convnets},
  author={Yang, Zichao and Moczulski, Marcin and Denil, Misha and de Freitas, Nando and Smola, Alex and Song, Le and Wang, Ziyu},
  booktitle={Proceedings of the IEEE International Conference on Computer Vision},
  pages={1476--1483},
  year={2015}
}

@InProceedings{he2016deep,
  author    = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  title     = {Deep residual learning for image recognition},
  booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},
  year      = {2016},
  pages     = {770--778},
  file      = {:papers/NNs/kaiming_he_resnet_paper.pdf:PDF},
  url       = {https://arxiv.org/pdf/1512.03385.pdf},
}
@article{belkin18reconciling,
  author =		 {Belkin, Mikhail and Hsu, Daniel and Ma, Siyuan and Mandal,
                  Soumik},
  title =		 {Reconciling Modern Machine Learning Practice and the
                  Bias-Variance Trade-Off},
  File = 		 {:papers/theory/belkin18reconciling.pdf:PDF},
  journal =		 {CoRR},
  year =		 2018,
  url =			 {http://arxiv.org/abs/1812.11118v2},
  archivePrefix ={arXiv},
  eprint =		 {1812.11118},
  primaryClass = {stat.ML},
}
@article{li19ecntk,
  author =		 {Li, Zhiyuan and Wang, Ruosong and Yu, Dingli and Du, Simon S.
                  and Hu, Wei and Salakhutdinov, Ruslan and Arora, Sanjeev},
  title =		 {Enhanced Convolutional Neural Tangent Kernels},
  journal =		 {CoRR},
  year =		 2019,
  url =			 {http://arxiv.org/abs/1911.00809v1},
  archivePrefix ={arXiv},
  eprint =		 {1911.00809},
  primaryClass = {cs.LG},
}