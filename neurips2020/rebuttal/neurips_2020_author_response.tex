\documentclass{article}

\usepackage{neurips_2020_author_response}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage[colorlinks=true, allcolors=blue]{hyperref}
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage[colorinlistoftodos]{todonotes}

\newcommand{\markcomment}[1]{\todo[color=green]{#1}}
\newcommand{\adriacomment}[1]{\todo[color=blue]{#1}}

\usepackage{lipsum} 
\usepackage{../MarkBiblatexCmds}

\begin{document}
Thank you all for your thorough reviews. Regardless of the outcome, incorporating them makes this paper stronger.

Infinitely wide limits of neural networks are investigated to determine, for example, what properties can be obtained in a shallow and wide architecture, and what properties are uniquely provided by depth.
In this work, we investigate the disappearance of patch-induced correlations in the limit of infinitely wide CNNs.
We contribute by:
\vspace{-0.27cm}
\begin{enumerate}
    \item Showing that the loss of patch-induced correlations is \emph{not} an essential consequence of the infinite limit, and that architectural changes (e.g.~pooling) are not needed to maintain the correlations. Changing the prior suffices.
    \vspace{-0.15cm}  %%% VSPAAAAACE
    \item Empirically demonstrating that patch-correlations are important, and that the performance of pooling can be matched or improved by a correlated prior, in the infinite limit.
    \vspace{-0.15cm}  %%% VSPAAAAACE
    \item Connecting kernels obtained from limits of infinitely wide NNs to those studied in the GP community.
\end{enumerate}
\vspace{-1ex}
While our argument does not require novel proof techniques, we believe it does make an important contribution to the discussion surrounding infinite limits of CNNs: the disappearance of patch-induced correlations was noted as a puzzling and undesirable occurrence by Novak et al. Our work shows that simply correlating weights in the prior avoids this occurrence, and moreover, can improve performance in the infinite limit regime.
We believe that this raises questions about the usefulness of correlated weights in finite CNNs, but have opted to leave this line of inquiry for future work.
We will clarify that these are our claims, and not more; since several reviewers have pointed out that we failed to convey this properly.

% The disappearance of patch-induced correlations in the limit of infinitely wide CNNs has been seen as an example of a desirable property of a neural net that disappears when taking an infinite limit, and obtaining a Gaussian process.

% The primary aim of this paper is to show that we can re-introduce and regulate patch-induced correlations in the GP that is the infinitely wide limit of a CNN, without architectural changes. This does not require novel proof techniques, but it does contribute to the discussion surrounding infinite limits of CNNs: the disappearance of patch-induced correlations was noted as a puzzling and undesirable occurrence, by Novak et al. We aim to show that this improves the empirical properties of algorithms for image data based on the corresponding kernel. We believe that this raises questions about the usefulness of correlated weights in finite CNNs, but have opted to leave this line of inquiry for future work.
% We will clarify that these are our claims, and not more; since several reviewers have pointed out that we failed to convey this properly.

Lines 200-201 have also been harmful to clarity. The correlated weight priors, when applied anywhere in the network, are exactly as expensive as mean pooling of the same size: both are 4D convolutions with a non-sparse 4D covariance tensor. The point made there corresponds to viewing layers with \emph{independent} weights also as 4D convolutions. Since the corresponding covariance tensors have sparse (diagonal) structure, we can (and do) convolve them more efficiently.

\vspace{-1.5ex}
\paragraph{R2:}
%There are two reasons why the Myrtle kernel in our paper has lower performance than Shankar
%et al [1]. First, [1] optimize the noise hyperparameter on the test set
%(B.2,
%\href{https://proceedings.icml.cc/book/2020/file/6950aa02ae8613af620668146dd11840-Supplemental.pdf}{supplementary material}), whereas we use a validation set. 

The reason the Myrtle kernel has lower performance than in Shankar et al [1], is that when training on the small, $\le 1280$ point subsets of CIFAR-10, they use ZCA preprocessing calculated on the \emph{full}, 50k point, data set with data augmentation. This introduces information from the full data set into the classifier.
In an email dated 24 May 2020, V. Shankar acknowledged that this is an error, both in their code and their paper. We have exactly replicated their results using their preprocessed data set, and our code. None of our included experiments preprocesses the training examples.

The Figure 3 results are indeed flat. Our interpretation is that optimising the kernel lengthscale recovers (but does not improve) the performance of the hand-designed Myrtle10 (Line 261). We will add the parenthetical. % Also, see L17 of this document for a clarification of computational complexity.

You are correct that Li et al. [3]'s local average pooling corresponds to Toeplitz 4D covariance tensors (though not, when flattened, Toeplitz matrices). We will acknowledge (in L143) that this interpolates between no pooling and full pooling. We chose to induce correlations with kernels because of the connection with the literature on GPs for images.

The Matérn-3/2 kernel and RBF kernel both parameterise PSD matrices where the correlation smoothly decays with distance. The Matérn-3/2 kernel's functions are less smooth. The choice is heuristic and follows Dutordoir et al.

%The Matérn-3/2 kernel is stationary and functions drawn from it are almost surely smooth, however, the class of functions that can be sampled from it is larger. \adriacomment{What do we say here?} We will include a brief explanation.
$\Sigma_{{pp'}}$ should indeed be normalised. Thank you for pointing that out. We had caught it in the code, but will amend the paper to reflect this. The empirical results should be unaffected by this: multiplying all dimensions by a constant factor in eq.~11 does not change their relative ordering, and thus the models' predictive performance.

Thank you for your thorough review, corrections and ``nits''. We will fix them, and the colors of Figure 3. We have cleaned up and will document the code for release.
\vspace{-1.5ex}
\paragraph{R3:}
Everything in your review is technically correct. However, our paper's main contribution is not that the extension to correlated weights still induces a GP limit. Rather, it is that correlated weights preserve patch-induced correlations in the infinite limit, and this is useful as a kernel method, which we believe is nontrivial. Potentially, it is also useful for finite CNNs, but we do not investigate this. We ask you to please reconsider the paper in this light.

We believe we acknowledge that Yang did most of the work for our proof in L159 (main) and L3-4 (supplement). We will strengthen
the language there. In any case, the proof is a necessary one: readers will rightly be
skeptical of CLT-like claims when the prior over NN weights involves correlations.

\vspace{-1.5ex}
\paragraph{R8:}
We agree that this paper suggests that patch-induced correlation may be one of the drivers of CNN performance. We believe our kernel method is practical: see L17 in this document for the computational cost of correlated weights.

\vspace{-1.5ex}
\paragraph{R9:}
Our proof is straightforward, but our novelty is elsewhere (c.f. R3).
Improving the use of NN kernels as ``methods in their own right'' (Line 3) is one of our primary aims. Properly arguing that this approach is better than improving BNNs requires more empirical evidence than we have, and is a better fit for possible future work that explores the consequences of correlated weights in finite BNNs. We will point out that we take one of the two alternative paths around Line 44.

Regarding notation: $P,Q$ are taken from Arora et al., 2019, but $p,q$'s use is slightly inconsistent. Thank you for the corrections, they will be incorporated.

\end{document}
=======
\documentclass{article}

\usepackage{neurips_2020_author_response}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage[colorlinks=true, allcolors=blue]{hyperref}
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage[colorinlistoftodos]{todonotes}

\newcommand{\markcomment}[1]{\todo[color=green]{#1}}
\newcommand{\adriacomment}[1]{\todo[color=blue]{#1}}

\begin{document}



Thank you to all reviewers for your interesting comments. Regardless of the outcome, we believe our paper will be stronger afterwards.

Emphasise that the aim of the paper was to address the issue of the
disappearance of patch-induced correlations in the infinite limit, and that we
show that this can be done without architectural changes. While we acknowledge
that this does not require new proof techniques, we believe that this does
contribute to the discussion surrounding infinite limits of NNs, as the
disappearance of patch-induced correlations was noted as a puzzling and
undesirable occurrence.

L200-201, about computational complexity, have caused much confusion and will be reworked. The correlated priors, when applied anywhere in the network, are as expensive as mean pooling: both are 4D convolutions with a non-sparse 4d tensor. These lines concerned the view of layers with independent weights as 4d convolutions with a diagonal tensor: when naively applied, these are more expensive, but

> While the authors allude this study could motivate using correlated weights in finite networks no analysis or support is given. For the kernels, it is not clear with more computation required than mean pooling (as claimed by the authors), whether improvement is worth it

\paragraph{R2:}
Thank you for your thorough review. We will fix the mistake with the colours of the left-hand side of Figure 3. The color gradient is hard to match visually, especially on grayscale prints, however the relative ordering of the training set sizes makes it possible to match. We will also document the code for the public release.

There are two reasons why our Myrtle kernel has lower performance than Shankar
et al [1]. First, [1] optimize the function noise hyperparameter on the test set
(B.2,
\href{https://proceedings.icml.cc/book/2020/file/6950aa02ae8613af620668146dd11840-Supplemental.pdf}{supplementary material}), and we use a validation set instead. Second, when training on a
small data set, [1] use ZCA preprocessing for the full CIFAR-10 data set with
data augmentation. This greatly increases their effective training set size
(from 1280 to 50000, at least), and is the algorithm used in their paper to
achieve some of the effects of data augmentation on kernel methods. The code in
\href{https://github.com/modestyachts/neural_kernels_code/blob/master/preprocess.py}{preprocess.py}
indicates that the data set \verb|"cifar-10"| is the ZCA-preprocessed one, and
the one what is used for the small data experiments (c.f.
\href{https://github.com/modestyachts/neural_kernels_code/blob/master/subsampled_cifar10/run_kernel_myrtle5_subsampled.sh}{\texttt{run\_kernel\_myrtle5\_subsampled.sh}}).
In an email dated 24 May 2020, V. Shankar acknowledged this bug, but it has not
been fixed in the ICML version of the paper. None of the experiments we included
in our paper preprocess the images.

The Matérn-3/2 kernel is stationary and functions drawn from it are almost surely smooth, however, the class of functions that can be sampled from it is larger. \adriacomment{What do we say here?} We will include a brief explanation.

$\Sigma_{{pp'}}$ should be normalised. Thank you for pointing that out. We saw it in code, but the results should be unaffected by this: a constant factor for all dimensions of eq.~11 does not

\paragraph{R3:}
Everything in your review is technically correct. However, we believe you are
evaluating our paper as a deep learning theory paper, which it is not. Our
motivation is to improve the properties of kernel methods for image
classification, and not to analyze the properties of finite networks. Our concern is with the undesirable properties that independent weights induce on the wide limit kernel, and not with finite networks (which do not strictly have this property).
We believe the implications of this work to be interesting: We can obtain the similar/better performance with a different weight prior, as others get with architectural changes.
%This motivates a closer look at the assumptions that are made when taking the infinite limit, which is a novel point.

The only proof in our paper is very straightforward given the work of Yang, and
we believe we acknowledged that in L159 (main) and L3-4 (supplement). To prevent
future readers from assigning us more credit than we deserve, we will strengthen
the language there. However, the proof is a necessary one: readers will rightly be
skeptical of CLT-like claims when the prior involves correlations.


\paragraph{R8:}
We should try to get this reviewer to champion the paper.
We agree that this paper sheds light on what properties of CNNs drive their good performance. They said "significant and novel"!
Maybe point out that the computational barriers is shared with *all* other infinite limit kernels. We're not any worse. But we agree that it's mainly theoretical.

> Maybe point out that the computational barriers is shared with *all*
> other infinite limit kernels

We really should do that. Lines 200-201 have done harm.  Our kernel is
exactly as expensive as having average pooling of the same size.

Sadly this review is pretty short, I don't think R8 thought too hard
about it, so they're more likely to be convinced by the others...


\paragraph{R9:}
Agree that novelty is limited in the proof. However, the main point is that we should be challenging the assumptions that we make when taking the infinite limit, which come from finite NNs. This *is* novel.
Emphasise that weaknesses (method is slow) is shared with all other infinite limit papers. We're no slower.
We are committed to improve clarity.
We agree with this reviewer that understanding properties of NNs was a strong motivation for this work. The main motivation was the curious disappearance of patch-induced correlations, which we provide new insight to. The emphasis placed on performance is mainly there to verify that our predictions of the importance of flexible patch-induced correlations are correct. We can clarify this in the paper (I think this is a really good point)

I think we can turn this reviewer. Agreed with your points, especially
the computational copmlexity one (I think we can write it in the "for
all reviewers" section.) I disagree with this however:

> understanding properties of NNs was a strong motivation for this
> work.

I think that R9's point is the exact opposite, it is the following:
Most GP-NN work is about understanding NNs, but this one is purely
about presenting methods with better properties (performance,
uncertainty calibration). In that it is similar to Bayesian NNs, which
do not seek to understand NNs, but rather to improve upon them.

To restate the point: R9 does not see "understanding properties of NNs"
as a motivation for this work at all. I think R9 has a point, since the
fact that patch correlations matter is pretty well accepted, and we're
just trying to bring this to kernel methods. Nevertheless finding out
that they do improve things is some additional evidence for this
hypothesis, a negative result would have been very interesting too.

\end{document}
