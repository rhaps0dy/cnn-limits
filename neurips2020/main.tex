\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2020

% ready for submission
% \usepackage{neurips_2020}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2020}

% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2020}

% to avoid loading the natbib package, add option nonatbib:
\usepackage[nonatbib]{neurips_2020}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Useful packages
% \usepackage{subcaption}
\usepackage{xcolor}
\usepackage{amsmath}
\usepackage{amsthm}
\newtheorem{lemma}{Lemma}
\usepackage{graphicx}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}
\usepackage{paralist}
\usepackage{cleveref}
\creflabelformat{equation}{#2#1#3}
% \usepackage{listings}
% \lstset{
%     showstringspaces=false,
%     basicstyle=\ttfamily,
%     breaklines=true,
%     breakatwhitespace=true,
% }
% \usepackage{booktabs}
\usepackage{cancel}
\usepackage{import}  % Inkscape figures

\usepackage[colorinlistoftodos]{todonotes}
\newcommand{\markcomment}[1]{\todo[color=green]{#1}\xspace}
\newcommand{\adriacomment}[1]{\todo[color=blue]{#1}\xspace}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Bibliography
\usepackage{MarkBiblatexCmds}
\addbibresource{ref.bib}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Mathematical commands
\usepackage{MarkMathCmds}

\newcommand{\vX}{\mathbf{X}}
\newcommand{\vW}{\mathbf{W}}
\newcommand{\vZ}{\mathbf{Z}}
\newcommand{\vA}{\mathbf{A}}

\newcommand{\patchfun}[1]{{#1}\text{th patch}}
\newcommand{\vecfun}{\text{vec}\bra}
\newcommand{\simiid}{\overset{i.i.d.}{\sim}}



\title{Correlated Weights in Infinite Limits \\ of Deep Convolutional Neural Networks}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

\author{%
  Adrià Garriga-Alonso \\
  University of Cambridge \\
  \texttt{ag919@cam.ac.uk} \\
  % examples of more authors
   \And
   Mark van der Wilk \\
   Imperial College London \\
   \texttt{m.vdwilk@imperial.ac.uk} \\
  % \AND
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
}

\begin{document}

\maketitle

\begin{abstract}
Infinite width limits of deep neural networks often have tractable forms. They have been used to analyse the behaviour of finite networks, as well as being useful methods in their own right. Currently used limits of deep convolutional networks lose correlating contributions from different spatial locations in the image, unlike their finite counterparts, even when those only use convolutions. We argue that this is undesirable, and remedy it by introducing spatial correlations in the prior over weights. This leads to correlated contributions being preserved in the wide limit. Varying the amount of correlation in the convolution weights allows interpolation between independent-weight limits and mean pooling (which is equivalent to complete correlation in the weights). Empirical evaluation of the infinitely wide network shows that optimal performance is achieved between the extremes, indicating the usefulness of considering correlations in the weights.
\end{abstract}

% Thoughts
% - Number of filters / neurons is said to be related to the capacity of the neural network to learn. Well, what happens if we increase capacity to infinity?
% - We should discuss both NTK and "GP" equivalent


\section{Introduction}
% Paragraph 1
% - NN theory
% - Infinite limits are often more tractable
% - Infinite limits have given understanding, and new methods
% - However, we lose certain properties
Neural networks have been enormously successful at learning relationships through minimising some loss on a training set, starting from a random initialisation. While a theoretical explanation of the success of neural networks is still lacking, analysis of the random initialisation has proven to be useful for providing both insight %\citep[e.g.][]{neal1996bayesian,schoenholz2017deepinformationpropagation}
and practically useful methods.
%\citep{gpml}.
% \markcomment{Are there any more papers that we can mention here that analyse random inits?}
%\citet{neal1996bayesian} analysed the functions implied by randomly initialised neural networks, and noted the correspondence to Gaussian processes in the infinite limit, which led to their widespread adoption \citep{williams1996gpr,gpml}. More recently, similar analyses have shed light on successful initialisations \citep{schoenholz2017deepinformationpropagation}, and training dynamics \citep{jacot2018ntk}.
In particular, limits where the width of layers is taken to infinity have been particularly crucial, as these correspond to linear basis function models, which are well-understood mathematically.
Such a relationship was first noted by \citet{neal1996bayesian} between infinitely wide Bayesian neural networks and Gaussian processes (GPs). The GP representation greatly simplified Bayesian learning, leading to its widespread adoption \citep{williams1996gpr,gpml}.

This success of GPs raised the question of whether a mathematically simple linear model could replace a complex neural network. \citet{mackay1998introgp} noted that in taking the infinite limit, the feature representation had become fixed. Given that learnable features are a key desirable property of neural networks which is lost due to the infinite limit, MacKay inquired: ``have we thrown the baby out with the bath water?'' Recent renewed interest in correspondences for deep networks \citep{matthews2018dnnlimit,lee2018dnnlimit} show that this still is not completely clear.
% question still has not been fully answered. \adriacomment{edit to make sure "this question" is understood to be "whether NNs correspond to a linear model"}

In this work we investigate a property that, similarly, is present in finite convolutional neural networks, but which disappears in the infinite limit: correlation from patches in different parts of the image \citep{garriga2018infiniteconv,novak2019infiniteconv}. Given that convolutions were developed to introduce these correlations, it seems undesirable that adding more capacity by increasing the number of filters removes them.

Currently, changing the architecture by introducing mean-pooling is the only way that these correlations have been reintroduced \citep{novak2019infiniteconv}, and they have been shown to be important for improving performance \citep{arora2019exact}. We show that correlations between patches can also be maintained in the limit without pooling. Instead, we introduce correlations between the weights in the prior. The amount of correlation can be controlled, which allows us to interpolate between the existing approaches of full independence and mean-pooling. Our approach allows the discrete architectural choice of mean-pooling to be replaced with a more flexible continuous amount of correlation. Empirical evaluation on CIFAR-10 shows that this additional flexibility improves performance.

Our work illustrates that there are choices to be made when analysing infinite limits of neural networks, and that good choices can improve performance. The success of this approach in the infinite limit also raises questions about whether correlated weights should be used in finite networks.



% \begin{itemize}
%     \item Motivate interest in infinitely wide neural networks
%     \begin{itemize}
%         \item Understand properties of neural networks (feature space, training dynamics)
%         \item Simplify Bayesian inference (infinite width makes GP)
%         \item Should read some of the other infinite width papers for more points
%     \end{itemize}
%     \item Mention that many different NN's have a GP equivalent, including CNNs.
%     \item Introduce problem:
%     \begin{itemize}
%         \item Not long after the uncovering of infinite-width correspondences [Neal 1996], limitations were discovered. MacKay baby bathwater.
%         \item Here we argue that a similar effect can be seen in infinite limits of CNNs. Independence over the weights removes correlations over distant patches.
%     \end{itemize}
%     \item Introduce our contribution:
%     \begin{itemize}
%         \item We investigate the effect of patch correlations on the performance of infinite-width models.
%         \item We show that a different prior over weights re-introduces correlations.
%         \item We discuss how mean-pooling (does max-pooling do this too? \adriacomment{yes, but nobody uses it in wide CNNs because nobody has bothered to figure out the expression for propagating the covariance forward}) has a similar effect, explaining why it has been used in other recent papers.
%         \item We show how the correlated prior allows us to interpolate between the independent limit, and mean-pooling limits, an that often something in the middle performs best.
%     \end{itemize}
%     \item Speculate on what's next:
%     \begin{itemize}
%         \item Perhaps finite-width networks should go beyond independent initialisations?
%     \end{itemize}
% \end{itemize}

\section{Related work}
Infinitely wide limits of neural networks are currently an important tool for creating approximations and analyses. Here we provide a background on the different infinite limits that have been developed, together with a brief overview of where they have been applied.

Interest in infinite limits first started with research into properties of Bayesian priors on the weights of neural networks. \citet{neal1996bayesian} noted that prior function draws from neural networks with a single hidden layer and appropriate Gaussian priors on the weights tended towards a Gaussian process as the width tended towards infinity. The simplicity of performing Bayesian inference in Gaussian process models led to their widespread adoption soon after \citep{williams1996gpr,gpml}. Over the years, various choices for weight priors and activation functions have led to different kernels which specify the properties of the GP prior \citep{williams1997inf,cho2009mkm}.

With increasing prominence of deep learning, recursive kernels were introduced in an attempt to obtain similar properties. \citet{cho2009mkm,mairal2014ckn} investigated such methods for fully-connected and convolutional architectures respectively. Despite similarities between recursive kernels and neural networks, the derivation did not provide clear relationships, or any equivalence in a limit. \citet{hazan2015} took initial steps to showing the wide limit equivalence of a neural network beyond the single layer case. Recently, \citet{matthews2018dnnlimit,lee2018dnnlimit} simultaneously provided general results for the convergence of the prior of deep fully-connected networks to a GP.\footnote{The derivation of the limiting kernel differs between the two papers, with the results being consistent. \citet{matthews2018dnnlimit} carefully take limits of realisable networks, while \citet{lee2018dnnlimit} take the infinite limit of each layer sequentially.%Follow-on work \citep{matthews2018dnnlimit2,novak2019infiniteconv} further relax technical restrictions.
} 
A different class of limiting kernels, the Neural Tangent Kernel (NTK), originated from analysis of the function implied by a neural network during optimisation \citep{jacot2018ntk}, rather than the prior implied by the weight initialisation. Just like the Bayesian prior limit, this kernel sheds light on certain properties of neural networks, as well as providing a method with predictive capabilities of its own. 
The two approaches end up with subtly different kernels, which both can be computed as a recursive kernel.

With the general tools in place, \citet{garriga2018infiniteconv,novak2019infiniteconv} derived limits of the prior of convolutional neural networks with infinite filters. %\footnote{This time with \citet{garriga2018infiniteconv} using the simpler sequential infinite limits, and \citet{novak2019infiniteconv} blah blah}
These two papers directly motivated this work by noting that spatial correlations disappeared in the infinite limit. Spatial mean pooling at the last layer was suggested as one way to recover correlations, with \citet{novak2019infiniteconv} providing initial evidence of its importance. Due to computational constraints, they were limited to using a Monte Carlo approximation to the limiting kernel, while \citet{arora2019exact} performed the computation with the exact NTK. Very recent preprints provide follow-on work that pushes the performance of limit kernels \citep{shankar2020without} and demonstrated the utility of limit kernels for small data tasks \citep{arora2020small}. Extending on the results for convolutional architectures, \citet{yang2019wide} showed how infinite limits could be derived for a much wider range of network architectures.

In the kernel and Gaussian process community, kernels with convolutional structure have also been proposed. Notably, these retained spatial correlation in either a fixed \citep{vdw2017convgp} or adjustable \citep{mairal2014ckn,dutordoir2020} way. While these methods were not derived using an infinite limit, \Citet{vdw2019thesis} provided an initial construction from an infinitely wide neural network limit. Inspired by these results, we propose limits of deep convolutional neural networks which retain spatial correlation in a similar way.



% This needs to be good, and I will have to rely on you to a large degree Adrià.
% \begin{itemize}
%     \item Summarise work along the line of infinite-width models and results
%     \begin{itemize}
%         \item Early work, Neal. Single hidden layer
%         \item More recent work. Matthews 2018 has good references
%         \item Noticing specialised architectures: Convolutions. Adrià and Brain.
%         \item Generalising: Greg Yang's paper showing how things can be done for general architectures, Neural Tangents package, ...
%         \item NTK: A different kind of infinite limit relying on optimisation rather than Bayes rule.
%         \item Recent uses of infinite width. Ben Recht, Arora.
%     \end{itemize}
%     \item Summarise parallel advancements in Gaussian processes (few sentences). ConvGP, TickGP. ``In parallel, there has been work on directly constructing Gaussian processes with properties that are known to be useful in NNs. In analogy, ConvGP has been shown to be infinite limit of single-layer convgp.''
% \end{itemize}


\section{Spatial Correlations in Single-Layer Networks}
To begin, we will analyse the infinite limit of a single hidden layer convolutional neural network. We first show how spatial correlations disappear when using infinite weights \citet{garriga2018infiniteconv,novak2019infiniteconv}, and how mean pooling reintroduces them \citet{novak2019infiniteconv}. This leads to kernels with the same structure as \Citet{vdw2017convgp}. Next, we show how spatial correlations can be recovered by adding weight correlations instead of making the discrete architectural modification of adding mean pooling. This leads to the 
``Translation Insensitive Convolutional Kernel'' (TICK) considered by \citet{dutordoir2020}. In the next section, we will generalise this to deep convolutional networks.

\newcommand{\layerC}[1]{C^{(#1)}}
\newcommand{\layerw}[1]{P^{(#1)}}
\newcommand{\layerh}[1]{Q^{(#1)}}
\newcommand{\layersize}[1]{\layerw{#1}\layerh{#1}}
\newcommand{\patchw}[1]{p^{(#1)}}
\newcommand{\patchh}[1]{q^{(#1)}}
\newcommand{\W}[1]{\vW}
\newcommand{\layerWs}[1]{W^{(#1)}}
\newcommand{\layerW}[1]{\vW^{(#1)}}
\newcommand{\priorWcov}[1]{\boldsymbol{\Sigma}^{(#1)}}
\newcommand{\layerAs}[1]{A^{(#1)}}
\newcommand{\layerNLAs}[1]{Z^{(#1)}}
A single-layer Convolutional Neural Network (CNN) takes in an image input $\vX \in \Reals^{\layerC{0}\times \layersize{0}}$ with width $\layerw{0}$, height $\layerh{0}$, and $\layerC{0}$ channels (e.g.~one per colour). The image is divided up into $\layersize{0}$ patches of size $\layerC{0}\times\!\patchw{1}\times\!\patchh{1}$ with zero padding at the edges. Weights are applied by taking an inner product with all patches, which we do $\layerC{1}$ times to give multiple channels in the next layer. By collecting all weights in the matrix $\layerW{1}\in\Reals^{\layerC1\times \layerC0\times \patchw1\patchh1}$ we can denote the computation of the pre- and post-non-linearity activations as
\begin{align}
    \layerAs1_{cp} = \sum_{\gamma p} X_{\gamma p} \layerWs1_{c\gamma p} \,, && \layerNLAs1_{cp} = \phi\big(\layerAs1_{cp}\big)\,.
\end{align}
In a single-layer CNN, these activations are followed by a fully-connected layer with weights $\layerW{2} \in \Reals^{\layerC 1\times \layerw0\layerh0}$. Our final output is again given by a summation reduction over the activations:
\begin{align}
    f(\vX) = \sum_{cp} \phi\big(\layerAs1_{cp}\big) W^{(2)}_{cp} = \sum_{cp} \layerNLAs1_{cp}\layerWs2_{cp} \,.
\end{align}

As in \citet{garriga2018infiniteconv,novak2019infiniteconv}, our goal is to analyse the distribution on function outputs $f(\vX)$ for some Gaussian prior on the weights $p(\vW)$, where $\vW$ is the collection of the weights at all layers. For all cases that we analyse, the prior is constrained to be independent over layers channels:
\begin{align}
    p(\layerW1) = \prod_{c'=0}^{\layerC0}\prod_{c=1}^{\layerC1}\NormDist{\layerW1_{[cc':]}; 0, \frac{1}{\layerC0}\priorWcov1} \,,
    && p(\layerW2) = \prod_{c=1}^{\layerC1} \NormDist{\layerW2_{c:}; 0, \frac{1}{\layerC1}\priorWcov2} \,.
\end{align}
Since $\layerAs1_{cp}$ and $\layerWs2_{cp}$ are both independent over $c$, $f(X)$ is a s

Since $f(X)$ is a sum of 

\subsection{Notes}

Give a high-level commentary on the infinite-width line of work. Summarise commentary on this line of work. As far as I see, the main point here is MacKay's point on the baby and the bathwater, and who else has discussed it (I know Matthews has).
\begin{itemize}
    \item Note MacKay's point about disappearing correlations. Have we thrown out the baby with the bathwater?
    \item Note that both \citet{garriga2018infiniteconv} and \citet{novak2019infiniteconv} show that correlations disappeard in infinite limit.
    \item Ask question: Is this really what we want? The whole point of CNNs was to have spatial correlations in different areas of the image!
    \item Compare and contrast to additive models in GPs and convolutional models in GPs, which were introduce specifically to gain these correlations.
    \item Discuss mean-pooling as a way to get back correlations. Raise point that this is a discrete non-learnable architectural choice!
    \item Note that \citet{vdw2019thesis} briefly discusses how ConvGP is an infinite limit.
    \item Can we construct an infinite limit of a CNN that has the correlations like the ConvGP, but also works for full deep networks?
\end{itemize}

\adriacomment{Mention the recent interest into improving the prior for Bayesian NNs, how independent weights don't seem to be so great?}

There are various parallel lines of work that are relevant here. The first connection between neural networks and GPs is due to \citet{neal1996bayesian}, who noted that infinitely-wide single-layer random neural networks are equivalent to Gaussian processes. 

\section{Infinite Limits for correlated weights}

As a summary: $N$-dimensional convolutions in weight space correspond to $2N$-dimensional convolutions in kernel space.

\begin{figure}
  % Width: 397.9pt
  \centering
  % \def\svgwidth{\textwidth}
  \import{fig/}{net_diagram.pdf_tex}
  \caption{A convolutional neural network. Indices $i,j,\mu,\nu$
    are depicted as used in expressions that focus on layer 1.}
\end{figure}

In this section we derive the kernel corresponding to an infinitely wide convolutional neural network (CNN) with correlated weights. Our analysis applies to all numbers of dimensions of convolutions, and more generally all settings where "groups" of weights would be applied to different inputs \adriacomment{make precise using Greg Yang's stuff}.

The CNN takes an input image of width $P\ssup{0}$, height $Q\ssup{0}$, and $C\ssup{0}$ input channels, which we denote as $\vX \in \mathbb{R}^{C\ssup{0} \times P\ssup{0}Q\ssup{0}}$. Each layer $\ell$ has a convolution filter $\vW\ssup{\ell} \in \mathbb{R}^{C\ssup{\ell-1} \times C\ssup{\ell} \times p\ssup{\ell}}$, of width $p\ssup{\ell}$ and height $q\ssup{\ell}$. Usually the filter is square, $p\ssup{\ell}=q\ssup{\ell}$ and its size is 1 or 3.

Given the activations $A$ of layer $\ell$, the preactivations $Z$ of layer $\ell+1$ are the result of the convolution 
\begin{equation}
Z\ssup{\ell+1}_{i,\mu}(\vX) = \sum_{j=1}^{C\ssup{\ell}} \sum_{\nu \in \patchfun{\mu}} W\ssup{\ell}_{i,j,\nu}\, A\ssup{\ell}_{j,\nu}(\vX).
\label{eq:def-nn-convolution}
\end{equation}
We have abstracted the definition of the convolution by expressing it as a sum over elements of image patches. The activations are
\begin{align}
    A\ssup{0}_{i,\mu}(\vX) &= X_{i,\mu}  & A\ssup{\ell}_{i,\mu} &= \phi\ssup{\ell}(Z\ssup{\ell}_{i, \mu}(\vX)) \text{ for $\ell>0$},
\label{eq:def-nn-activations}
\end{align}
where $\phi\ssup{\ell}$ is an elementwise activation function.

\subsection{Independent Weights}

Previous work \citep{garriga2018infiniteconv, novak2019infiniteconv} proceeds by drawing each individual weight from an independent Gaussian distribution with zero mean
\begin{equation}
    W\ssup{\ell}_{i,j,\nu} \simiid \NormDist{0, \sigma_\text{w}^2/C\ssup{\ell}}.
    \label{eq:w-independent-dist}
\end{equation}

Then, the mean and covariance functions of the corresponding GP are the mean and covariance of the NN evaluated at inputs $\vX, \vX'$. The mean $m\ssup{\ell}_{i,\mu} = \ExpSymb\sqb{Z\ssup{\ell}_{i,\mu}} = 0$, because $\ExpSymb\sqb{W\ssup{\ell}_{i,j,\nu}}=0$. The covariance function $K$ is more involved:
\begin{equation}
    K\ssup{\ell+1}_{i,i',\mu,\mu'}(\vX, \vX') %= \ExpSymb\sqb{Z_{i,\mu}(\vX)\, Z_{i',\mu'}\bra{\vX'}} = \ExpSymb\sqb{}
    = \sum_{j=1}^{C\ssup{\ell}} \sum_{j'=1}^{C\ssup{\ell}} \sum_{\nu \in \patchfun{\mu}} \sum_{\nu' \in \patchfun{\mu'}} \ExpSymb\sqb{
    W\ssup{\ell}_{i,j,\nu} W\ssup{\ell}_{i',j',\nu'}} \ExpSymb\sqb{A\ssup{\ell}_{j,\nu}(\vX) A\ssup{\ell}_{j',\nu'}(\vX')}.
  \label{eq:cov-independent}
\end{equation}
Looking at equation~\cref{eq:w-independent-dist}, the uncentered covariance of $W$ is $\ExpSymb\sqb{W\ssup{\ell}_{i,j,\nu}W\ssup{\ell}_{i',j',\nu'}} = \delta_{i,i'} \, \delta_{j, j'} \, \delta_{\nu, \nu'}\, \sigma_\text{w}^2/C\ssup{\ell}$. Thus, we can remove the double sum over $j$, and each output channel $i$ is independent of the others. The result is 
\begin{align}
    K\ssup{\ell+1}_{i,\mu,\mu}(\vX, \vX') %= \ExpSymb\sqb{Z_{i,\mu}(\vX)\, Z_{i',\mu'}\bra{\vX'}} = \ExpSymb\sqb{}
    &= \sum_{j=1}^{C\ssup{\ell}} \sum_{\nu \in \patchfun{\mu}} \sum_{\nu' \in \patchfun{\mu'}} \delta_{\nu,\nu'} \frac{\sigma^2_\text{w}}{C\ssup{\ell}}\,
    \ExpSymb\sqb{A\ssup{\ell}_{j,\nu}(\vX) A\ssup{\ell}_{j,\nu}(\vX')} \\
    &= \sum_{\nu \in \patchfun{\mu}} \sum_{\nu' \in \patchfun{\mu'}} \delta_{\nu,\nu'} \sigma^2_\text{w}\,
      \ExpSymb\sqb{A\ssup{\ell}_{\cdot,\nu}(\vX) A\ssup{\ell}_{\cdot,\nu}(\vX')} 
\end{align}

If additionally we are only interested in the diagonal ($\mu=\mu'$), then this
becomes a just a convolution:
\begin{equation*}
    K\ssup{\ell+1}_{i,\mu,\mu}(\vX, \vX') %= \ExpSymb\sqb{Z_{i,\mu}(\vX)\, Z_{i',\mu'}\bra{\vX'}} = \ExpSymb\sqb{}
    = \sum_{j=1}^{C\ssup{\ell}} \sum_{\nu \in \patchfun{\mu}}  \ExpSymb\sqb{A\ssup{\ell}_{j,\nu}(\vX) A\ssup{\ell}_{j,\nu}(\vX')}.
\end{equation*}

Thus, the variance at the next layer is a convolution of the same specification as the NN.

\subsection{Correlated Weights}
Here, we place a Gaussian prior on the weights that is not independent. The weights
corresponding to each input (output) channel $j$ ($i$) have joint covariance
$\Sigma_\text{w}/C\ssup{\ell}$
\begin{equation}
  \vecfun{\vW\ssup{\ell}_{i,j}} \simiid \NormDist{\boldsymbol{0}, \Sigma_\text{w}/C\ssup{\ell}}.
  \label{eq:w-correlated-dist}
\end{equation}
Crucially, the weights are still independent across channels. Intuitively this
implies that, in the infinite width limit, each channel in layer $\ell+1$ is a
sum of infinitely many independent channels from layer $\ell$. Thus, a random
convolutional network initialized with these weights still converges in distribution
to a GP.

\begin{lemma}
  Consider a convolutional neural network defined as in
  \cref{eq:def-nn-convolution,eq:def-nn-activations}, with weights drawn from a
  zero-mean Gaussian correlated within each channel, as in \cref{eq:w-correlated-dist}.
  Set the number of channels to $C\ssup{\ell} = n$ for each layer $\ell$. Then,
  as $n\to \infty$, each output channel $\vZ\ssup{L}_i(\vX)$ of the network converges in
  distribution to an independent Gaussian process.
\end{lemma}
\begin{proof}
  We express the network using the \textsc{Netsor} programming language due to
  \citet{yang2019wide}. See \cref{app:netsor}.
\end{proof}

We can obtain an expression for the covariance $K\ssup{\ell}_{i,\mu,\mu'}(\vX,
\vX')$ starting from \cref{eq:cov-independent}. This time, however,
$\ExpSymb\sqb{W\ssup{\ell}_{i,j,\nu}W\ssup{\ell}_{i',j',\nu'}} = \delta_{i,i'} \, \delta_{j, j'} \, \Sigma_{\text{w}}\bra{\nu,\nu'}/C\ssup{\ell}$.

  
\begin{itemize}
    \item Derivation with correlated weights.
\end{itemize}
Summary of this: D-dimensional convolutions of NN weights become 2D-dimensional convolutions of covariance tensors. 

Highlight the differences and the different computational choices we need to make.

We could have taken another path to generalize TICK to multiple layers.

What choice of covariance do we make ?
This was inspired by TICK

\section{Experiments}
Here we seek to answer the question:
 Can we improve the accuracy of Bayesian classifiers by adding more sophisticated correlation structures in the prior?
\begin{itemize}
    \item For the last layer, as almost everyone until now has done?
    \item For intermediate layers?
\end{itemize}

We investigate how the effect above varies with data set size.



\section{Conclusion}
We show
\begin{itemize}
    \item ...
\end{itemize}

"Wev'e identified a new property that is helpful in the infinite limit. Would correlated weights be useful for finite neural netwokrs as well?"

the argument about "if you increase the size of your images" 

\section*{Broader Impact}

Authors are required to include a statement of the broader impact of their work, including its ethical aspects and future societal consequences. 
Authors should discuss both positive and negative outcomes, if any. For instance, authors should discuss a) 
who may benefit from this research, b) who may be put at disadvantage from this research, c) what are the consequences of failure of the system, and d) whether the task/method leverages
biases in the data. If authors believe this is not applicable to them, authors can simply state this.

Use unnumbered first level headings for this section, which should go at the end of the paper. {\bf Note that this section does not count towards the eight pages of content that are allowed.}

\begin{ack}
Use unnumbered first level headings for the acknowledgments. All acknowledgments
go at the end of the paper before the list of references. Moreover, you are required to declare 
funding (financial activities supporting the submitted work) and competing interests (related financial activities outside the submitted work). 
More information about this disclosure can be found at: \url{https://neurips.cc/Conferences/2020/PaperInformation/FundingDisclosure}.


Do {\bf not} include this section in the anonymized submission, only in the final paper. You can use the \texttt{ack} environment provided in the style file to autmoatically hide this section in the anonymized submission.
\end{ack}

\printbibliography

\newpage

\input{supplementary.tex}

\end{document}
