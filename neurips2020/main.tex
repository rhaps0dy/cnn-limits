\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2020

% ready for submission
% \usepackage{neurips_2020}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2020}

% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2020}

% to avoid loading the natbib package, add option nonatbib:
\usepackage[nonatbib]{neurips_2020}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Useful packages
% \usepackage{subcaption}
\usepackage{xcolor}
\usepackage{amsmath}
\usepackage{amsfonts}

% Theorem environments
\usepackage{amsthm}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}

% Algorithms
\usepackage{algorithm}
\usepackage{algpseudocode}

\usepackage{graphicx}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}
\usepackage{paralist}
\usepackage{cleveref}
\creflabelformat{equation}{#2#1#3}
% \usepackage{listings}
% \lstset{
%     showstringspaces=false,
%     basicstyle=\ttfamily,
%     breaklines=true,
%     breakatwhitespace=true,
% }
% \usepackage{booktabs}
\usepackage{cancel}
\usepackage{import}  % Inkscape figures

\usepackage[colorinlistoftodos]{todonotes}
\newcommand{\markcomment}[1]{\todo[color=green]{#1}\xspace}
\newcommand{\adriacomment}[1]{\todo[color=blue]{#1}\xspace}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Bibliography
\usepackage{MarkBiblatexCmds}
\addbibresource{ref.bib}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Mathematical commands
\usepackage{MarkMathCmds}

\newcommand{\vX}{\mathbf{X}}
\newcommand{\vW}{\mathbf{W}}
\newcommand{\vU}{\mathbf{U}}
\newcommand{\vZ}{\mathbf{Z}}
\newcommand{\vA}{\mathbf{A}}
\newcommand{\vQ}{\mathbf{Q}}
\newcommand{\vK}{\mathbf{K}}
\newcommand{\vY}{\mathbf{Y}}
\newcommand{\vL}{\mathbf{L}}
\newcommand{\vSigma}{\boldsymbol{\Sigma}}
\newcommand{\vLambda}{\boldsymbol{\Lambda}}
\newcommand{\eye}{\mathbf{I}}

\newcommand{\patchfun}[1]{{#1}\text{th patch}}
\newcommand{\vecfun}{\text{vec}\bra}
\newcommand{\simiid}{\overset{i.i.d.}{\sim}}

\newcommand{\tp}{{\mathrm{\textsf{\tiny T}}}}
\newcommand{\vbar}{{\,|\,}}

% Brackets and super/sub -indices
\newcommand{\bracket}[3]{{\left#1 #3 \right#2}}
\newcommand{\bra}{\bracket{(}{)}}
\newcommand{\cb}{\bracket{\{}{\}}}
\newcommand{\sqb}{\bracket{[}{]}}
\newcommand{\abs}{\bracket{|}{|}}
\newcommand{\ceil}{\bracket{\lceil}{\rceil}}
\newcommand{\floor}{\bracket{\lfloor}{\rfloor}}
\DeclareMathOperator*{\ExpOperator}{\ExpSymb}

\newcommand{\ssup}[1]{^{\bra{#1}}}
\usepackage{dsfont}
\newcommand{\indicator}[1]{{\mathds{1}}\left[#1\right]}



\title{Correlated Weights in Infinite Limits \\ of Deep Convolutional Neural Networks}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

\author{%
  Adrià Garriga-Alonso \\
  University of Cambridge \\
  \texttt{ag919@cam.ac.uk} \\
  % examples of more authors
   \And
   Mark van der Wilk \\
   Imperial College London \\
   \texttt{m.vdwilk@imperial.ac.uk} \\
  % \AND
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
}

\begin{document}

\maketitle

\begin{abstract}
Infinite width limits of deep neural networks often have tractable forms. They have been used to analyse the behaviour of finite networks, as well as being useful methods in their own right. Currently used limits of deep convolutional networks lose correlating contributions from different spatial locations in the image, unlike their finite counterparts, even when those only use convolutions. We argue that this is undesirable, and remedy it by introducing spatial correlations in the prior over weights. This leads to correlated contributions being preserved in the wide limit. Varying the amount of correlation in the convolution weights allows interpolation between independent-weight limits and mean pooling (which is equivalent to complete correlation in the weights). Empirical evaluation of the infinitely wide network shows that optimal performance is achieved between the extremes, indicating the usefulness of considering correlations in the weights.
\end{abstract}

% Thoughts
% - Number of filters / neurons is said to be related to the capacity of the neural network to learn. Well, what happens if we increase capacity to infinity?
% - We should discuss both NTK and "GP" equivalent


\section{Introduction}
% Paragraph 1
% - NN theory
% - Infinite limits are often more tractable
% - Infinite limits have given understanding, and new methods
% - However, we lose certain properties
Neural networks have been enormously successful at learning relationships through minimising some loss on a training set, starting from a random initialisation. While a full theoretical explanation of the success of neural networks is still lacking, analysis of the random initialisation has proven to be useful for providing both insight and practically useful methods.
In particular, limits where the width of layers is taken to infinity have been particularly crucial, as these correspond to linear basis function models, which are well-understood mathematically.
\citet{neal1996bayesian} first noted such a relationship, through the correspondence between infinitely wide Bayesian neural networks and Gaussian processes (GPs). The GP representation greatly simplified Bayesian learning, leading to its widespread adoption \citep{williams1996gpr,gpml}.

The success of GPs raised the question of whether a mathematically simple linear model could replace a complex neural network. \citet{mackay1998introgp} noted that in taking the infinite limit, the feature representation had become fixed. Given that learnable features are a key desirable property of neural networks which is lost due to the infinite limit, MacKay inquired: ``have we thrown the baby out with the bath water?'' Recent renewed interest in GP correspondences for deep networks \citep{matthews2018dnnlimit,lee2018dnnlimit} show that it is still unclear to what degree neural networks exhibit GP behaviour.

In this work we investigate a property that, similarly to feature learning, is present in finite convolutional neural networks, but which disappears in the infinite limit: correlation from patches in different parts of the image \citep{garriga2018infiniteconv,novak2019infiniteconv}. Given that convolutions were developed to introduce these correlations, it seems undesirable that they are lost when more filters are added.
Currently, changing the architecture by introducing mean-pooling is the only way
that these correlations have been reintroduced \citep{novak2019infiniteconv},
and they have been shown to be important for improving performance
\citep{arora2019exact}. We show that correlations between patches can also be
maintained in the limit without pooling. Instead, we introduce correlations
between the weights in the prior. The amount of correlation can be controlled,
which allows us to interpolate between the existing approaches of full
independence and mean-pooling. Our approach allows the discrete architectural choice of mean-pooling to be replaced with a more flexible continuous amount of correlation. Empirical evaluation on CIFAR-10 shows that this additional flexibility improves performance.

Our work illustrates how choices that are made in the prior affect properties of the limit, and that good choices can improve performance. The success of this approach in the infinite limit also raises questions about whether correlated weights should be used in finite networks.



% \begin{itemize}
%     \item Motivate interest in infinitely wide neural networks
%     \begin{itemize}
%         \item Understand properties of neural networks (feature space, training dynamics)
%         \item Simplify Bayesian inference (infinite width makes GP)
%         \item Should read some of the other infinite width papers for more points
%     \end{itemize}
%     \item Mention that many different NN's have a GP equivalent, including CNNs.
%     \item Introduce problem:
%     \begin{itemize}
%         \item Not long after the uncovering of infinite-width correspondences [Neal 1996], limitations were discovered. MacKay baby bathwater.
%         \item Here we argue that a similar effect can be seen in infinite limits of CNNs. Independence over the weights removes correlations over distant patches.
%     \end{itemize}
%     \item Introduce our contribution:
%     \begin{itemize}
%         \item We investigate the effect of patch correlations on the performance of infinite-width models.
%         \item We show that a different prior over weights re-introduces correlations.
%         \item We discuss how mean-pooling (does max-pooling do this too? \adriacomment{yes, but nobody uses it in wide CNNs because nobody has bothered to figure out the expression for propagating the covariance forward}) has a similar effect, explaining why it has been used in other recent papers.
%         \item We show how the correlated prior allows us to interpolate between the independent limit, and mean-pooling limits, an that often something in the middle performs best.
%     \end{itemize}
%     \item Speculate on what's next:
%     \begin{itemize}
%         \item Perhaps finite-width networks should go beyond independent initialisations?
%     \end{itemize}
% \end{itemize}

\section{Related work}
Infinitely wide limits of neural networks are currently an important tool for creating approximations and analyses. Here we provide a background on the different infinite limits that have been developed, together with a brief overview of where they have been applied.

Interest in infinite limits first started with research into properties of Bayesian priors on the weights of neural networks. \citet{neal1996bayesian} noted that prior function draws from a single hidden layer neural network with appropriate Gaussian priors on the weights tended to a Gaussian process as the width grew to infinity. The simplicity of performing Bayesian inference in Gaussian process models led to their widespread adoption soon after \citep{williams1996gpr,gpml}. Over the years, the wide limits of networks with different weight priors and activation functions have been analysed, leading to various \emph{kernels} which specify the properties of the limiting Gaussian processes \citep{williams1997inf,cho2009mkm}.

With the increasing prominence of deep learning, recursive kernels were introduced in an attempt to obtain similar properties. \citet{cho2009mkm,mairal2014ckn} investigated such methods for fully-connected and convolutional architectures respectively. Despite similarities between recursive kernels and neural networks, the derivation did not provide clear relationships, or any equivalence in a limit. \citet{hazan2015} took initial steps to showing the wide limit equivalence of a neural network beyond the single layer case. Recently, \citet{matthews2018dnnlimit,lee2018dnnlimit} simultaneously provided general results for the convergence of the prior of deep fully-connected networks to a GP.\footnote{The derivation of the limiting kernel differs between the two papers, with the results being consistent. \citet{matthews2018dnnlimit} carefully take limits of realisable networks, while \citet{lee2018dnnlimit} take the infinite limit of each layer sequentially.%Follow-on work \citep{matthews2018dnnlimit2,novak2019infiniteconv} further relax technical restrictions.
} 
A different class of limiting kernels, the Neural Tangent Kernel (NTK), originated from analysis of the function implied by a neural network during optimisation \citep{jacot2018ntk}, rather than the prior implied by the weight initialisation. Just like the Bayesian prior limit, this kernel sheds light on certain properties of neural networks, as well as providing a method with predictive capabilities of its own. 
The two approaches end up with subtly different kernels, which both can be computed as a recursive kernel.

With the general tools in place, \citet{garriga2018infiniteconv,novak2019infiniteconv} derived limits of the prior of convolutional neural networks with infinite filters. %\footnote{This time with \citet{garriga2018infiniteconv} using the simpler sequential infinite limits, and \citet{novak2019infiniteconv} blah blah}
These two papers directly motivated this work by noting that spatial correlations disappeared in the infinite limit. Spatial mean pooling at the last layer was suggested as one way to recover correlations, with \citet{novak2019infiniteconv} providing initial evidence of its importance. Due to computational constraints, they were limited to using a Monte Carlo approximation to the limiting kernel, while \citet{arora2019exact} performed the computation with the exact NTK. Very recent preprints provide follow-on work that pushes the performance of limit kernels \citep{shankar2020without} and demonstrated the utility of limit kernels for small data tasks \citep{arora2020small}. Extending on the results for convolutional architectures, \citet{yang2019wide} showed how infinite limits could be derived for a much wider range of network architectures.

In the kernel and Gaussian process community, kernels with convolutional structure have also been proposed. Notably, these retained spatial correlation in either a fixed \citep{vdw2017convgp} or adjustable \citep{mairal2014ckn,dutordoir2020} way. While these methods were not derived using an infinite limit, \Citet{vdw2019thesis} provided an initial construction from an infinitely wide neural network limit. Inspired by these results, we propose limits of deep convolutional neural networks which retain spatial correlation in a similar way.



% This needs to be good, and I will have to rely on you to a large degree Adrià.
% \begin{itemize}
%     \item Summarise work along the line of infinite-width models and results
%     \begin{itemize}
%         \item Early work, Neal. Single hidden layer
%         \item More recent work. Matthews 2018 has good references
%         \item Noticing specialised architectures: Convolutions. Adrià and Brain.
%         \item Generalising: Greg Yang's paper showing how things can be done for general architectures, Neural Tangents package, ...
%         \item NTK: A different kind of infinite limit relying on optimisation rather than Bayes rule.
%         \item Recent uses of infinite width. Ben Recht, Arora.
%     \end{itemize}
%     \item Summarise parallel advancements in Gaussian processes (few sentences). ConvGP, TickGP. ``In parallel, there has been work on directly constructing Gaussian processes with properties that are known to be useful in NNs. In analogy, ConvGP has been shown to be infinite limit of single-layer convgp.''
% \end{itemize}


\section{Spatial Correlations in Single-Layer Networks}
\label{sec:single-layer}
To begin, we will analyse the infinite limit of a single hidden layer
convolutional neural network (CNN). We extend \citet{garriga2018infiniteconv,novak2019infiniteconv} by considering weight priors with correlations. By adjusting the correlation we can interpolate between existing independent weight limits and mean-pooling, which previously had to be introduced as a discrete architectural choice. We also make connections to variants of convolutional GPs \citep{vdw2017convgp,dutordoir2020}. In the next section, we generalise to deep neural networks.

% We first show how spatial correlations disappear
% when using infinite weights \citep{garriga2018infiniteconv,novak2019infiniteconv}, and how mean pooling reintroduces them \citep{novak2019infiniteconv}. This leads to kernels with the same structure as \Citet{vdw2017convgp}. Next, we show how spatial correlations can be recovered by adding weight correlations instead of making the discrete architectural modification of adding mean pooling. This leads to the 
% ``Translation Insensitive Convolutional Kernel'' (TICK) considered by \citet{dutordoir2020}. In the next section, we will generalise this to deep convolutional networks.

\newcommand{\layerC}[1]{C^{(#1)}}
\newcommand{\layerw}[1]{P^{#1}}
\newcommand{\layerh}[1]{Q^{#1}}
\newcommand{\layersize}[1]{\layerw{#1}\cdot\layerh{#1}}
\newcommand{\patchw}[1]{p^{(#1)}}
\newcommand{\patchh}[1]{q^{(#1)}}
\newcommand{\patchsize}[1]{\patchw{#1}\patchh{#1}}
\newcommand{\patchidx}{q}
\newcommand{\W}[1]{\vW}
\newcommand{\layerWs}[1]{W^{(#1)}}
\newcommand{\layerW}[1]{\vW^{(#1)}}
\newcommand{\priorWcov}[1]{\boldsymbol{\Sigma}^{(#1)}}
\newcommand{\priorWcovs}[1]{\Sigma^{(#1)}}
\newcommand{\layerA}[2]{\mathbf{Z}^{(#1)}_{#2}(\vX)}
\newcommand{\layerAd}[2]{\mathbf{Z}^{(#1)}_{#2}(\vX')}
\newcommand{\layerAs}[2]{Z^{\!(#1)}_{#2}\!(\vX)}
\newcommand{\layerAsd}[2]{Z^{\!(#1)}_{#2}\!(\vX')}
\newcommand{\layerNLAs}[2]{A^{\!#1}_{#2}(\vX)}
\newcommand{\layerNLAsd}[2]{A^{\!#1}_{#2}(\vX')}
\newcommand{\layerNLA}[2]{\vA^{\!#1}_{#2}\!(\vX)}
\newcommand{\layerNLAd}[2]{\vA^{\!#1}_{#2}\!(\vX')}
% These 4 are used for the picture below
\newcommand{\chan}{c}
\newcommand{\prevchan}{\gamma}   % <- Possibly use these for c, p, gamma? I've
\newcommand{\patch}{p}               % been using \nu,\mu
\newcommand{\nextpatch}{\mu}





\begin{figure}[b]
  % Width: 397.9pt
  \centering
{\renewcommand{\layerAs}[1]{\vZ^{(#1)}}
  % \def\svgwidth{\textwidth}
  \import{fig/}{net_diagram.pdf_tex}}
  \caption{A deep convolutional neural network following our notation. Infinite limits are taken over the number of convolutional filters $\layerC{\ell}$. As the number of filters grow (vertical), the number of channels in the following layer (horizontal) grow as well.}
    % {N.B.: We need to emphasise that the last layer $Z^{(3)}$ is $f(\cdot)$.}
  \label{fig:fancy-cnn}
\end{figure}

\newcommand{\convpatch}[2]{{#1}^{[#2]}}
A single-layer Convolutional Neural Network (see \cref{fig:fancy-cnn} for a graphical representation of the notation) takes in an image input $\vX \in \Reals^{\layerC{0}\times \layersize{}}$ with width $\layerw{}$, height $\layerh{}$, and $\layerC{0}$ channels (e.g.~one per colour). The image is divided up into patches $\convpatch{\vX}{p} \in \Reals^{\layerC{0}\times\patchw{1}\patchh{1}}$, with $p$ representing a location of one of the $\layersize{}$ zero-padded patches. Weights are applied by taking an inner product with all patches, which we do $\layerC{1}$ times to give multiple channels in the next layer. By collecting all weights in the matrix $\layerW{1}\in\Reals^{\layerC1\times \layerC0\times \patchw1\patchh1}$ we can denote the computation of the pre- and post-nonlinearity activations as
\begin{align}
    \layerAs{1}{cp} = \sum_{\gamma=1}^{\layerC0}\sum_{q=1}^{\patchw0\patchh0} \convpatch{X}{p}_{\gamma q} \layerWs1_{c\gamma q} \,, && \layerNLAs{(1)}{cp} = \phi\big(\layerAs{1}{cp}\big)\,.
\end{align}
In a single-layer CNN, these activations are followed by a fully-connected layer with weights $\layerW{2} \in \Reals^{\layerC 1\times \layerw{}\layerh{}}$. Our final output is again given by a summation reduction over the activations:
\begin{align}
    f(\vX) = \sum_{c=1}^{\layerC1}\sum_{p=1}^{\layersize{}} \phi\big(\layerAs{1}{cp}\big) W^{(2)}_{cp} = \sum_{cp} \layerNLAs{(1)}{cp}\layerWs{2}_{cp} = \sum_p \layerNLAs{(f)}{p} \label{eq:single-layer-f} \,,
\end{align}
where $\layerNLAs{(f)}{p}$ denotes the result before the summation over the patch locations $p$. 

We analyse the distribution on function outputs $f(\vX)$ for some Gaussian prior on the weights $p(\vW)$, where $\vW$ is the collection of the weights at all layers. In all the cases we consider, we take the prior to be independent over layers and channels. Here we extend on earlier work by allowing spatial correlation in the final layer's weights (we will consider all layers later) through the covariance matrix $\priorWcov1 \in \Reals^{\layersize{}\times\layersize{}}$. This gives the prior
\begin{align}
    p(\layerW1) = \prod_{c'=0}^{\layerC0}\prod_{c=1}^{\layerC1}\NormDist{\layerW1_{[cc':]}; 0, \eye} \,,
    && p(\layerW2) = \prod_{c=1}^{\layerC1} \NormDist{\layerW2_{[c:]}; 0, \frac{1}{\layerC1}\priorWcov2} \,,
\end{align}
where we use square brackets to index into a matrix or tensor, using the Numpy colon notation for the collection of all variables along an axis.

Independence between channels makes the collection of all activations\footnote{We use boldface variables to collect all subscripted tensors into a single matrix or tensor. This can then be indexed using square brackets, i.e.~$\layerNLA{(f)}{[p]} = \layerNLAs{(f)}{p}$.} $\layerNLA{(f)}{}$ a sum of i.i.d.~multivariate random variables, which allows us to apply the central limit theorem (CLT) as $\layerC1\to\infty$ \citep{neal1996bayesian}. The covariance between the final-layer activations for two inputs $\vX,\vX'$ becomes
\begin{align}
    \mathbb{C}_{\vW}\left[\layerNLAs{(f)}{p},\layerNLAsd{(f)}{p'}\right] &= \Exp{\vW}{\sum_{c=1}^{\layerC1}\sum_{c'=1}^{\layerC1} \layerNLAs{(1)}{cp}\layerWs{2}_{cp}\layerNLAsd{(1)}{c'p'}\layerWs{2}_{c'p'}} \nonumber \\
    &= \sum_{c=1}^{\layerC1}\sum_{c'=1}^{\layerC1} \Exp{\layerW1}{\layerNLAs{(1)}{cp}\layerNLAsd{(1)}{c'p'}}\Exp{\layerW2}{\layerWs{2}_{cp} \layerWs{2}_{c'p'}} \nonumber \\
    &= \sum_{c=1}^{\layerC1}\sum_{c'=1}^{\layerC1} \delta_{cc'} \Exp{\layerW1}{\layerNLAs{(1)}{cp}\layerNLAsd{(1)}{c'p'}}\frac{\priorWcovs{2}_{pp'}}{\layerC{\ell}} \nonumber \\
    &= \sum_{c=1}^{\layerC1} \Exp{\layerW1}{\layerNLAs{(1)}{cp}\layerNLAsd{(1)}{cp'}} \frac{\priorWcovs{2}_{pp'}}{\layerC{\ell}} = k^{(1)}(\vX^{[p]}, {\vX'}{}^{[p']}) \priorWcovs{2}_{pp'} \,.
\end{align}
The limit of the sum of the final expectation over $\layerW1$ can be found (see appendix B for details) in closed form and is denoted as $k^{(1)}(\vX^{[p]}, {\vX'}{}^{[p']})$.
We find the final kernel for the GP by taking the covariance between function values $f(\vX)$ and $f(\vX')$ and performing the final sum in \cref{eq:single-layer-f}:
\begin{align}
    k(\vX, \vX') = \mathbb{C}\left[f(\vX), f(\vX')\right] = \sum_{pp'} k^{(1)}(\vX^{[p]}, {\vX'}{}^{[p']}) \priorWcovs{2}_{pp'} \,.
\end{align}
We can now see how different choices for $\priorWcov2$ give different forms of spatial correlation.

\paragraph{Independence} \citet{garriga2018infiniteconv,novak2019infiniteconv} consider $\priorWcovs{2}_{pp'} = \delta_{pp'}$, i.e.~the case where all weights are independent. The resulting kernel simply sums components over patches, which implies an \emph{additive model} \citep{stone1985}, where a \emph{different} function is applied to each patch, after which they are all summed together: $f(\vX) = \sum_p f_p(\vX^{[p]})$. This structure has commonly been applied to improve GP performance in high-dimensional settings \citep[e.g.][]{duvenaud2011additive,durrande2012additive}. \citet{novak2019infiniteconv} point out that the same kernel can be obtained by taking an infinite limit of a \emph{locally connected network} (LCN) \citep{lecun1989generalization} where connectivity is the same as in a CNN, but without weight sharing, indicating that a key desirable feature of CNNs is lost.

\paragraph{Mean-pooling} By taking $\priorWcovs2_{pp'} = 1$ we make the weights fully correlated over all locations, leading to identical weights for all $p$, i.e.~$\layerWs{2}_{cp} = \layerWs{2}_{c}$. This is equivalent to taking the mean response over all spatial locations (see \cref{eq:single-layer-f}), or mean-pooling. As \citet{novak2019infiniteconv} discuss, this reintroduces the spatial correlation that is the intended result of weight sharing. This case is identical to the ``translation invariant'' construction in \Citet{vdw2017convgp}. Their experiments show that mean-pooling is too restrictive in this single-layer case. To remedy this, they perform a pooling with a learned weight ${\alpha_p}$ for each patch, which can be recovered by taking $\priorWcovs{2}_{pp'} = \alpha_p \alpha_{p'}$. These weights need to be learned, which departs slightly from the strict Bayesian framework.

\paragraph{Spatially correlated weights} In the pooling examples above, the spatial covariance of weights is taken to be a rank-1 matrix. We can add more flexibility to the model by varying the strength of correlation between weights based on their distance in the image. We consider an exponential decay depending on the distance between two patches: $\priorWcovs{2}_{pp'} = \exp(-d(p, p')/l)$. We recover full independence by taking $l\to 0$, and mean-pooling with $l\to\infty$. Intermediate values of $l$ allow the rigid assumption of complete weight sharing to be relaxed, while still retaining spatial correlations between similar patches. This construction gives exactly the same kernel as investigated by \citet{dutordoir2020}, who named this property ``translation insensitivity'', as opposed to the stricter invariance that mean-pooling gives. The additional flexibility improved performance without needing to add many parameters that are learned in an non-Bayesian fashion.

Here, we showed that spatial correlation can be retained in infinite limits without needing to resort to architectural changes. A simple change to the prior on the weights is all that was needed. This property is retained in wide limits of deep networks in a similar way, which we investigate next.



% \subsection{Notes}

% Give a high-level commentary on the infinite-width line of work. Summarise commentary on this line of work. As far as I see, the main point here is MacKay's point on the baby and the bathwater, and who else has discussed it (I know Matthews has).
% \begin{itemize}
%     \item Note MacKay's point about disappearing correlations. Have we thrown out the baby with the bathwater?
%     \item Note that both \citet{garriga2018infiniteconv} and \citet{novak2019infiniteconv} show that correlations disappeard in infinite limit.
%     \item Ask question: Is this really what we want? The whole point of CNNs was to have spatial correlations in different areas of the image!
%     \item Compare and contrast to additive models in GPs and convolutional models in GPs, which were introduce specifically to gain these correlations.
%     \item Discuss mean-pooling as a way to get back correlations. Raise point that this is a discrete non-learnable architectural choice!
%     \item Note that \citet{vdw2019thesis} briefly discusses how ConvGP is an infinite limit.
%     \item Can we construct an infinite limit of a CNN that has the correlations like the ConvGP, but also works for full deep networks?
% \end{itemize}





\section{Spatial Correlations in Infinitely Wide Deep Convolutional Networks\label{sec:correlated-weights}}
The setup for the case of a deep neural network follows that of \cref{sec:single-layer}, but with weights applied to each patch of the activations in the previous layer as
\begin{align}
    \layerAs{\ell}{cp} = \sum_{\gamma=1}^{\layerC{\ell}}\sum_{q=1}^{\patchw{\ell}\patchh\ell} \layerNLAs{(\ell-1)[p]}{\gamma q} \layerWs{\ell}_{c\gamma q} \,, && \layerNLAs{(\ell)}{cp} = \phi\big(\layerAs{\ell}{cp}\big) \,. \label{eq:deep-recursion}
\end{align}
We use two ways to index into activations. We either index into the $p$th location as $\layerNLAs{(\ell)}{cp}$, or into the $q$th location in the $p$th patch as $\layerNLAs{(\ell)[p]}{cq}$. For regular convolutions, the number of patches is equal to the number of spatial positions in the layer before due to zero-padding, regardless of filter size. The only operation that changes the spatial size of the activations is a strided convolution. The one exception is the final layer, where we reduce all activations with their own weight. To unify notation, we see this as just another convolutional layer, but with a patch size equal to the activation size, and without zero padding. The final layer can have multiple output channels to allow e.g.~classification with multiple classes.

As pointed out by \citet{matthews2018dnnlimit}, a straightforward application of the central limit theorem is not strictly possible for deep networks. Fortunately, \citet{yang2019wide} developed a general framework for expressing neural network architectures and finding their corresponding Gaussian process infinite limits. The resulting kernel is given by the recursion that can be derived from a more informal argument which takes the infinite width limit in a sequential layer-by-layer fashion, as was used in \citet{garriga2018infiniteconv}. We follow this informal derivation, as this more naturally illustrates the procedure for computing the kernel. A formal justification can be found in appendix A.

\subsection{Recursive Computation of the Kernel}
To derive the limiting kernel for the output of the neural network, we will derive the distribution of the activations for each layer. In our weight prior, we correlate weights \emph{within} a convolutional filter:
\begin{align}
    \NormDist{\layerW{\ell}_{[c\gamma :]}; 0, \frac{1}{\layerC{\ell-1}} \priorWcov{\ell}} \,.
\end{align}
Our derivation is general for any covariance matrix, so layers with correlated weights can be interspersed with the usual layers.
% Following \citet{dutordoir2020}, we let the correlation decay exponentially with the distance between weights within the convolutional filter:
% \begin{align}
% \priorWcovs{2}_{pp'} = \exp(-d(p, p')/l) \,.
% \end{align}

A Gaussian process is determined by the covariance of function values for pairs of inputs $\vX,\vX'$. Since the activations at the top layer are the function values, we will compute covariances between activations from the bottom of the network up.
Starting from the recursion in \cref{eq:deep-recursion}, we can find the covariance between any two pre-nonlinearity activations from a pair of inputs $\vX,\vX'$:
\begin{align}
    \mathbb{C}_{\vW}\left[\layerAs{\ell}{cp}, \layerAsd{\ell}{c'p'} \right] &= \sum_{\gamma=1}^{\layerC{\ell-1}} \sum_{\gamma'=1}^{\layerC{\ell-1}} \sum_{q=1}^{}\sum_{q'=1}^{} \Exp{\vW}{\layerNLAs{(\ell-1)[p]}{\gamma q}\layerWs{\ell}_{c\gamma q} \layerNLAsd{(\ell-1)[p']}{\gamma q'}\layerWs{\ell}_{c\gamma' q'}} \nonumber \\
    &= \delta_{cc'} \frac{1}{\layerC{\ell-1}} \sum_{\gamma qq'} \Exp{\vW}{\layerNLAs{(\ell-1)[p]}{\gamma q} \layerNLAsd{(\ell-1)[p']}{\gamma q'}} \priorWcovs{\ell}_{qq'} \nonumber \\
    &= \delta_{cc'} K^{(\ell)}_{pp'}(\vX, \vX') \label{eq:kernel-recursion} \,.
\end{align}
For $\ell=1$, the activations in the previous layer are the image inputs, i.e.~$\layerNLA{(0)}{} = \vX$, making the expectation a simple product between image patches. The pre-nonlinearity activations are Gaussian, because of the linear relationship with the weights. This allows us to find the covariance of the post-nonlinearity activations. Since $\layerA{\ell}{},\layerAd{\ell}{}$ are jointly Gaussian, the expectation will only depend on pairwise covariances. Here we represent this dependence through the function $F(\cdot, \cdot, \cdot)$ (see appendix B for details on the computation):
\begin{align}
    %\mathbb{C}_\vW\left[\layerNLAs{\ell}{cp}, \layerNLAsd{\ell}{cp'}\right] =
    \Exp{\vW}{\layerNLAs{\ell}{cp}\layerNLAsd{\ell}{cp'}} &= \Exp{ \layerA{\ell}{}, \layerAd{\ell}{} }{\phi\big(\layerAs{\ell}{cp}\big) \phi\big(\layerAsd{\ell}{cp}\big)} \label{eq:post-nonlin} \nonumber \\
    &= F(K^{(\ell)}_{pp}(\vX, \vX), K^{(\ell)}_{pp'}(\vX, \vX'), K^{(\ell)}_{p'p'}(\vX', \vX'))\nonumber\\
    &= V_{pp'}^{(\ell)}(\vX, \vX')\,.
\end{align}
The pre- and post-nonlinearity activations are independent between different channels, and identical over all channels, so we omit denoting the channel indices.

To compute the covariance of the pre-nonlinearity for $\ell\geq 2$, we can again apply \cref{eq:kernel-recursion}. \Cref{eq:post-nonlin} shows that the post-nonlinearity covariances are constant over channels, so we can simplify \cref{eq:kernel-recursion} further:
\begin{align}
    K_{pp'}^{(\ell)}(\vX, \vX') &= \frac{1}{\layerC{\ell-1}} \sum_{\gamma qq'} \Exp{\vW}{\layerNLAs{(\ell-1)[p]}{\gamma q} \layerNLAsd{(\ell-1)[p']}{\gamma q'}} \priorWcovs{\ell}_{qq'} \nonumber \\
    &= \sum_{q \in p\text{th patch}}\sum_{q'\in p'\text{th patch}} V_{qq'}^{(\ell-1)[p]}(\vX,\vX') \priorWcovs{\ell}_{qq'} \,.
\end{align}
We next want to compute the post-nonlinearity activations for layer $\ell$. For finite $\layerC{1}$, $\layerA{2}{}$ will not be Gaussian, which is required by \cref{eq:post-nonlin}. However, if we take $\layerC{\ell-1}\to\infty$, $\layerA{\ell}{}$ will converge to a Gaussian by the central limit theorem, all while keeping the covariance constant. After taking the limit, we can then apply \cref{eq:post-nonlin}. This provides us with a recursive procedure to compute the covariances all the way up to the final layer, by sequentially taking limits of $\layerC\ell\to\infty$. % We summarise the process in algorithm 1.

% We can now apply \cref{eq:kernel-recursion} to find the covariance of the pre-nonlinearity activations at layer $\ell=2$, given the covariance of the post-nonlinarity activations in layer $\ell=1$ (\cref{eq:post-nonlin}). For finite $\layerC{1}$, $\layerA{2}{}$ will not be Gaussian, and so \cref{eq:post-nonlin} can not be applied. However, if we take $\layerC1\to\infty$, $\layerA{2}{}$ will converge to a Gaussian by the central limit theorem, all while keeping the covariance constant. After taking the limit, we can then apply \cref{eq:post-nonlin}. This provides us with a recursive procedure to compute the covariances all the way up to the final layer, by sequentially taking limits of $\layerC\ell\to\infty$.


% \begin{algorithm}[tb]
% \begin{algorithmic}[1]
% %\STATE \emph{Input}: two images, $\vX,\vX' \in \mathbb{R}^{C^{(0)} \times (H^{(0)}W^{(0)})}$.
% %\STATE Compute $\bdiagcov_{\vX\vX}^{(1)}$, $\bdiagcov_{\vX\vX'}^{(1)}$, and
% %$\bdiagcov_{\vX'\vX'}^{(1)}$; using \fref{eq:kernel-base}.
% %\FOR{$\ell=1,2,\dots,L$}
% %\STATE Compute $\E[\phi\phi]^{(\ell)}$, $\E[\phi\phi']^{(\ell)}$ and
% %$\E[\phi'\phi']^{(\ell)}$ using equation (\ref{eq:nlin-relu}),
% %(\ref{eq:nlin-erf}), or some other nonlinearity.
% %\STATE Compute $\bdiagcov_{\vX\vX}^{(\ell+1)}$, $\bdiagcov_{\vX\vX'}^{(\ell+1)}$, and
% %$\bdiagcov_{\vX'\vX'}^{(\ell+1)}$; using \fref{eq:kernel-recursive}.
% %\ENDFOR
% %\STATE Output $\diagcov_{\vX\vX'}^{(L+1)}$, which is a nonnegative scalar.
% \State \emph{Input}: two images, $\vX,\vX' \in \mathbb{R}^{\layerC0 \times \layersize{}}$.
% % \State Compute $K_\mu\ssup{1}(\vX, \vX)$, $K_\mu\ssup{1}(\vX, \vX')$, and
% % $K_\mu\ssup{1}(\vX', \vX')$ \\\hspace{2em}for $\mu \in \{1,\dotsc,H\ssup{1}D\ssup{1}\}$; using Eq.~kernel-base.
% \State Compute $K_{pp}^{(1)}(\vX, \vX)$ and $K_{pp}^{(1)}(\vX', \vX')$ for all patch locations $p$ in the input image $\vX$.
% \State Compute $K_{pp'}^{(1)}(\vX, \vX')$ for all pairs of patch locations $p, p'$ in the input images $\vX$ and $\vX'$.
% \For{$\ell=1,2,\dots,L$}
% \State Compute $V_{pp'}^{(\ell)}(\vX, \vX')$ for all pairs of patch locations $p, p'$ in layer $\ell$.
% \State Compute $K_{pp}^{(\ell)}(\vX, \vX)$ and $K_{pp}^{(1)}(\vX', \vX')$ for all patch locations $p$ in the activations of layer $\ell$.
% \State Compute $K_{pp'}^{(\ell)}(\vX, \vX')$ for all pairs of patch locations $p, p'$ in the activations of layer $\ell$.
% % \State Compute $V_\mu\ssup{\ell}(\vX, \vX')$, $V_\mu\ssup{\ell}(\vX, \vX')$ and
% % $V_\mu\ssup{\ell}(\vX,\vX')$ \\\hspace{2em}for $\mu \in \{1,\dotsc,H\ssup{\ell}D\ssup{\ell}\}$; using Eq.~nlin-relu, or some other nonlinearity.
% % \State Compute $K_\mu\ssup{\ell+1}(\vX,\vX)$, $K_\mu\ssup{\ell+1}(\vX,\vX')$, and
% % $K_\mu\ssup{\ell+1}(\vX',\vX')$ \\\hspace{2em}for $\mu \in \{1,\dotsc,H\ssup{\ell+1}D\ssup{\ell+1}\}$; using Eq.~kernel-recursive.
% \EndFor
% \State Output the scalar $K_{11}\ssup{L}(\vX,\vX')$ (the only covariance left after the last layer).
% \end{algorithmic}
% \caption{Computation of correlated weight ConvNet kernel $k(\vX, \vX')$}
% \label{alg:kernel}
% \end{algorithm}

\subsection{Computational Properties: convolutions double in dimensions}
%- Independent layers -- We still need covariances :adria: what did you mean by this?

% The "computational properties" section I thought was to discuss properties of the computational load of computing the kernel. E.g. needing the covariances all the way down the network, because of covariance in the top layer. And that this is computationally expensive.

The core of the kernel computation for convolutional networks, whether or not they have spatial correlations, is the sum over pairs of elements of input patches $qq'$, for each pair of output locations $pp'$ in \cref{eq:kernel-recursion}. For a network that is built with convolutions of 2-dimensional inputs with 2-dimensional \emph{weights}, the sum in \ref{eq:kernel-recursion} is exactly a 4-dimensional convolution of the full second moment of the input distribution (for inputs $\vX$, the outer product), with the 4-dimensional \emph{covariance tensor} of the weights. In general, a $D$-dimensional convolution in weight space corresponds to a $2D$-dimensional convolution in covariance space, with the same strides, dilation and padding.

With this framework, the expression for the covariance of the next layer when using independent weights becomes a 4-d convolution of the activations' second moment with a diagonal 4-d covariance tensor $\priorWcov{\ell}$. This is conceptually simpler, but computationally more complex, than the convolution-like sums over diagonals $pp'$ of \citet{arora2019exact}.

% \subsubsection{Diagonal propagation of the covariance}
%  Consider what is needed to compute the covariance at locationpp′in layer`,K(`)pp′(X,X′). If the202weights are independent (i.e. their covariance isΣ(`)qq′=δqq′σ2w), only the sum terms whereq=q′203are needed. If it is also the case that we only need the diagonal of the locationsK(`)pp′, for example204if the output only has one location, then we only need the diagonalK(`−1)qq′of the layer below.  If205the weight matrices of all the layers are independent, this propagates down the network until the206input, which implies that the covariance computation can be expressed as a 2-d convolution. This207was already noted by [Novak et al., 2019; Garriga-Alonso et al., 2019]
% It is less well-known that the diagonal propagation also applies when not all the weights are independent. If we need $n$ diagonals for the covariance of layer $\ell$ (that is, $q-q'$ has $n$ possible values at $\ell$), and the weights $\layerW{\ell}$ are independent, then we only need $n$ diagonals for layer $\ell-1$, regardless of its size. This also propagates down the network, so long as the weights are independent. It also enables considerable computational savings for some architectures. For example, consider the ResNet-32 architecture \citep{he2016deep,garriga2018infiniteconv} which has mean pooling on a $8\times 8$ activation at the end. Since the reduction in size is created by convolutions with stride 2, if they have independent weights, we only need to consider an $8\times 8 \times 8 \times 8$ covariance tensor $K_{pp'}(\vX, \vX')$, as opposed to a $32 \times 32 \times 32 \times 32$, for a $256\times$ computational savings.

\subsection{Implementation}
We extend the \texttt{neural-tangents} \citep{neuraltangents2020} library with a convolution layer and a read-out layer, that admit a 4-dimensional covariance tensor for the weights. This allows interoperation with existing operators.
% 4-dimensional convolutions are expressible in most common deep learning frameworks (Jax, Pytorch, Tensorflow). However, these libraries only implement 4-d convolutions in the CPU, because they are a very uncommon operation. We express the 4-d convolution with a size $k \times k \times k \times k$ convolutional kernel as a sum over $d$ 3-d convolutions \citep{funke4dconv}, which can be accelerated using the GPU. Because convolutional kernels are usually small ($\le 5 \times 5$), this still provides a speedup over the CPU.

4d convolutions are uncommon in deep learning, so our implementation uses a sum over $d$ 3-d convolutions. While this enables GPU acceleration, computing the kernel is a computationally costly operation. Reproducing our results takes around 10 days using an nVidia RTX 2070 GPU. Access to computational resources limited our experiments to subsets of data on CIFAR-10.

\section{Experiments}
We seek to test two hypotheses. 1) Can we eliminate architectural choices, and
recover their effect using continuous hyperparameters instead? 
2) In the additional search space we have uncovered, can we find a kernel that
performs better than the existing ones?

\paragraph{Experimental setup} We evaluate various models on class-balanced subsets of CIFAR-10 of size $2^i \cdot 10$, following \citep{arora2020small}. As is
standard practice in the wide network literature, we reframe classification as
regression to one-hot targets $Y_{nk} = \indicator{t_n = k} - C$. We set $C=0.1$ to make the mean over the data set zero, but we observed that this affects the results very little. The test
predictions are the argmax over $k$ of the posterior Gaussian process mean
\begin{equation}f(x_*) = \vK_{x_* \vX}\bra{\sigma^2\eye + \vK_{\vX\vX}}^{-1}\vY_{[:k]} \label{eq:mean-gp}\end{equation}, where
$\sigma^2$ is a hyperparameter, the variance of the observation noise of the GP regression. 

\paragraph{Implementation} To calculate the least-squares regression solution, common practice is to use the Cholesky decomposition of $\bra{\sigma^2\eye + \vK_{xx}}$ and a triangular solve. Since we need to cross-validate over many settings of $\sigma$, this would be costly, as it would require recomputation.
% For our purposes, this has two disadvantages. First, it needs to be solved once for every different $\sigma^2$ that we try during cross-validation. Additionally, because of floating point rounding errors, $\vK_{xx}$ often has small negative eigenvalues, in which case the Cholesky decomposition needs to be attempted several times, usually with exponentially increasing values of $\sigma^2$.
Instead we use the eigendecomposition $\vK_{xx} = \vQ\vLambda\vQ^\tp$, calculated once, we solve both of these problems. The least-squares regression solution (GP mean) is $\vK_{*x}\vQ\bra{\sigma^2\eye + \vLambda}^{-1}\vQ^\tp\vY$, which only requires solving for a diagonal matrix, and an inner product. This allows us to search over many values of $\sigma$ very quickly. This also tells us what the smallest value of $\sigma^2$ has to be to make the kernel matrix positive definite.

For all experiments, we find the maximum cross-validation accuracy over 1000 values of $\log \sigma^2$ linearly spaced in $[\max(c, -36), 5]$, where $e^c$ is the smallest value such that $e^c + \lambda_i > 0$ in floating-point arithmetic, and $\lambda_i$ is the smallest eigenvalue.

\begin{figure}[htpb]
 \scalebox{1.0}{\input{fig/myrtle10.pgf}}
  \caption{Cross-validation accuracy of the CNNGP-14 and Myrtle10 networks on subsets of CIFAR10, with varying lengthscale of the Matérn-$3/2$ kernel that determines the weight correlation in the last layer. With larger data set sizes $N$, the improvement is larger, and the optimal lengthscale converges to a similar value $(\approx 4)$. For all data sets except the largest, the values are averaged over several runs, and the thin lines represent the $\pm 2\sigma_n$, the estimated standard deviation of the mean. \label{ref:fig-last-layer}}
\end{figure}

\begin{figure}[htpb]
  \scalebox{1.0}{\input{fig/internal_correlation.pgf}}.
  \caption{Correlated weights in intermediate layers. We replace pooling layers in the Myrtle10 architecture with larger convolutional filters with correlated weights. The amount of correlation is varied along the x-axis. \label{ref:fig-all-layers}}
\end{figure}


\subsection{Correlated weights in the last layer}

We start with two architectures used in the neural network kernel literature, the CNN-GP \citep{novak2019infiniteconv,arora2019exact} with 14 layers, and the Myrtle network \citep{shankar2020without} with 10 layers. The CNNGP-14 architecture $((\texttt{conv}, \texttt{relu})\times 14, \texttt{pool})$ has a $32 \times 32$-sized layer at the end, which is usually transformed into the $1 \times 1$ output using mean pooling. The Myrtle10 architecture $(((\texttt{conv},\texttt{relu})\times 2, \texttt{pool}_{2\times 2}) \times 3, \texttt{pool})$ has a $8\times 8$ pooling layer at the end.

We replace the final pooling layers with a layer with correlated weights. Following \citep{dutordoir2020} covariance $\vSigma_{pp'}$ of the weights is given by the Matérn-3/2 kernel \citep{gpml} with lengthscale $\lambda$:
\begin{equation}
    \vSigma_{pp'} = \bra{1 + \frac{\sqrt{3}||p - p'||_2}{\lambda}} \exp\bra{-\frac{\sqrt{3}||p - p'||_2}{\lambda}}.
\end{equation}
Note that $p,p'$ are 2-d vectors representing patch locations. The ``extremes'' of independent weights and mean pooling are represented by setting $\vSigma_{pp'} = \delta_{pp'}$ and $\vSigma_{pp'} = {\boldsymbol 11}^\tp$, respectively.

In figure~\ref{ref:fig-last-layer}, we investigate how the 4-fold cross-validation accuracy on data sets of size $N=2^i \cdot 10$ varies with the lengthscale $\lambda$ of the Matérn-3/2 kernel, which controls the ``amount'' of spatial correlation in the weights of the last layer. For each data point in each line, we split the data set into 4 folds, and we calculate the test accuracy on 1 fold using the other 3 as training set, for each value of $\sigma$ that we try. We take the maximum accuracy over $\sigma$.

We investigate how the effect above varies with data set size. As the data set grows larger, we observe that the advantage of having a structured covariance matrix in the output becomes more apparent. We can also see the optimal lengthscale, $\lambda$, converging to a similar value, of $\approx 4$, which is evidence for the hypothesis holding with more data. 

The largest data set size in each part of the plot was run only once because of computational constraints. We transform one data set of size $N$ into two data sets of size $N/2$ by taking block diagonals of the stored kernel matrix, so we have more runs for the smallest sizes. This is a valid Monte-Carlo estimate of the true accuracy under the data distribution, with less variance than independent data sets, because the data sets taken are anti-correlated, since they have no points in common. We have at least 1 other independent run for the second smallest data set size.

\subsection{Correlated weights in intermediate layers}
\vspace{-0.3cm}
We take the same approach to the experiment in figure~\ref{ref:fig-all-layers}. This time, we replace the $2\times 2$ intermediate mean-pooling layer, together with the next $3\times 3$ convolution layer, in the Myrtle10 architecture with correlated weights. We change it to a $6 \times 6$ weight-correlated matrix. We observe that when the lengthscale is 0, the performance of the network is poor, suggesting that the mean-pooling layers in Myrtle10 are necessary. Additionally, we are able to recover the performance of the hand-tuned architecture by varying our lengthscale parameter.


\section{Conclusion}
\vspace{-0.3cm}
The disappearance of spatial correlations in infinitely wide limits of deep convolutional neural networks could be seen as another example of how Gaussian processes lose favourable properties of neural networks. While other work sought to remedy this problem by making architectural changes, e.g.~through mean-pooling, we showed that changing the weight prior could achieve the same effect. Our work has three main consequences.
Firstly, weight correlation shows that locally connected models (without spatial correlation) and mean-pooling architectures (with spatial correlation) actually exist at ends of a spectrum. This unifies the two views in the neural network domain. We also unify to known convolutional architectures that were introduced from the Gaussian process community.
Secondly, we show empirically that modest performance improvements can be gained by using weight correlations \emph{between} the extremes of locally connected networks or mean-pooling. We also show that the performance of mean-pooling in intermediate layers can be matched by weight correlation.
Finally, using weight correlation may provide advantages during hyperparameter tuning. Discrete architectural choices need to be searched through simple evaluation, while continuous parameters can use gradient-based optimisation. While we have not taken advantage of this in our current work, this may be a fruitful direction for future research.

% Implications
% - Continuous parameter
% - Bayesian priors





\section*{Broader Impact}
Neural networks are a key component in current machine learning and artificial intelligence methods. In this paper we study theoretical properties of neural networks. We do this by considering the properties of networks which grow to an infinite size, which are easier to study. Our results give insights into how neural networks should grow while retaining some of their good properties. This can help practitioners design networks which perform better. In addition, our results are also relate to Bayesian inference in neural networks, which is seen as a way to improve the reliability of machine learning systems by allowing them to understand their own uncertainty. Our results can help understand how to design better representations of uncertainty.

In short, this paper contributes to the understanding of neural network behaviour, and Bayesian inference in them. We hope that this understanding will help create more powerful, and more robust methods. Robustness is often seen as a requirement for application of machine learning to safety critical tasks, like in medical domains. However, ultimately the impact of this work will happen indirectly through how machine learning is applied.



\begin{ack}
Use unnumbered first level headings for the acknowledgments. All acknowledgments
go at the end of the paper before the list of references. Moreover, you are required to declare 
funding (financial activities supporting the submitted work) and competing interests (related financial activities outside the submitted work). 
More information about this disclosure can be found at: \url{https://neurips.cc/Conferences/2020/PaperInformation/FundingDisclosure}.


Do {\bf not} include this section in the anonymized submission, only in the final paper. You can use the \texttt{ack} environment provided in the style file to autmoatically hide this section in the anonymized submission.
\end{ack}

\printbibliography

\newpage

% \section{Notes}


% Given the activations $A$ of layer $\ell$, the preactivations $Z$ of layer $\ell+1$ are the result of the convolution 
% \begin{equation}
% Z\ssup{\ell+1}_{i,\mu}(\vX) = \sum_{j=1}^{C\ssup{\ell}} \sum_{\nu \in \patchfun{\mu}} W\ssup{\ell}_{i,j,\nu}\, A\ssup{\ell}_{j,\nu}(\vX).
% \label{eq:def-nn-convolution}
% \end{equation}
% We have abstracted the definition of the convolution by expressing it as a sum over elements of image patches. The activations are
% \begin{align}
%     A\ssup{0}_{i,\mu}(\vX) &= X_{i,\mu}  & A\ssup{\ell}_{i,\mu} &= \phi\ssup{\ell}(Z\ssup{\ell}_{i, \mu}(\vX)) \text{ for $\ell>0$},
% \label{eq:def-nn-activations}
% \end{align}
% where $\phi\ssup{\ell}$ is an elementwise activation function.

% \subsection{Independent Weights}

% Previous work \citep{garriga2018infiniteconv, novak2019infiniteconv} proceeds by drawing each individual weight from an independent Gaussian distribution with zero mean
% \begin{equation}
%     W\ssup{\ell}_{i,j,\nu} \simiid \NormDist{0, \sigma_\text{w}^2/C\ssup{\ell}}.
%     \label{eq:w-independent-dist}
% \end{equation}

% Then, the mean and covariance functions of the corresponding GP are the mean and covariance of the NN evaluated at inputs $\vX, \vX'$. The mean $m\ssup{\ell}_{i,\mu} = \ExpSymb\sqb{Z\ssup{\ell}_{i,\mu}} = 0$, because $\ExpSymb\sqb{W\ssup{\ell}_{i,j,\nu}}=0$. The covariance function $K$ is more involved:
% \begin{equation}
%     K\ssup{\ell+1}_{i,i',\mu,\mu'}(\vX, \vX') %= \ExpSymb\sqb{Z_{i,\mu}(\vX)\, Z_{i',\mu'}\bra{\vX'}} = \ExpSymb\sqb{}
%     = \sum_{j=1}^{C\ssup{\ell}} \sum_{j'=1}^{C\ssup{\ell}} \sum_{\nu \in \patchfun{\mu}} \sum_{\nu' \in \patchfun{\mu'}} \ExpSymb\sqb{
%     W\ssup{\ell}_{i,j,\nu} W\ssup{\ell}_{i',j',\nu'}} \ExpSymb\sqb{A\ssup{\ell}_{j,\nu}(\vX) A\ssup{\ell}_{j',\nu'}(\vX')}.
%   \label{eq:cov-independent}
% \end{equation}
% Looking at equation~\cref{eq:w-independent-dist}, the uncentered covariance of $W$ is $\ExpSymb\sqb{W\ssup{\ell}_{i,j,\nu}W\ssup{\ell}_{i',j',\nu'}} = \delta_{i,i'} \, \delta_{j, j'} \, \delta_{\nu, \nu'}\, \sigma_\text{w}^2/C\ssup{\ell}$. Thus, we can remove the double sum over $j$, and each output channel $i$ is independent of the others. The result is 
% \begin{align}
%     K\ssup{\ell+1}_{i,\mu,\mu}(\vX, \vX') %= \ExpSymb\sqb{Z_{i,\mu}(\vX)\, Z_{i',\mu'}\bra{\vX'}} = \ExpSymb\sqb{}
%     &= \sum_{j=1}^{C\ssup{\ell}} \sum_{\nu \in \patchfun{\mu}} \sum_{\nu' \in \patchfun{\mu'}} \delta_{\nu,\nu'} \frac{\sigma^2_\text{w}}{C\ssup{\ell}}\,
%     \ExpSymb\sqb{A\ssup{\ell}_{j,\nu}(\vX) A\ssup{\ell}_{j,\nu}(\vX')} \\
%     &= \sum_{\nu \in \patchfun{\mu}} \sum_{\nu' \in \patchfun{\mu'}} \delta_{\nu,\nu'} \sigma^2_\text{w}\,
%       \ExpSymb\sqb{A\ssup{\ell}_{\cdot,\nu}(\vX) A\ssup{\ell}_{\cdot,\nu}(\vX')} 
% \end{align}

% If additionally we are only interested in the diagonal ($\mu=\mu'$), then this
% becomes a just a convolution:
% \begin{equation*}
%     K\ssup{\ell+1}_{i,\mu,\mu}(\vX, \vX') %= \ExpSymb\sqb{Z_{i,\mu}(\vX)\, Z_{i',\mu'}\bra{\vX'}} = \ExpSymb\sqb{}
%     = \sum_{j=1}^{C\ssup{\ell}} \sum_{\nu \in \patchfun{\mu}}  \ExpSymb\sqb{A\ssup{\ell}_{j,\nu}(\vX) A\ssup{\ell}_{j,\nu}(\vX')}.
% \end{equation*}

% Thus, the variance at the next layer is a convolution of the same specification as the NN.

% \subsection{Correlated Weights}
% Here, we place a Gaussian prior on the weights that is not independent. The weights
% corresponding to each input (output) channel $j$ ($i$) have joint covariance
% $\Sigma_\text{w}/C\ssup{\ell}$
% \begin{equation}
%   \vecfun{\vW\ssup{\ell}_{i,j}} \simiid \NormDist{\boldsymbol{0}, \Sigma_\text{w}/C\ssup{\ell}}.
%   \label{eq:w-correlated-dist}
% \end{equation}
% Crucially, the weights are still independent across channels. Intuitively this
% implies that, in the infinite width limit, each channel in layer $\ell+1$ is a
% sum of infinitely many independent channels from layer $\ell$. Thus, a random
% convolutional network initialized with these weights still converges in distribution
% to a GP.

% \begin{lemma}
%   Consider a convolutional neural network defined as in
%   \cref{eq:def-nn-convolution,eq:def-nn-activations}, with weights drawn from a
%   zero-mean Gaussian correlated within each channel, as in \cref{eq:w-correlated-dist}.
%   Set the number of channels to $C\ssup{\ell} = n$ for each layer $\ell$. Then,
%   as $n\to \infty$, each output channel $\vZ\ssup{L}_i(\vX)$ of the network converges in
%   distribution to an independent Gaussian process.
% \end{lemma}
% \begin{proof}
%   We express the network using the \textsc{Netsor} programming language due to
%   \citet{yang2019wide}. See \cref{app:netsor}.
% \end{proof}

% We can obtain an expression for the covariance $K\ssup{\ell}_{i,\mu,\mu'}(\vX,
% \vX')$ starting from \cref{eq:cov-independent}. This time, however,
% $\ExpSymb\sqb{W\ssup{\ell}_{i,j,\nu}W\ssup{\ell}_{i',j',\nu'}} = \delta_{i,i'} \, \delta_{j, j'} \, \Sigma_{\text{w}}\bra{\nu,\nu'}/C\ssup{\ell}$.

  
% \begin{itemize}
%     \item Derivation with correlated weights.
% \end{itemize}
% Summary of this: D-dimensional convolutions of NN weights become 2D-dimensional convolutions of covariance tensors. 

% Highlight the differences and the different computational choices we need to make.

% We could have taken another path to generalize TICK to multiple layers.

% What choice of covariance do we make ?
% This was inspired by TICK

% Representation as sum of diagonals

% \input{supplementary.tex}

\end{document}
