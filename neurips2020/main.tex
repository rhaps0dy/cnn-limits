\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2020

% ready for submission
% \usepackage{neurips_2020}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2020}

% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2020}

% to avoid loading the natbib package, add option nonatbib:
\usepackage[nonatbib]{neurips_2020}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Useful packages
% \usepackage{subcaption}
\usepackage{xcolor}
\usepackage{amsmath}
\usepackage{amsfonts}

% Theorem environments
\usepackage{amsthm}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}

% Algorithms
\usepackage{algorithm}
\usepackage{algpseudocode}

\usepackage{graphicx}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}
\usepackage{paralist}
\usepackage{cleveref}
\creflabelformat{equation}{#2#1#3}
% \usepackage{listings}
% \lstset{
%     showstringspaces=false,
%     basicstyle=\ttfamily,
%     breaklines=true,
%     breakatwhitespace=true,
% }
% \usepackage{booktabs}
\usepackage{cancel}
\usepackage{import}  % Inkscape figures

\usepackage[colorinlistoftodos]{todonotes}
\newcommand{\markcomment}[1]{\todo[color=green]{#1}\xspace}
\newcommand{\adriacomment}[1]{\todo[color=blue]{#1}\xspace}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Bibliography
\usepackage{MarkBiblatexCmds}
\addbibresource{ref.bib}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Mathematical commands
\usepackage{MarkMathCmds}

\newcommand{\vX}{\mathbf{X}}
\newcommand{\vW}{\mathbf{W}}
\newcommand{\vZ}{\mathbf{Z}}
\newcommand{\vA}{\mathbf{A}}
\newcommand{\vK}{\mathbf{K}}
\newcommand{\vL}{\mathbf{L}}
\newcommand{\vSigma}{\boldsymbol{\Sigma}}
\newcommand{\eye}{\mathbf{I}}

\newcommand{\patchfun}[1]{{#1}\text{th patch}}
\newcommand{\vecfun}{\text{vec}\bra}
\newcommand{\simiid}{\overset{i.i.d.}{\sim}}

\newcommand{\tp}{{\mathrm{\textsf{\tiny T}}}}
\newcommand{\vbar}{{\,|\,}}

% Brackets and super/sub -indices
\newcommand{\bracket}[3]{{\left#1 #3 \right#2}}
\newcommand{\bra}{\bracket{(}{)}}
\newcommand{\cb}{\bracket{\{}{\}}}
\newcommand{\sqb}{\bracket{[}{]}}
\newcommand{\abs}{\bracket{|}{|}}
\newcommand{\ceil}{\bracket{\lceil}{\rceil}}
\newcommand{\floor}{\bracket{\lfloor}{\rfloor}}
\DeclareMathOperator*{\ExpOperator}{\ExpSymb}

\newcommand{\ssup}[1]{^{\bra{#1}}}
\usepackage{dsfont}
\newcommand{\indicator}[1]{{\mathds{1}}\left[#1\right]}



\title{Correlated Weights in Infinite Limits \\ of Deep Convolutional Neural Networks}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

\author{%
  Adri√† Garriga-Alonso \\
  University of Cambridge \\
  \texttt{ag919@cam.ac.uk} \\
  % examples of more authors
   \And
   Mark van der Wilk \\
   Imperial College London \\
   \texttt{m.vdwilk@imperial.ac.uk} \\
  % \AND
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
}

\begin{document}

\maketitle

\begin{abstract}
Infinite width limits of deep neural networks often have tractable forms. They have been used to analyse the behaviour of finite networks, as well as being useful methods in their own right. Currently used limits of deep convolutional networks lose correlating contributions from different spatial locations in the image, unlike their finite counterparts, even when those only use convolutions. We argue that this is undesirable, and remedy it by introducing spatial correlations in the prior over weights. This leads to correlated contributions being preserved in the wide limit. Varying the amount of correlation in the convolution weights allows interpolation between independent-weight limits and mean pooling (which is equivalent to complete correlation in the weights). Empirical evaluation of the infinitely wide network shows that optimal performance is achieved between the extremes, indicating the usefulness of considering correlations in the weights.
\end{abstract}

% Thoughts
% - Number of filters / neurons is said to be related to the capacity of the neural network to learn. Well, what happens if we increase capacity to infinity?
% - We should discuss both NTK and "GP" equivalent


\section{Introduction}
% Paragraph 1
% - NN theory
% - Infinite limits are often more tractable
% - Infinite limits have given understanding, and new methods
% - However, we lose certain properties
Neural networks have been enormously successful at learning relationships through minimising some loss on a training set, starting from a random initialisation. While a full theoretical explanation of the success of neural networks is still lacking, analysis of the random initialisation has proven to be useful for providing both insight and practically useful methods.
In particular, limits where the width of layers is taken to infinity have been particularly crucial, as these correspond to linear basis function models, which are well-understood mathematically.
\citet{neal1996bayesian} first noted such a relationship, through the correspondence between infinitely wide Bayesian neural networks and Gaussian processes (GPs). The GP representation greatly simplified Bayesian learning, leading to its widespread adoption \citep{williams1996gpr,gpml}.

The success of GPs raised the question of whether a mathematically simple linear model could replace a complex neural network. \citet{mackay1998introgp} noted that in taking the infinite limit, the feature representation had become fixed. Given that learnable features are a key desirable property of neural networks which is lost due to the infinite limit, MacKay inquired: ``have we thrown the baby out with the bath water?'' Recent renewed interest in GP correspondences for deep networks \citep{matthews2018dnnlimit,lee2018dnnlimit} show that it is still unclear to what degree neural networks exhibit GP behaviour.

In this work we investigate a property that, similarly to feature learning, is present in finite convolutional neural networks, but which disappears in the infinite limit: correlation from patches in different parts of the image \citep{garriga2018infiniteconv,novak2019infiniteconv}. Given that convolutions were developed to introduce these correlations, it seems undesirable that they are lost when more filters are added.
Currently, changing the architecture by introducing mean-pooling is the only way
that these correlations have been reintroduced \citep{novak2019infiniteconv},
and they have been shown to be important for improving performance
\citep{arora2019exact}. We show that correlations between patches can also be
maintained in the limit without pooling. Instead, we introduce correlations
between the weights in the prior. The amount of correlation can be controlled,
which allows us to interpolate between the existing approaches of full
independence and mean-pooling. Our approach allows the discrete architectural choice of mean-pooling to be replaced with a more flexible continuous amount of correlation. Empirical evaluation on CIFAR-10 shows that this additional flexibility improves performance.

Our work illustrates how choices that are made in the prior affect properties of the limit, and that good choices can improve performance. The success of this approach in the infinite limit also raises questions about whether correlated weights should be used in finite networks.



% \begin{itemize}
%     \item Motivate interest in infinitely wide neural networks
%     \begin{itemize}
%         \item Understand properties of neural networks (feature space, training dynamics)
%         \item Simplify Bayesian inference (infinite width makes GP)
%         \item Should read some of the other infinite width papers for more points
%     \end{itemize}
%     \item Mention that many different NN's have a GP equivalent, including CNNs.
%     \item Introduce problem:
%     \begin{itemize}
%         \item Not long after the uncovering of infinite-width correspondences [Neal 1996], limitations were discovered. MacKay baby bathwater.
%         \item Here we argue that a similar effect can be seen in infinite limits of CNNs. Independence over the weights removes correlations over distant patches.
%     \end{itemize}
%     \item Introduce our contribution:
%     \begin{itemize}
%         \item We investigate the effect of patch correlations on the performance of infinite-width models.
%         \item We show that a different prior over weights re-introduces correlations.
%         \item We discuss how mean-pooling (does max-pooling do this too? \adriacomment{yes, but nobody uses it in wide CNNs because nobody has bothered to figure out the expression for propagating the covariance forward}) has a similar effect, explaining why it has been used in other recent papers.
%         \item We show how the correlated prior allows us to interpolate between the independent limit, and mean-pooling limits, an that often something in the middle performs best.
%     \end{itemize}
%     \item Speculate on what's next:
%     \begin{itemize}
%         \item Perhaps finite-width networks should go beyond independent initialisations?
%     \end{itemize}
% \end{itemize}

\section{Related work}
Infinitely wide limits of neural networks are currently an important tool for creating approximations and analyses. Here we provide a background on the different infinite limits that have been developed, together with a brief overview of where they have been applied.

Interest in infinite limits first started with research into properties of Bayesian priors on the weights of neural networks. \citet{neal1996bayesian} noted that prior function draws from a single hidden layer neural network with appropriate Gaussian priors on the weights tended to a Gaussian process as the width grew to infinity. The simplicity of performing Bayesian inference in Gaussian process models led to their widespread adoption soon after \citep{williams1996gpr,gpml}. Over the years, the wide limits of networks with different weight priors and activation functions have been analysed, leading to various \emph{kernels} which specify the properties of the limiting Gaussian processes \citep{williams1997inf,cho2009mkm}.

With the increasing prominence of deep learning, recursive kernels were introduced in an attempt to obtain similar properties. \citet{cho2009mkm,mairal2014ckn} investigated such methods for fully-connected and convolutional architectures respectively. Despite similarities between recursive kernels and neural networks, the derivation did not provide clear relationships, or any equivalence in a limit. \citet{hazan2015} took initial steps to showing the wide limit equivalence of a neural network beyond the single layer case. Recently, \citet{matthews2018dnnlimit,lee2018dnnlimit} simultaneously provided general results for the convergence of the prior of deep fully-connected networks to a GP.\footnote{The derivation of the limiting kernel differs between the two papers, with the results being consistent. \citet{matthews2018dnnlimit} carefully take limits of realisable networks, while \citet{lee2018dnnlimit} take the infinite limit of each layer sequentially.%Follow-on work \citep{matthews2018dnnlimit2,novak2019infiniteconv} further relax technical restrictions.
} 
A different class of limiting kernels, the Neural Tangent Kernel (NTK), originated from analysis of the function implied by a neural network during optimisation \citep{jacot2018ntk}, rather than the prior implied by the weight initialisation. Just like the Bayesian prior limit, this kernel sheds light on certain properties of neural networks, as well as providing a method with predictive capabilities of its own. 
The two approaches end up with subtly different kernels, which both can be computed as a recursive kernel.

With the general tools in place, \citet{garriga2018infiniteconv,novak2019infiniteconv} derived limits of the prior of convolutional neural networks with infinite filters. %\footnote{This time with \citet{garriga2018infiniteconv} using the simpler sequential infinite limits, and \citet{novak2019infiniteconv} blah blah}
These two papers directly motivated this work by noting that spatial correlations disappeared in the infinite limit. Spatial mean pooling at the last layer was suggested as one way to recover correlations, with \citet{novak2019infiniteconv} providing initial evidence of its importance. Due to computational constraints, they were limited to using a Monte Carlo approximation to the limiting kernel, while \citet{arora2019exact} performed the computation with the exact NTK. Very recent preprints provide follow-on work that pushes the performance of limit kernels \citep{shankar2020without} and demonstrated the utility of limit kernels for small data tasks \citep{arora2020small}. Extending on the results for convolutional architectures, \citet{yang2019wide} showed how infinite limits could be derived for a much wider range of network architectures.

In the kernel and Gaussian process community, kernels with convolutional structure have also been proposed. Notably, these retained spatial correlation in either a fixed \citep{vdw2017convgp} or adjustable \citep{mairal2014ckn,dutordoir2020} way. While these methods were not derived using an infinite limit, \Citet{vdw2019thesis} provided an initial construction from an infinitely wide neural network limit. Inspired by these results, we propose limits of deep convolutional neural networks which retain spatial correlation in a similar way.



% This needs to be good, and I will have to rely on you to a large degree Adri√†.
% \begin{itemize}
%     \item Summarise work along the line of infinite-width models and results
%     \begin{itemize}
%         \item Early work, Neal. Single hidden layer
%         \item More recent work. Matthews 2018 has good references
%         \item Noticing specialised architectures: Convolutions. Adri√† and Brain.
%         \item Generalising: Greg Yang's paper showing how things can be done for general architectures, Neural Tangents package, ...
%         \item NTK: A different kind of infinite limit relying on optimisation rather than Bayes rule.
%         \item Recent uses of infinite width. Ben Recht, Arora.
%     \end{itemize}
%     \item Summarise parallel advancements in Gaussian processes (few sentences). ConvGP, TickGP. ``In parallel, there has been work on directly constructing Gaussian processes with properties that are known to be useful in NNs. In analogy, ConvGP has been shown to be infinite limit of single-layer convgp.''
% \end{itemize}


\section{Spatial Correlations in Single-Layer Networks}
\subsection{Notation}
To begin, we will analyse the infinite limit of a single hidden layer
convolutional neural network. We first show how spatial correlations disappear
when using infinite weights \citep{garriga2018infiniteconv,novak2019infiniteconv}, and how mean pooling reintroduces them \citep{novak2019infiniteconv}. This leads to kernels with the same structure as \Citet{vdw2017convgp}. Next, we show how spatial correlations can be recovered by adding weight correlations instead of making the discrete architectural modification of adding mean pooling. This leads to the 
``Translation Insensitive Convolutional Kernel'' (TICK) considered by \citet{dutordoir2020}. In the next section, we will generalise this to deep convolutional networks.

\newcommand{\layerC}[1]{C^{(#1)}}
\newcommand{\layerw}[1]{P^{(#1)}}
\newcommand{\layerh}[1]{Q^{(#1)}}
\newcommand{\layersize}[1]{\layerw{#1}\layerh{#1}}
\newcommand{\patchw}[1]{p^{(#1)}}
\newcommand{\patchh}[1]{q^{(#1)}}
\newcommand{\patchsize}[1]{\patchw{#1}\patchh{#1}}
\newcommand{\patchidx}{\nu}
\newcommand{\W}[1]{\vW}
\newcommand{\layerWs}[1]{W^{(#1)}}
\newcommand{\layerW}[1]{\vW^{(#1)}}
\newcommand{\priorWcov}[1]{\boldsymbol{\Sigma}^{(#1)}}
\newcommand{\priorWcovs}[1]{\Sigma^{(#1)}}
\newcommand{\layerAs}[2]{A^{\!(#1)}_{#2}\!(\vX)}
\newcommand{\layerAsd}[2]{A^{\!(#1)}_{#2}\!(\vX')}
\newcommand{\layerNLAs}[2]{Z^{(#1)}_{#2}\!(\vX)}
\newcommand{\layerNLAsd}[2]{Z^{(#1)}_{#2}\!(\vX')}
\newcommand{\layerA}[1]{\vA^{\!(#1)}\!(\vX)}
\newcommand{\layerAd}[1]{\vA^{\!(#1)}\!(\vX')}
% These 4 are used for the picture below
\newcommand{\chan}{c}
\newcommand{\prevchan}{\gamma}   % <- Possibly use these for c, p, gamma? I've
\newcommand{\patch}{p}               % been using \nu,\mu
\newcommand{\nextpatch}{\mu} 
A single-layer Convolutional Neural Network (CNN) takes in an image input $\vX \in \Reals^{\layerC{0}\times \layersize{0}}$ with width $\layerw{0}$, height $\layerh{0}$, and $\layerC{0}$ channels (e.g.~one per colour). The image is divided up into patches of size $\layerC{0}\times\!\patchw{1}\times\!\patchh{1}$, giving a total of $\layersize{0}$ zero-padded patches. Weights are applied by taking an inner product with all patches, which we do $\layerC{1}$ times to give multiple channels in the next layer. By collecting all weights in the matrix $\layerW{1}\in\Reals^{\layerC1\times \layerC0\times \patchw1\patchh1}$ we can denote the computation of the pre- and post-non-linearity activations as
\begin{align}
    \layerAs{1}{cp} = \sum_{\gamma=1}^{\layerC0}\sum_{p} X_{\gamma p} \layerWs1_{c\gamma p} \,, && \layerNLAs{1}{cp} = \phi\big(\layerAs{1}{cp}\big)\,.
\end{align}
In a single-layer CNN, these activations are followed by a fully-connected layer with weights $\layerW{2} \in \Reals^{\layerC 1\times \layerw0\layerh0}$. Our final output is again given by a summation reduction over the activations:
\begin{align}
    f(\vX) = \sum_{c=1}^{\layerC1}\sum_{p} \phi\big(\layerAs{1}{cp}\big) W^{(2)}_{cp} = \sum_{cp} \layerNLAs{1}{cp}\layerWs{2}_{cp} = \sum_p \layerAs{f}{p} \label{eq:single-layer-f} \,,
\end{align}
where $\layerAs{f}{p}$ denotes the result before the summation over the patch locations $p$. 

We analyse the distribution on function outputs $f(\vX)$ for some Gaussian prior on the weights $p(\vW)$, where $\vW$ is the collection of the weights at all layers. In all the cases we consider, we take the prior to be independent over layers and channels. Here we extend on earlier work by allowing spatial correlation in the final layer's weights (we will consider all layers later) through the covariance matrix $\priorWcov1 \in \Reals^{\layersize0\times\layersize0}$. This gives the prior
\begin{align}
    p(\layerW1) = \prod_{c'=0}^{\layerC0}\prod_{c=1}^{\layerC1}\NormDist{\layerW1_{[cc':]}; 0, \eye} \,,
    && p(\layerW2) = \prod_{c=1}^{\layerC1} \NormDist{\layerW2_{c:}; 0, \frac{1}{\layerC1}\priorWcov2} \,.
\end{align}
Independence between channels makes $\layerA{f}$ a sum of i.i.d.~multivariate random variables, which allows us to apply the central limit theorem (CLT) as $\layerC1\to\infty$ \citep{neal1996bayesian}.
\begin{align}
    \mathbb{C}_{\vW}\left[\layerAs{f}{p},\layerAsd{f}{p'}\right] &= \Exp{\vW}{\sum_{c=1}^{\layerC1}\sum_{c'=1}^{\layerC1} \layerNLAs{1}{cp}\layerWs{2}_{cp}\layerNLAsd{1}{c'p'}\layerWs{2}_{c'p'}} \nonumber \\
    &= \sum_{c=1}^{\layerC1}\sum_{c'=1}^{\layerC1} \Exp{\layerW1}{\layerNLAs{1}{cp}\layerNLAsd{1}{c'p'}}\Exp{\layerW2}{\layerWs{2}_{cp} \layerWs{2}_{c'p'}} \nonumber \\
    &= \sum_{c=1}^{\layerC1}\sum_{c'=1}^{\layerC1} \Exp{\layerW1}{\layerNLAs{1}{cp}\layerNLAsd{1}{c'p'}}\delta_{cc'}\priorWcovs{2}_{pp'} \nonumber \\
    &= \sum_{c=1}^{\layerC1} \Exp{\layerW1}{\layerNLAs{1}{cp}\layerNLAsd{1}{cp'}}\priorWcovs{2}_{pp'} = k^{(1)}_{p}(\vX^{[p]}, \vX^{[p]}) \priorWcovs{2}_{pp'}
\end{align}
We find the kernel for the GP by taking the covariance between function values $f(\vX)$ and $f(\vX')$ and performing the final sum in \cref{eq:single-layer-f}:
\begin{align}
    k(\vX, \vX') = \mathbb{C}\left[f(\vX), f(\vX')\right] = \sum_{pp'} k^{(1)}_{p}(\vX^{[p]}, \vX^{[p]}) \priorWcovs{2}_{pp'} \,.
\end{align}
We can now see how different choices for $\priorWcov2$ give different forms of spatial correlation.

\paragraph{Independent} \citet{garriga2018infiniteconv,novak2019infiniteconv} consider $\priorWcovs{2}_{pp'} = \delta_{pp'}$, i.e.~the case where all weights are independent. The kernel resulting kernel simply sums components over patches, which implies an \emph{additive model} \citep{stone1985}, where different functions that each operate on subsets of the input are summed together: $f(\vX) = \sum_p f_p(\vX^{[p]})$. This structure has commonly applied to improve GP performance in high-dimensional settings \citep[e.g.][]{duvenaud2011additive,durrande2012additive}. \citet{novak2019infiniteconv} point out that the same kernel can be obtained by taking an infinite limit of a \emph{locally connected network} (LCN) \citep{lecun1989generalization} where connectivity is the same as in a CNN, but without weight sharing.

\paragraph{Mean-pooling} By taking $\priorWcovs2_{pp'} = 1$ we make the weights fully correlated over all locations, leading to identical weights for all $p$, i.e.~$\layerWs{2}_{cp} = \layerWs{2}_{c}$. This is equivalent to taking the mean response over all spatial locations (\cref{eq:single-layer-f}), or mean-pooling. As \citet{novak2019infiniteconv} discuss, this reintroduces the spatial correlation that is the intended result of weight sharing. This case is identical to the ``translation invariant'' construction in \Citet{vdw2017convgp}. Their experiments show that mean-pooling is too restrictive in this single-layer case. To remedy this, they perform a pooling with a learned weight ${\alpha_p}$ for each patch, which can be recovered by taking $\priorWcovs{2}_{pp'} = \alpha_p \alpha_{p'}$. These weights need to be learned, which departs slightly from the strict Bayesian framework.

\paragraph{Spatially correlated weights} We can also choose to vary the strength of correlation between weights based on their distance in the image, through e.g.~an exponential decay depending on the distance between two patches: $\priorWcovs{2}_{pp'} = \exp(-d(p, p')/\ell)$. This construction exactly gives the ``translation insensitive convolutional kernel'' discussed in \citet{dutordoir2020}. In contrast to weighted pooling, this construction only requires tuning the lengthscale $\ell$ as the single hyperparameter.




% Following \citet{neal1996bayesian} we note that $\layerAs{f}_p$ is a sum of $\layerC1$ i.i.d.~multivariate random variables, which converges to a multivariate Gaussian as $\layerC1\to\infty$ by the central limit theorem. 

% We introduce $\layerAs{f}$ to denote the result before the summation over the patch locations $p$:

% We analyse the distribution on function outputs $f(\vX)$ for some Gaussian prior on the weights $p(\vW)$, where $\vW$ is the collection of the weights at all layers. In all cases that we analyse, the prior is constrained to be independent over layers and channels. In this 


% Since $\layerAs1_{cp}$ and $\layerWs2_{cp}$ are both independent over $c$, $f(X)$ is a s

% Since $f(X)$ is a sum of  \adriacomment{To write the kernel, we need to make
%   clear the dependence of $\layerAs{\cdot}$ and $\layerNLAs{\cdot}$ on $\vX$.}

\subsection{Notes}

Give a high-level commentary on the infinite-width line of work. Summarise commentary on this line of work. As far as I see, the main point here is MacKay's point on the baby and the bathwater, and who else has discussed it (I know Matthews has).
\begin{itemize}
    \item Note MacKay's point about disappearing correlations. Have we thrown out the baby with the bathwater?
    \item Note that both \citet{garriga2018infiniteconv} and \citet{novak2019infiniteconv} show that correlations disappeard in infinite limit.
    \item Ask question: Is this really what we want? The whole point of CNNs was to have spatial correlations in different areas of the image!
    \item Compare and contrast to additive models in GPs and convolutional models in GPs, which were introduce specifically to gain these correlations.
    \item Discuss mean-pooling as a way to get back correlations. Raise point that this is a discrete non-learnable architectural choice!
    \item Note that \citet{vdw2019thesis} briefly discusses how ConvGP is an infinite limit.
    \item Can we construct an infinite limit of a CNN that has the correlations like the ConvGP, but also works for full deep networks?
\end{itemize}



\section{Infinite limits for correlated weights\label{sec:correlated-weights}}

As a summary: $N$-dimensional convolutions in weight space correspond to $2N$-dimensional convolutions in kernel space.

\begin{figure}
  % Width: 397.9pt
  \centering
  % \def\svgwidth{\textwidth}
  \import{fig/}{net_diagram.pdf_tex}
  \caption{A convolutional neural network. Indices $\prevchan,\chan,\patch,\nextpatch$
    are depicted as used in expressions that focus on layer 1.}
  \label{fig:fancy-cnn}
\end{figure}

Here we extend the analysis of the previous section to mulitple layers. 

In this section we derive the kernel corresponding to an infinitely wide convolutional neural network (CNN) with correlated weights. Our analysis applies to all numbers of dimensions of convolutions, and more generally all settings where "groups" of weights would be applied to different inputs \adriacomment{make precise using Greg Yang's stuff}.


Given the activations $A$ of layer $\ell$, the preactivations $Z$ of layer $\ell+1$ are the result of the convolution 
\begin{equation}
Z\ssup{\ell+1}_{i,\mu}(\vX) = \sum_{j=1}^{C\ssup{\ell}} \sum_{\nu \in \patchfun{\mu}} W\ssup{\ell}_{i,j,\nu}\, A\ssup{\ell}_{j,\nu}(\vX).
\label{eq:def-nn-convolution}
\end{equation}
We have abstracted the definition of the convolution by expressing it as a sum over elements of image patches. The activations are
\begin{align}
    A\ssup{0}_{i,\mu}(\vX) &= X_{i,\mu}  & A\ssup{\ell}_{i,\mu} &= \phi\ssup{\ell}(Z\ssup{\ell}_{i, \mu}(\vX)) \text{ for $\ell>0$},
\label{eq:def-nn-activations}
\end{align}
where $\phi\ssup{\ell}$ is an elementwise activation function.

\subsection{Independent Weights}

Previous work \citep{garriga2018infiniteconv, novak2019infiniteconv} proceeds by drawing each individual weight from an independent Gaussian distribution with zero mean
\begin{equation}
    W\ssup{\ell}_{i,j,\nu} \simiid \NormDist{0, \sigma_\text{w}^2/C\ssup{\ell}}.
    \label{eq:w-independent-dist}
\end{equation}

Then, the mean and covariance functions of the corresponding GP are the mean and covariance of the NN evaluated at inputs $\vX, \vX'$. The mean $m\ssup{\ell}_{i,\mu} = \ExpSymb\sqb{Z\ssup{\ell}_{i,\mu}} = 0$, because $\ExpSymb\sqb{W\ssup{\ell}_{i,j,\nu}}=0$. The covariance function $K$ is more involved:
\begin{equation}
    K\ssup{\ell+1}_{i,i',\mu,\mu'}(\vX, \vX') %= \ExpSymb\sqb{Z_{i,\mu}(\vX)\, Z_{i',\mu'}\bra{\vX'}} = \ExpSymb\sqb{}
    = \sum_{j=1}^{C\ssup{\ell}} \sum_{j'=1}^{C\ssup{\ell}} \sum_{\nu \in \patchfun{\mu}} \sum_{\nu' \in \patchfun{\mu'}} \ExpSymb\sqb{
    W\ssup{\ell}_{i,j,\nu} W\ssup{\ell}_{i',j',\nu'}} \ExpSymb\sqb{A\ssup{\ell}_{j,\nu}(\vX) A\ssup{\ell}_{j',\nu'}(\vX')}.
  \label{eq:cov-independent}
\end{equation}
Looking at equation~\cref{eq:w-independent-dist}, the uncentered covariance of $W$ is $\ExpSymb\sqb{W\ssup{\ell}_{i,j,\nu}W\ssup{\ell}_{i',j',\nu'}} = \delta_{i,i'} \, \delta_{j, j'} \, \delta_{\nu, \nu'}\, \sigma_\text{w}^2/C\ssup{\ell}$. Thus, we can remove the double sum over $j$, and each output channel $i$ is independent of the others. The result is 
\begin{align}
    K\ssup{\ell+1}_{i,\mu,\mu}(\vX, \vX') %= \ExpSymb\sqb{Z_{i,\mu}(\vX)\, Z_{i',\mu'}\bra{\vX'}} = \ExpSymb\sqb{}
    &= \sum_{j=1}^{C\ssup{\ell}} \sum_{\nu \in \patchfun{\mu}} \sum_{\nu' \in \patchfun{\mu'}} \delta_{\nu,\nu'} \frac{\sigma^2_\text{w}}{C\ssup{\ell}}\,
    \ExpSymb\sqb{A\ssup{\ell}_{j,\nu}(\vX) A\ssup{\ell}_{j,\nu}(\vX')} \\
    &= \sum_{\nu \in \patchfun{\mu}} \sum_{\nu' \in \patchfun{\mu'}} \delta_{\nu,\nu'} \sigma^2_\text{w}\,
      \ExpSymb\sqb{A\ssup{\ell}_{\cdot,\nu}(\vX) A\ssup{\ell}_{\cdot,\nu}(\vX')} 
\end{align}

If additionally we are only interested in the diagonal ($\mu=\mu'$), then this
becomes a just a convolution:
\begin{equation*}
    K\ssup{\ell+1}_{i,\mu,\mu}(\vX, \vX') %= \ExpSymb\sqb{Z_{i,\mu}(\vX)\, Z_{i',\mu'}\bra{\vX'}} = \ExpSymb\sqb{}
    = \sum_{j=1}^{C\ssup{\ell}} \sum_{\nu \in \patchfun{\mu}}  \ExpSymb\sqb{A\ssup{\ell}_{j,\nu}(\vX) A\ssup{\ell}_{j,\nu}(\vX')}.
\end{equation*}

Thus, the variance at the next layer is a convolution of the same specification as the NN.

\subsection{Correlated Weights}
Here, we place a Gaussian prior on the weights that is not independent. The weights
corresponding to each input (output) channel $j$ ($i$) have joint covariance
$\Sigma_\text{w}/C\ssup{\ell}$
\begin{equation}
  \vecfun{\vW\ssup{\ell}_{i,j}} \simiid \NormDist{\boldsymbol{0}, \Sigma_\text{w}/C\ssup{\ell}}.
  \label{eq:w-correlated-dist}
\end{equation}
Crucially, the weights are still independent across channels. Intuitively this
implies that, in the infinite width limit, each channel in layer $\ell+1$ is a
sum of infinitely many independent channels from layer $\ell$. Thus, a random
convolutional network initialized with these weights still converges in distribution
to a GP.

\begin{lemma}
  Consider a convolutional neural network defined as in
  \cref{eq:def-nn-convolution,eq:def-nn-activations}, with weights drawn from a
  zero-mean Gaussian correlated within each channel, as in \cref{eq:w-correlated-dist}.
  Set the number of channels to $C\ssup{\ell} = n$ for each layer $\ell$. Then,
  as $n\to \infty$, each output channel $\vZ\ssup{L}_i(\vX)$ of the network converges in
  distribution to an independent Gaussian process.
\end{lemma}
\begin{proof}
  We express the network using the \textsc{Netsor} programming language due to
  \citet{yang2019wide}. See \cref{app:netsor}.
\end{proof}

We can obtain an expression for the covariance $K\ssup{\ell}_{i,\mu,\mu'}(\vX,
\vX')$ starting from \cref{eq:cov-independent}. This time, however,
$\ExpSymb\sqb{W\ssup{\ell}_{i,j,\nu}W\ssup{\ell}_{i',j',\nu'}} = \delta_{i,i'} \, \delta_{j, j'} \, \Sigma_{\text{w}}\bra{\nu,\nu'}/C\ssup{\ell}$.

  
\begin{itemize}
    \item Derivation with correlated weights.
\end{itemize}
Summary of this: D-dimensional convolutions of NN weights become 2D-dimensional convolutions of covariance tensors. 

Highlight the differences and the different computational choices we need to make.

We could have taken another path to generalize TICK to multiple layers.

What choice of covariance do we make ?
This was inspired by TICK

Representation as sum of diagonals

\section{Experiments}
We seek to test two hypotheses. 1) Can we eliminate architectural choices, and
recover their effect using continuous hyperparameters instead? 
2) In the additional search space we have uncovered, can we find a kernel that
performs better than the existing ones, or is the optimum to be found at one of
the existing points? \adriacomment{reword}

% \paragraph{Experiment setup} We evaluate these models on CIFAR-10. Following
% standard practice in the wide network literature, we reframe classification as
% regression to one-hot targets $t_{nk} = \indicator{y_n = k} - 0.1$. The test
% predictions are $\vK_{x*x}\bra{\sigma^2\eye + \vK_{xx}}^{-1}\vY$, where
% $\sigma^2$ is the variance of the observation noise of the Gaussian process
% regression. It is common practice in the GP community to invert
% $\bra{\sigma^2\eye + \vK_{xx}}$ using the Cholesky decomposition and a
% triangular solve. Contrary to this, we use the eigendecomposition of $\vK_{xx}$
% to quickly search over 1000 values of $\sigma^2$, from
% $\text{exp}\bra{\text{max}(-\lambda_1, -36)}$ in a log-scale $e^5$, using cross-validation.



\paragraph{Correlated weights in the last layer.} The 


We use plain convolutional networks in the 

We investigate how the effect above varies with data set size.



\section{Conclusion}
We show
\begin{itemize}
    \item ...
\end{itemize}

"Wev'e identified a new property that is helpful in the infinite limit. Would correlated weights be useful for finite neural netwokrs as well?"

the argument about "if you increase the size of your images" 

Mention Wenzel et al. SGLD

\section*{Broader Impact}

Authors are required to include a statement of the broader impact of their work, including its ethical aspects and future societal consequences. 
Authors should discuss both positive and negative outcomes, if any. For instance, authors should discuss a) 
who may benefit from this research, b) who may be put at disadvantage from this research, c) what are the consequences of failure of the system, and d) whether the task/method leverages
biases in the data. If authors believe this is not applicable to them, authors can simply state this.

Use unnumbered first level headings for this section, which should go at the end of the paper. {\bf Note that this section does not count towards the eight pages of content that are allowed.}

\begin{ack}
Use unnumbered first level headings for the acknowledgments. All acknowledgments
go at the end of the paper before the list of references. Moreover, you are required to declare 
funding (financial activities supporting the submitted work) and competing interests (related financial activities outside the submitted work). 
More information about this disclosure can be found at: \url{https://neurips.cc/Conferences/2020/PaperInformation/FundingDisclosure}.


Do {\bf not} include this section in the anonymized submission, only in the final paper. You can use the \texttt{ack} environment provided in the style file to autmoatically hide this section in the anonymized submission.
\end{ack}

\printbibliography

\newpage

\input{supplementary.tex}

\end{document}
