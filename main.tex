\documentclass{article} % For LaTeX2e
\usepackage{iclr2020_conference,times}

\usepackage{amssymb,amsthm}
\usepackage{adria-fancyref}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\newcommand{\bracket}[3]{{\left#1 #3 \right#2}}
\newcommand{\bra}{\bracket{(}{)}}
\newcommand{\cb}{\bracket{\{}{\}}}
\newcommand{\sqb}{\bracket{[}{]}}
\newcommand{\abs}{\bracket{|}{|}}

\newcommand{\ssup}[1]{^\bra{#1}}
\newcommand{\GP}[1]{{\text{GP}\bra{#1}}}

\newcommand{\eqdef}{\triangleq}
\newcommand{\tp}{\mathsf{T}}

\newcommand{\vbar}{\,|\,}
\newcommand{\mvbar}{\,\middle |\,}
\newcommand{\patch}[1]{{{#1}\text{th patch}}}

\usepackage{hyperref}
\usepackage{url}
\usepackage{import}
\usepackage{graphicx}


\title{The Baby and the Bathwater: 
Desirable Properties in Infinite Limits of CNNs}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Antiquus S.~Hippocampus, Natalia Cerebro \& Amelie P. Amygdale \thanks{ Use footnote for providing further information
about author (webpage, alternative address)---\emph{not} for acknowledging
funding agencies.  Funding acknowledgements go at the end of the paper.} \\
Department of Computer Science\\
Cranberry-Lemon University\\
Pittsburgh, PA 15213, USA \\
\texttt{\{hippo,brain,jen\}@cs.cranberry-lemon.edu} \\
\And
Ji Q. Ren \& Yevgeny LeNet \\
Department of Computational Neuroscience \\
University of the Witwatersrand \\
Joburg, South Africa \\
\texttt{\{robot,net\}@wits.ac.za} \\
\AND
Coauthor \\
Affiliation \\
Address \\
\texttt{email}
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\begin{abstract}
  We phrase other existing convolutional GPs in terms of NN limits. We try to
  save the baby, yet throw out some of the dirty bathwater.
\end{abstract}

The network takes an arbitrary input image $\mX$ of height $H\ssup{0}$ and
width $D\ssup{0}$, as a $C\ssup{0} \times (H\ssup{0}
D\ssup{0})$ real matrix. 
% Let $\vX \in \reals^{C^{(0)} \times (H^{(0)} \cdot W^{(0)})}$ be an input
% image to the network.
Each row, which we denote $\vx_1,\vx_2,\dotsc,\vx_{C\ssup{0}}$, corresponds to a channel
of the image (e.g. $C\ssup{0} = 3$ for RGB), flattened to form a 
vector.

\section{Convolutional GPs}

In convolutional GPs \cite{markvdw2017convolutional} the prior over functions is
\begin{equation}
  f(\mX) = \sum_{p=1}^P u_p g(\mX^\sqb{p}) \text{ with } g \sim \GP{0, k(\vx^\sqb{p}, \vx^\sqb{p'})}.
\end{equation}
This leads to the kernel 
\begin{equation}
\Cov\sqb{f(\vx), f(\vx')} = \sum_{p=1}^P\sum_{p'=1}^P u_p u_{p'} k(\vx^\sqb{p}, \vx^\sqb{p'})
\end{equation}
Let the postactivations be $\mZ\ssup{0}(\mX) \eqdef \mX$, and
$\mZ\ssup{\ell}(\mX) \eqdef \phi\bra{\mA\ssup{\ell}(\mX)}$. Let
$\mA\ssup{\ell}(\mX)$ be the preactivations for an input $\mX$. Given some
constant weights ${u_1, \dots, u_{D\ssup{L}}}$, where $D\ssup{L}$ is the size of
the convolutional postactivations at the previous to last layer, we can define a
network

\begin{align}
  % \emA_{i,\mu}\ssup{1}(\mX) &\eqdef b_i\ssup{1} + \sum_{j=1}^{C\ssup{0}}\sum_{\nu \in \patch{\mu}}\etW\ssup{1}_{i,j,\mu,\nu}\,\emX_{j,\nu}
  % \label{eq:convgp-network-base} \\
  \emA_{i,\mu}\ssup{\ell+1}(\mX) &\eqdef \evb_i\ssup{\ell+1} +
  \sum_{j=1}^{C\ssup{\ell}} \ \sum_{\nu\in\patch{\mu}} \etW\ssup{\ell+1}_{i,j,\mu,\nu}\, \emZ\ssup{\ell}_{j,\nu}(\mX).
  \label{eq:convgp-network-recursive} \\
  \emA_{i,\mu}\ssup{L+1}(\mX) &\eqdef \evb_i\ssup{L+1} +
                                   \sum_{j=1}^{C\ssup{L}} \ \sum_{\nu=1}^{D\ssup{L}} u_\nu \etW\ssup{L+1}_{i,j,\mu,\nu}\, \emZ\ssup{L}_{j,\nu}(\mX).
                                   \label{eq:convgp-network-final}
\end{align}

Sadly when we take the covariance of \fref{eq:convgp-network-final}, we find
(remember $\mu=1$ always because the output is a scalar)
that $\E\sqb{u_\nu\etW\ssup{L+1}_{i,j,\mu,\nu}\; u_{\nu'}\etW\ssup{L+1}_{i',j',\mu,\nu'}} =
\frac{1}{C\ssup{L}}\delta_{ii'}\delta_{jj'}\delta_{\nu\nu'}u_\nu^2$.

To  obtain the desired
$\frac{1}{C\ssup{L}}\delta_{ii'}\delta_{jj'}u_\nu u_{\nu'}$, we need to make the
covariance matrix of $\tW\ssup{L+1}_{i,j,\mu, :}$ be $\delta_{ii'}\delta_{jj'}\vu\vu^\tp$
(recall \fref{fig:conv-as-linear}). This outer product is positive definite but
singular, so the Gaussian is degenerate. But that's fine because we sum it up in
the end.


\begin{figure}%[ht]
  \centerline{\subimport{img/}{convolution.pdf_tex}}
  \caption{The 2D convolution $\emU\ssup{0}_{i,j} * \vx_j$ as the dot product
    $\emW\ssup{0}_{i,j} \vx_j$. The blank elements of $\emW\ssup{0}_{i,j}$ are zeros.
    The $\mu$th row of $\emW\ssup{0}_{i,j}$ corresponds to applying the filter to the
    $\mu$th convolutional patch of the channel $\vx_j$. \label{fig:conv-as-linear}}
\end{figure}
\section{TICK-GPs}

\bibliography{iclr2020_conference}
\bibliographystyle{iclr2020_conference}

\end{document}
