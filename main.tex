\documentclass{article} % For LaTeX2e
\usepackage{iclr2020_conference,times}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amssymb,amsthm}
\usepackage{adria-fancyref}
\usepackage{textcomp}

% Remove for final rendering
\usepackage{geometry}
\geometry{paperwidth=\dimexpr\textwidth+3mm,
  paperheight=\dimexpr\textheight+3mm,
  margin=1.5mm}
  
 \newcommand{\marknote}[1]{{\color{red}#1}}
 \newcommand{\adrianote}[1]{{\color{blue}#1}}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\newcommand{\bracket}[3]{{\left#1 #3 \right#2}}
\newcommand{\bra}{\bracket{(}{)}}
\newcommand{\cb}{\bracket{\{}{\}}}
\newcommand{\sqb}{\bracket{[}{]}}
\newcommand{\abs}{\bracket{|}{|}}
\newcommand{\ceil}{\bracket{\lceil}{\rceil}}
\newcommand{\floor}{\bracket{\lfloor}{\rfloor}}

\newcommand{\ssup}[1]{^{#1}}
\newcommand{\GP}[1]{{\text{GP}\bra{#1}}}

\newcommand{\eqdef}{\triangleq}
\newcommand{\tp}{\mathsf{T}}

\newcommand{\vbar}{\,|\,}
\newcommand{\mvbar}{\,\middle |\,}

% Image of a map
\DeclareMathOperator{\Ima}{Im}

% Patch function
\newcommand{\patch}[2]{{p\ssup{#1}_{#2}}}
\newcommand{\p}[2]{{\patch{\ell #1}{\nu #2}\bra{\lambda}}}

% Dimensions of different things at different layers
% Images
\newcommand{\Iw}[1]{{I\ssup{#1}_\text{w}}}
\newcommand{\Iwl}[1]{\Iw{\ell #1}}
\newcommand{\Ih}[1]{{I\ssup{#1}_\text{h}}}
\newcommand{\Ihl}[1]{\Ih{\ell #1}}

% Strides
\newcommand{\Sw}[1]{{S\ssup{#1}_\text{w}}}
\newcommand{\Swl}[1]{\Sw{\ell #1}}
\newcommand{\Sh}[1]{{S\ssup{#1}_\text{h}}}
\newcommand{\Shl}[1]{\Sh{\ell #1}}

% Filters
\newcommand{\Fw}[1]{{F\ssup{#1}_\text{w}}}
\newcommand{\Fwl}[1]{\Fw{\ell #1}}
\newcommand{\Fh}[1]{{F\ssup{#1}_\text{h}}}
\newcommand{\Fhl}[1]{\Fh{\ell #1}}

%Channels
\newcommand{\chan}[1]{{C\ssup{#1}}}
\newcommand{\chanl}[1]{{C\ssup{\ell #1}}}


\usepackage{hyperref}
\usepackage{url}
\usepackage{import}
\usepackage{graphicx}

% Theorem environments
\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{remark}[theorem]{Remark}

\title{The Baby and the Bathwater: 
Desirable Properties in Infinite Limits of CNNs}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Antiquus S.~Hippocampus, Natalia Cerebro \& Amelie P. Amygdale \thanks{ Use footnote for providing further information
about author (webpage, alternative address)---\emph{not} for acknowledging
funding agencies.  Funding acknowledgements go at the end of the paper.} \\
Department of Computer Science\\
Cranberry-Lemon University\\
Pittsburgh, PA 15213, USA \\
\texttt{\{hippo,brain,jen\}@cs.cranberry-lemon.edu} \\
\And
Ji Q. Ren \& Yevgeny LeNet \\
Department of Computational Neuroscience \\
University of the Witwatersrand \\
Joburg, South Africa \\
\texttt{\{robot,net\}@wits.ac.za} \\
\AND
Coauthor \\
Affiliation \\
Address \\
\texttt{email}
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\begin{abstract}
  We phrase other existing convolutional GPs in terms of NN limits. We try to
  save the baby, yet throw out some of the dirty bathwater.
\end{abstract}

The network takes an arbitrary input image $\mX$ of height $\Ih{0}$ and width
$\Iw{0}$, as a $\chan{0} \times I\ssup{0}$ matrix. Each row, which we denote
$\vx_1,\vx_2,\dotsc,\vx_{\chan{0}}$, corresponds to a channel of the image (e.g.
$\chan{0} = 3$ for RGB), flattened to form a vector. Thus, for a layer $\ell$, $I\ssup{\ell} \eqdef
\Ihl{}\Iwl{}$ is the number of elements in a channel of the image or activation.
For a layer $\ell$, the patches taken from it are of height $\Fhl{}$ and width
$\Fwl{}$, with total number of elements $F\ssup{\ell} \eqdef \Fhl{}\Fwl{}$.

\section{Overview \& Motivation}
Our core contribution to modelling is the idea of \emph{weight correlations} in the prior.
\begin{itemize}
    \item This leads to a more general way to retain correlations between outputs from different parts of the image input.
    \item We get a more general infinite-limit kernel than than previously considered. The closest papers are \citet{arora2019exact,li2019enhanced,shankar2020} who consider average pooling. We can interpolate between average pooling and no correlation.
    \item Previous work argues that taking the infinite limit destroys patch correlations. \citet{arora2019exact,li2019enhanced,shankar2020} need to change the architecture to regain the correlations. We show that the correlations can be regained by correlations in the weights, even while taking the infinite limit. Can this argue for different priors in the finite case?
    \item We show theoretical relations to other convolutional GP constructions \citep{markvdw2017convolutional,dutordoir2019tick}.
    \item We can look at correlations in intermediate layers. Mean pooling can conceptually be done in intermediate layers and \adrianote{has been done by \citet{li19ecntk,shankar2020} in the infinite-width setting.}\footnote{There is some old-ish work in computer vision showing that max-pooling in intermediate layers (not mean-) can be substituted for strided convolutions \citep{springenberg14allconv}. Additionally, the current state of the art in ImageNet and CIFAR100  (without extra training data, using established data augmentation schemes) is obtained by EfficientNet \citep{tan19efficientnet}, a variant of ResNet, which only has mean-pooling at the end.} We provide a better justification too \adrianote{are you talking about the convolutional filters being smooth a posteriori here?}.
\end{itemize}
The interestingness of weight correlations depends on how much they improve the overall performance of the kernel.

Another possible strand of enquiry is to improve the \emph{computational} aspects of NN limit kernels through inducing variables. Nobody has looked into this yet.
\begin{itemize}
    \item Inducing patch approximation.
    \item Data augmentations (straightforward augmentation of data) or invariance approach.
\end{itemize}
This will likely give performance a significant improvement.

\section{Possible experiments}
\subsection{Full correlated classification }
Classify CIFAR10 with a wide network with a GP corresponding to a CNN with
correlated filters at every layer. The wide network's architecture is a 20-layer
2016 ResNet \citep{he2016deep}, the state-of-the-art EfficientNet
\citep{tan19efficientnet} is similar in size for a similar depth (but the SoTA
is obtained with >100 layers I think).

\paragraph{Problems:}
\begin{itemize}
  \item Using existing neural-tangents code, this takes upward of 3000 GPU
    hours. We need to calculate the double sum in \fref{eq:full-cov} for every
    layer, and all the summands are non-zero.
  \item Under very optimistic assumptions (16 GB GPU, $32\times 32$ image size,
    keeping 2 buffers for computations) we can process ~40 images at a time
    (much better memory efficiency than neural-tangents, probably not
    computation efficiency.)
  \item We only know how to do an inducing ``patches'' approximation for the last
    layer, and we still 
\end{itemize}

\paragraph{Other considerations:}
\begin{itemize}
  \item The random feature (random NN) approximation to this kernel is as costly
    as a forward pass through a wide-ish NN. Thus, generating more 
    features than data points to get over variance starvation / the first peak
    of double descent \citep{belkin18reconciling} might be possible
  \item Doing inference in the finite Bayesian NN of this, using SGLD, should be
    no harder than inference in an uncorrelated-weights model using SGLD.
\end{itemize}

\subsection{Last-layer-correlated classification}
Classify CIFAR10 with a wide 20-layer network, with correlated weights only at
the last layer

\paragraph{Problems:}
\begin{itemize}
\item Optimistic assumptions say it takes ~600 GPU hours to do this experiment
  exactly (using fancy translation data augmentation).
\end{itemize}

\paragraph{Other considerations:}
\begin{itemize}
  \item We can apply an inducing patch approximation in each layer. Using
    existing neural-tangents code, we can process (in a 16 GB GPU):
    \begin{itemize}
      \item 190k inducing patches + training examples per iteration (when not
        optimizing inducing patches)
      \item 57.6k inducing patches + training examples, when optimizing inducing
        patches.
    \end{itemize}
\end{itemize}

\paragraph{Opportunities:}
Possibly compare the result with just translation data augmentation, using a
model with uncorrelated weights and

\subsection{Qualitatively examine the prior of various NNs}
Using at least the covariance matrix test and the ``number of class assignments
in the prior'' test.

\section{Convolutional GPs}

In convolutional GPs \citep{markvdw2017convolutional} the prior over functions is
\begin{equation}
  f(\mX) = \sum_{\nu=1}^{I\ssup{1}} \evu_\nu g(\mX_{:, p\ssup{0}_\nu}) \text{ with } g \sim \GP{0, k\bra{\mX_{:, p\ssup{0}_\nu}, \mX'_{:, p\ssup{0}_\nu}}}
\end{equation}
This leads to the kernel 
\begin{equation}
  \Cov\sqb{f(\mX), f(\mX')} = \sum_{\nu=1}^{I\ssup{1}}\sum_{\nu'=1}^{I\ssup{1}} \evu_{\nu} \evu_{\nu'} k\bra{\mX_{:, p\ssup{0}_\nu}, \mX'_{:, p\ssup{0}_\nu}}.
\end{equation}

\begin{definition}[Patch function]
  For a layer $\ell$ and patch index $\nu$, define the patch function $p\ssup{\ell}_\nu: [0, F\ssup{\ell})
  \cap \mathbb{N} \to [0, I\ssup{\ell}) \cap \mathbb{N}$.
  The patch function gives us the $\lambda$th element of the $\nu$th patch at layer
  $\ell$. For 2D convolutions, and vertical and horizontal stride sizes $\Shl{}$ and $\Swl{}$,
  its expression is
  \begin{equation}
    \p{}{}{}
    = \bra{\lambda \bmod \Fwl{}} + \Swl{}\bra{\nu\bmod \Iwl{+1}}
    + \Iwl{}\bra{\floor{\frac{\lambda}{\Fwl{}}} + \Shl{} \floor{\frac{\nu}{\Iwl{+1}}}}.
  \end{equation}
  We abuse notation and write just $p\ssup{\ell}_\nu$ to talk about its image, that is,
  the set of positions in the activation at layer $\ell$ that correspond to the $\nu$th patch.
  \label{def:patch-function}
\end{definition}



Let the postactivations be $\mZ\ssup{0}(\mX) \eqdef \mX$, and
$\mZ\ssup{\ell}(\mX) \eqdef \phi\bra{\mA\ssup{\ell}(\mX)}$. Let
$\mA\ssup{\ell}(\mX)$ be the preactivations for an input $\mX$. Given some
constant weights ${u_1, \dots, u_{I\ssup{L}}}$, we can define a network
\begin{align}
  % \emA_{i,\mu}\ssup{1}(\mX) &\eqdef b_i\ssup{1} + \sum_{j=1}^{C\ssup{0}}\sum_{\nu \in \thpatch{\mu}}\etW\ssup{1}_{i,j,\mu,\nu}\,\emX_{j,\nu}
  % \label{eq:convgp-network-base} \\
  \emA_{i,\mu}\ssup{\ell+1}(\mX) &\eqdef \evb_i\ssup{\ell+1} +
  \sum_{j=1}^{C\ssup{\ell}} \ \sum_{\nu\in\patch{\ell}{\mu}} \etW\ssup{\ell+1}_{i,j,\mu,\nu}\, \emZ\ssup{\ell}_{j,\nu}(\mX).
  \label{eq:convgp-network-recursive} \\
  \emA_{i,\mu}\ssup{L+1}(\mX) &\eqdef \evb_i\ssup{L+1} +
                                   \sum_{j=1}^{C\ssup{L}} \ \sum_{\nu=1}^{D\ssup{L}} u_\nu \etW\ssup{L+1}_{i,j,\mu}\, \emZ\ssup{L}_{j,\nu}(\mX).
                                   \label{eq:convgp-network-final}
\end{align}

Note that the tensors $\etW$ in the last layer only have 3 dimensions. Usually
$\mu=1$, because the last layer is fully connected, that is, the output is a
scalar for each channel. The covariance of the weights in the last channel is
$\E\sqb{u_\nu\etW\ssup{L+1}_{i,j,\mu}\ u_{\nu'}\etW\ssup{L+1}_{i',j',\mu}} =
\frac{1}{C\ssup{L}}\delta_{ii'}\delta_{jj'}\evu_\nu \evu_{\nu'}$.

But this presents the problem of computing
$\E\sqb{\emZ\ssup{L}_{i,\mu}(\mX)\, \emZ\ssup{L}_{i,\mu'}(\mX)}$ for $\mu\neq
\mu'$.

\begin{equation}
\begin{aligned}
  \E\sqb{\emA\ssup{L}_{i,\mu}(\mX)\, \emA\ssup{L}_{i,\mu'}(\mX')} &= \sigma_\text{b}^2
  + \sum_{j=1}^{C\ssup{L-1}} \sum_{\nu\in \patch{\ell}{\mu}}\sum_{\nu'\in \patch{\ell}{\mu'}} \E\sqb{\etW_{i,j,\mu,\nu} \etW_{i,j,\mu',\nu'}}\\
  &\hspace{4em}\E\sqb{Z\ssup{L-1}_{j,\nu}(\mX)Z\ssup{L-1}_{j,\nu'}(\mX')} \\
  &= \sigma_\text{b}^2 + \sum_{\nu=1}^P \E\sqb{Z\ssup{L-1}_{j,p_\mu(\nu)}(\mX), Z\ssup{L-1}_{j,p_{\mu'}(\nu)}(\mX')}
\end{aligned}
\end{equation}


\section{TICK-GPs}
In TICK-GPs \citep{dutordoir2019tick}, which are a generalization of Conv-GPs, the kernel is given by
\begin{equation}
\Cov\sqb{f(\vx), f(\vx')} = \sum_{p=1}^P\sum_{p'=1}^P k\bra{\text{loc}\bra{p}, \text{loc}\bra{p'}} k(\vx^\sqb{p}, \vx^\sqb{p'}).
\end{equation}

We construct the same network as before, but the last layer is
\begin{align}
\emA_{i,\mu}\ssup{L+1}(\mX) &\eqdef \evb_i\ssup{L+1} +
\sum_{j=1}^{C\ssup{L}} \ \sum_{\nu=1}^{D\ssup{L}} \etW\ssup{L+1}_{i,j,\mu,\nu}\, \emZ\ssup{L}_{j,\nu}(\mX).
\label{eq:tickgp-network-final}
\end{align}
where the weights are drawn such that their covariance is
$\Cov\sqb{\etW\ssup{L+1}_{i,j,\mu,\nu},\, \etW\ssup{L+1}_{i,j,\mu,\nu'}} =
k(\text{loc}(\nu), \text{loc}(\nu'))$. Note that this is a generalization of
Conv-GPs, if $k(\text{loc}(\nu), \text{loc}(\nu')) = \evu_\nu \evu_{\nu'}$.

\subsection{Kernel for correlated weights}

The output of the network usually has only one spatial location, so we only need to care about its covariance between two inputs. Assuming the mean function is $\E\sqb{\emA_{i,\mu}\ssup{L+1}(\mX)} = 0$, we have that $k(\mX, \mX') = \E\sqb{\emA_{i,\mu}\ssup{L+1}(\mX)\emA_{i,\mu}\ssup{L+1}(\mX')}$. For the TICK-GP this is
\begin{align}
\E\sqb{\emA_{i,\mu}\ssup{L+1}(\mX)\emA_{i,\mu}\ssup{L+1}(\mX')} &=
\sum_{\nu=1}^{I^L} \sum_{\nu'=1}^{I^L} \sum_{j=1}^{C^L} \sum_{j'=1}^{C^L} \E\sqb{\etW\ssup{L+1}_{i,j,\mu,\nu}\etW\ssup{L+1}_{i,j',\mu,\nu'}}
\E\sqb{\emZ\ssup{L}_{j,\nu}(\mX) \emZ\ssup{L}_{j',\nu'}(\mX')} \\
&= \sum_{\nu=1}^{I^L} \sum_{\nu'=1}^{I^L}k(\text{loc}(\nu), \text{loc}(\nu')) V_\phi\bra{\emA\ssup{L}_{\cdot,\nu}(\mX), \emA\ssup{L}_{\cdot,\nu'}(\mX')}.
\end{align}

We can remove the channel double sum $(j, j')$ because the channels are independent, the spatial correlation between weights is given by the location kernel, and the $V_\phi$ functional that takes in a bivariate Gaussian and passes it through an elementwise nonlinearity. We abuse notation here and write it as a function of the random variable. To know these bivariate Gaussians, we need to know the covariances between activations $\emA_{\cdot,\mu}\ssup{L}(\mX)$, with different channels $\mu$ and $\mu'$.

\begin{align}
\E\sqb{\emA_{\cdot,\mu}\ssup{L}(\mX)\emA_{\cdot,\mu'}\ssup{L}(\mX')} &=
\sum_{\nu \in p_\mu^{L-1}} \sum_{\nu'\in p_{\mu'}^{L-1}} \E\sqb{\etW\ssup{L}_{\cdot,\cdot,\mu,\nu}\etW\ssup{L}_{\cdot,\cdot,\mu',\nu'}} V_\phi\bra{\emA\ssup{L-1}_{\cdot,\nu}(\mX), \emA\ssup{L-1}_{\cdot,\nu'}(\mX')}.
\end{align}

In the expression above the tensors $\tW$ are the convolutional filters $\tU$ laid out in ``rows''. Writing the above as a more convenient convolution expression, with a patch of size $F^{L-1}$:
\begin{align}
\E\sqb{\emA_{\cdot,\mu}\ssup{L}(\mX)\emA_{\cdot,\mu'}\ssup{L}(\mX')} &=
\sum_{\lambda=1}^{F^{L-1}} \sum_{\lambda'=1}^{F^{L-1}}
\E\sqb{\etU\ssup{L}_{\cdot,\cdot,\lambda}\etU\ssup{L}_{\cdot,\cdot,\lambda'}}
V_\phi\bra{\emA\ssup{L-1}_{\cdot,p_{\mu}^{L-1}(\lambda)}(\mX), \emA\ssup{L-1}_{\cdot,p_{\mu'}^{L-1}(\lambda')}(\mX')}.
\label{eq:full-cov}
\end{align}

From here it can be seen that, if the elements of $\tU$ are independent, the double sum above can be reduced to a single sum over subsets of the ``diagonals'' of the covariance matrix.

It doesn't seem like a Nyström approximation can save you computation here, since you still have to sum over the entries of the covariance matrix even if it is low rank.
\appendix

\section{Proofs}

\begin{lemma}[Patch covariance depends on diagonals]
  Consider a layer $\ell$ and two patches $\nu$, $\nu'$. For all patch elements $\lambda$, the covariance between the corresponding elements
  $\p{}{}$ and $\p{}{'}$ is in the same diagonal,
  $d$, of the covariance matrix of the filter below. That is,
  $\p{}{} - \p{}{'}$ does not depend on $\lambda$.
  \label{lemma:patches-are-diagonal}
\end{lemma}
\begin{proof}
\begin{align*}
\p{}{}{} - \p{}{'}{}
  &= \bra{\lambda \bmod \Fwl{}} + \Swl{}\bra{\nu\bmod \Iwl{+1}}
  + \Iwl{}\bra{\floor{\frac{\lambda}{\Fwl{}}} + \Shl{} \floor{\frac{\nu}{\Iwl{+1}}}} \\
  &\hspace{1em}- \bra{\lambda \bmod \Fwl{}} - \Swl{}\bra{\nu'\bmod \Iwl{+1}}
    - \Iwl{}\bra{\floor{\frac{\lambda}{\Fwl{}}} + \Shl{} \floor{\frac{\nu'}{\Iwl{+1}}}} \\
  &=\Swl{}\bra{\nu\bmod \Iwl{+1}} +
    \Iwl{}\bra{\Shl{} \floor{\frac{\nu}{\Iwl{+1}}}}
    -\Swl{}\bra{\nu'\bmod \Iwl{+1}} -
    \Iwl{}\bra{\Shl{} \floor{\frac{\nu'}{\Iwl{+1}}}}.
\end{align*}
\end{proof}

{
  \newcommand{\pmu}[2]{{p\ssup{\ell #1}_{\mu #2}\bra{\lambda}}}
\begin{lemma}[Diagonals propagate]
  Consider a layer $\ell$ and two patch indices $\mu$, $\mu'$. For an index
  $\lambda$, $\nu=\pmu{}{}$ and $\nu'=\pmu{}{'}$ are the indices of the
  elements from the $(\ell-1)$th layer that are needed to compute the corresponding
  activations at $\ell-1$. Then, for all $\lambda$ and $\iota$, the pairs
  $p\ssup{\ell-1}_{\pmu{}{}}\bra{\iota}$ and
  $p\ssup{\ell-1}_{\pmu{}{'}}\bra{\iota}$, of the patch covariances needed
  from the layers below are on the same diagonal. That is, 
  $p\ssup{\ell-1}_{\pmu{}{}}\bra{\iota} - p\ssup{\ell-1}_{\pmu{}{'}}\bra{\iota}$
  does not depend on $\iota$ or $\lambda$.
  \label{lemma:diagonal-propagation}
\end{lemma}
\begin{proof}
  From \Fref{lemma:patches-are-diagonal} we know that the result does not depend
  on $\iota$.
  Notice that
  \begin{align}
    \pmu{}{} \bmod \Iwl{} &= \bra{\lambda \bmod \Fwl{}} + \Swl{}\bra{\mu\bmod \Iwl{+1}}\\
    \floor{\frac{\pmu{}{}}{\Iwl{}}} &=
                                          \floor{\frac{\lambda}{\Fwl{}}} + \Shl{} \floor{\frac{\mu}{\Iwl{+1}}}
  \end{align}
  Using this, we can write
  \begin{align*}
    p\ssup{\ell-1}_{\pmu{}{}}\bra{\iota} - p\ssup{\ell-1}_{\pmu{}{'}}\bra{\iota}
    &=\Swl{-1}\bra{\pmu{}{}\bmod \Iwl{}} +
      \Iwl{-1}\bra{\Shl{-1} \floor{\frac{\pmu{}{}}{\Iwl{}}}} \\
      &\hspace{1em}-\Swl{-1}\bra{\pmu{}{'}\bmod \Iwl{}} -
        \Iwl{-1}\bra{\Shl{-1} \floor{\frac{\pmu{}{'}}{\Iwl{}}}} \\
    &= \Swl{-1}\bra{
      \bra{\lambda \bmod \Fwl{}} + \Swl{}\bra{\mu\bmod \Iwl{+1}}
      - 
      \bra{\lambda \bmod \Fwl{}} - \Swl{}\bra{\mu'\bmod \Iwl{+1}}
      } \\
    &= \Iwl{-1}\bra{
      \floor{\frac{\lambda}{\Fwl{-1}}} + \Shl{-1} \floor{\frac{\mu}{\Iwl{}}}
      - \floor{\frac{\lambda}{\Fwl{-1}}} - \Shl{-1} \floor{\frac{\mu'}{\Iwl{}}}
      }
  \end{align*}
  which clearly does not depend on $\lambda$ if we cancel all the equal terms of
  opposite sign.
\end{proof}
}
\begin{remark}
  \Fref{lemma:diagonal-propagation} is not true for general sets of pairs of patches $\nu$ and
  $\nu'$ on the same diagonal in layer $\ell-1$, it is only true if the pair
  comes from a convolution in the layer above.
That is, the elements of one diagonal of the
  covariance matrix of layer $\ell-1$ in general need the elements of more than one
  diagonal of the covariance matrix of layer $\ell-2$ to be computed. It is this
  non-proliferation of diagonals that allows us to compute the kernel in a
  sensible amount of time.
\end{remark}


\bibliography{iclr2020_conference}
\bibliographystyle{iclr2020_conference}


\end{document}
