\documentclass{article} % For LaTeX2e
\usepackage{iclr2020_conference,times}

\usepackage{amssymb,amsthm}
\usepackage{adria-fancyref}

% Remove for final rendering
\usepackage{geometry}
\geometry{paperwidth=\dimexpr\textwidth+3mm,
  paperheight=\dimexpr\textheight+3mm,
  margin=1.5mm}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\newcommand{\bracket}[3]{{\left#1 #3 \right#2}}
\newcommand{\bra}{\bracket{(}{)}}
\newcommand{\cb}{\bracket{\{}{\}}}
\newcommand{\sqb}{\bracket{[}{]}}
\newcommand{\abs}{\bracket{|}{|}}
\newcommand{\ceil}{\bracket{\lceil}{\rceil}}
\newcommand{\floor}{\bracket{\lfloor}{\rfloor}}

\newcommand{\ssup}[1]{^{#1}}
\newcommand{\GP}[1]{{\text{GP}\bra{#1}}}

\newcommand{\eqdef}{\triangleq}
\newcommand{\tp}{\mathsf{T}}

\newcommand{\vbar}{\,|\,}
\newcommand{\mvbar}{\,\middle |\,}

% Image of a map
\DeclareMathOperator{\Ima}{Im}

% Patch function
\newcommand{\patch}[2]{{p\ssup{#1}_{#2}}}
\newcommand{\p}[2]{{\patch{\ell #1}{\nu #2}\bra{\lambda}}}

% Dimensions of different things at different layers
% Images
\newcommand{\Iw}[1]{{I\ssup{#1}_\text{w}}}
\newcommand{\Iwl}[1]{\Iw{\ell #1}}
\newcommand{\Ih}[1]{{I\ssup{#1}_\text{h}}}
\newcommand{\Ihl}[1]{\Ih{\ell #1}}

% Strides
\newcommand{\Sw}[1]{{S\ssup{#1}_\text{w}}}
\newcommand{\Swl}[1]{\Sw{\ell #1}}
\newcommand{\Sh}[1]{{S\ssup{#1}_\text{h}}}
\newcommand{\Shl}[1]{\Sh{\ell #1}}

% Filters
\newcommand{\Fw}[1]{{F\ssup{#1}_\text{w}}}
\newcommand{\Fwl}[1]{\Fw{\ell #1}}
\newcommand{\Fh}[1]{{F\ssup{#1}_\text{h}}}
\newcommand{\Fhl}[1]{\Fh{\ell #1}}

%Channels
\newcommand{\chan}[1]{{C\ssup{#1}}}
\newcommand{\chanl}[1]{{C\ssup{\ell #1}}}


\usepackage{hyperref}
\usepackage{url}
\usepackage{import}
\usepackage{graphicx}

% Theorem environments
\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{remark}[theorem]{Remark}

\title{The Baby and the Bathwater: 
Desirable Properties in Infinite Limits of CNNs}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Antiquus S.~Hippocampus, Natalia Cerebro \& Amelie P. Amygdale \thanks{ Use footnote for providing further information
about author (webpage, alternative address)---\emph{not} for acknowledging
funding agencies.  Funding acknowledgements go at the end of the paper.} \\
Department of Computer Science\\
Cranberry-Lemon University\\
Pittsburgh, PA 15213, USA \\
\texttt{\{hippo,brain,jen\}@cs.cranberry-lemon.edu} \\
\And
Ji Q. Ren \& Yevgeny LeNet \\
Department of Computational Neuroscience \\
University of the Witwatersrand \\
Joburg, South Africa \\
\texttt{\{robot,net\}@wits.ac.za} \\
\AND
Coauthor \\
Affiliation \\
Address \\
\texttt{email}
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\begin{abstract}
  We phrase other existing convolutional GPs in terms of NN limits. We try to
  save the baby, yet throw out some of the dirty bathwater.
\end{abstract}

The network takes an arbitrary input image $\mX$ of height $\Ih{0}$ and width
$\Iw{0}$, as a $\chan{0} \times I\ssup{0}$ matrix. Each row, which we denote
$\vx_1,\vx_2,\dotsc,\vx_{\chan{0}}$, corresponds to a channel of the image (e.g.
$\chan{0} = 3$ for RGB), flattened to form a vector. Thus, for a layer $\ell$, $I\ssup{\ell} \eqdef
\Ihl{}\Iwl{}$ is the number of elements in a channel of the image or activation.
For a layer $\ell$, the patches taken from it are of height $\Fhl{}$ and width
$\Fwl{}$, with total number of elements $F\ssup{\ell} \eqdef \Fhl{}\Fwl{}$.

\section{Convolutional GPs}

In convolutional GPs \citep{markvdw2017convolutional} the prior over functions is
\begin{equation}
  f(\mX) = \sum_{\nu=1}^{I\ssup{1}} \evu_\nu g(\mX_{:, p\ssup{0}_\nu}) \text{ with } g \sim \GP{0, k\bra{\mX_{:, p\ssup{0}_\nu}, \mX'_{:, p\ssup{0}_\nu}}}
\end{equation}
This leads to the kernel 
\begin{equation}
  \Cov\sqb{f(\mX), f(\mX')} = \sum_{\nu=1}^{I\ssup{1}}\sum_{\nu'=1}^{I\ssup{1}} \evu_{\nu} \evu_{\nu'} k\bra{\mX_{:, p\ssup{0}_\nu}, \mX'_{:, p\ssup{0}_\nu}}.
\end{equation}

\begin{definition}[Patch function]
  For a layer $\ell$ and patch index $\nu$, define the patch function $p\ssup{\ell}_\nu: [0, F\ssup{\ell})
  \cap \mathbb{N} \to [0, I\ssup{\ell}) \cap \mathbb{N}$.
  The patch function gives us the $\lambda$th element of the $\nu$th patch at layer
  $\ell$. For 2D convolutions, and vertical and horizontal stride sizes $\Shl{}$ and $\Swl{}$,
  its expression is
  \begin{equation}
    \p{}{}{}
    = \bra{\lambda \bmod \Fwl{}} + \Swl{}\bra{\nu\bmod \Iwl{+1}}
    + \Iwl{}\bra{\floor{\frac{\lambda}{\Fwl{}}} + \Shl{} \floor{\frac{\nu}{\Iwl{+1}}}}.
  \end{equation}
  We abuse notation and write just $p\ssup{\ell}_\nu$ to talk about its image, that is,
  the set of positions in the activation at layer $\ell$ that correspond to the $\nu$th patch.
  \label{def:patch-function}
\end{definition}



Let the postactivations be $\mZ\ssup{0}(\mX) \eqdef \mX$, and
$\mZ\ssup{\ell}(\mX) \eqdef \phi\bra{\mA\ssup{\ell}(\mX)}$. Let
$\mA\ssup{\ell}(\mX)$ be the preactivations for an input $\mX$. Given some
constant weights ${u_1, \dots, u_{I\ssup{L}}}$, we can define a network
\begin{align}
  % \emA_{i,\mu}\ssup{1}(\mX) &\eqdef b_i\ssup{1} + \sum_{j=1}^{C\ssup{0}}\sum_{\nu \in \thpatch{\mu}}\etW\ssup{1}_{i,j,\mu,\nu}\,\emX_{j,\nu}
  % \label{eq:convgp-network-base} \\
  \emA_{i,\mu}\ssup{\ell+1}(\mX) &\eqdef \evb_i\ssup{\ell+1} +
  \sum_{j=1}^{C\ssup{\ell}} \ \sum_{\nu\in\patch{\ell}{\mu}} \etW\ssup{\ell+1}_{i,j,\mu,\nu}\, \emZ\ssup{\ell}_{j,\nu}(\mX).
  \label{eq:convgp-network-recursive} \\
  \emA_{i,\mu}\ssup{L+1}(\mX) &\eqdef \evb_i\ssup{L+1} +
                                   \sum_{j=1}^{C\ssup{L}} \ \sum_{\nu=1}^{D\ssup{L}} u_\nu \etW\ssup{L+1}_{i,j,\mu}\, \emZ\ssup{L}_{j,\nu}(\mX).
                                   \label{eq:convgp-network-final}
\end{align}

Note that the tensors $\etW$ in the last layer only have 3 dimensions. Usually
$\mu=1$, because the last layer is fully connected, that is, the output is a
scalar for each channel. The covariance of the weights inhe last channel is
$\E\sqb{u_\nu\etW\ssup{L+1}_{i,j,\mu}\ u_{\nu'}\etW\ssup{L+1}_{i',j',\mu}} =
\frac{1}{C\ssup{L}}\delta_{ii'}\delta_{jj'}\evu_\nu \evu_{\nu'}$.

But this presents the problem of computing
$\E\sqb{\emZ\ssup{L}_{i,\mu}(\mX)\, \emZ\ssup{L}_{i,\mu'}(\mX)}$ for $\mu\neq
\mu'$.

\begin{equation}
\begin{aligned}
  \E\sqb{\emA\ssup{L}_{i,\mu}(\mX)\, \emA\ssup{L}_{i,\mu'}(\mX')} &= \sigma_\text{b}^2
  + \sum_{j=1}^{C\ssup{L-1}} \sum_{\nu\in \patch{\ell}{\mu}}\sum_{\nu'\in \patch{\ell}{\mu'}} \E\sqb{\etW_{i,j,\mu,\nu} \etW_{i,j,\mu',\nu'}}\\
  &\E\sqb{Z\ssup{L-1}_{j,\nu}(\mX)Z\ssup{L-1}_{j,\nu'}(\mX')} \\
  &= \sigma_\text{b}^2 + \sum_{\nu=1}^P \E\sqb{Z\ssup{L-1}_{j,p_\mu(\nu)}(\mX), Z\ssup{L-1}_{j,p_{\mu'}(\nu)}(\mX')}
\end{aligned}
\end{equation}


\section{TICK-GPs}
In TICK-GPs \citep{dutordoir2019tick}, which are a generalization of Conv-GPs, the kernel is given by
\begin{equation}
\Cov\sqb{f(\vx), f(\vx')} = \sum_{p=1}^P\sum_{p'=1}^P k\bra{\text{loc}\bra{p}, \text{loc}\bra{p'}} k(\vx^\sqb{p}, \vx^\sqb{p'}).
\end{equation}

We construct the same network as before, but the last layer is
\begin{align}
\emA_{i,\mu}\ssup{L+1}(\mX) &\eqdef \evb_i\ssup{L+1} +
\sum_{j=1}^{C\ssup{L}} \ \sum_{\nu=1}^{D\ssup{L}} \etW\ssup{L+1}_{i,j,\mu,\nu}\, \emZ\ssup{L}_{j,\nu}(\mX).
\label{eq:tickgp-network-final}
\end{align}
where the weights are drawn such that their covariance is
$\Cov\sqb{\etW\ssup{L+1}_{i,j,\mu,\nu},\, \etW\ssup{L+1}_{i,j,\mu,\nu'}} =
k(\text{loc}(\nu), \text{loc}(\nu'))$. Note that this is a generalization of
Conv-GPs, if $k(\text{loc}(\nu), \text{loc}(\nu')) = \evu_\nu \evu_{\nu'}$.

\appendix{Proofs}

\begin{lemma}[Patch covariance depends on diagonals]
  Consider a layer $\ell$ and two patches $\nu$, $\nu'$. For all patch elements $\lambda$, the covariance between the corresponding elements
  $\p{}{}$ and $\p{}{'}$ is in the same diagonal,
  $d$, of the covariance matrix of the filter below. That is,
  $\p{}{} - \p{}{'}$ does not depend on $\lambda$.
  \label{lemma:patches-are-diagonal}
\end{lemma}
\begin{proof}
\begin{align*}
\p{}{}{} - \p{}{'}{}
  &= \bra{\lambda \bmod \Fwl{}} + \Swl{}\bra{\nu\bmod \Iwl{+1}}
  + \Iwl{}\bra{\floor{\frac{\lambda}{\Fwl{}}} + \Shl{} \floor{\frac{\nu}{\Iwl{+1}}}} \\
  &\hspace{1em}- \bra{\lambda \bmod \Fwl{}} - \Swl{}\bra{\nu'\bmod \Iwl{+1}}
    - \Iwl{}\bra{\floor{\frac{\lambda}{\Fwl{}}} + \Shl{} \floor{\frac{\nu'}{\Iwl{+1}}}} \\
  &=\Swl{}\bra{\nu\bmod \Iwl{+1}} +
    \Iwl{}\bra{\Shl{} \floor{\frac{\nu}{\Iwl{+1}}}}
    -\Swl{}\bra{\nu'\bmod \Iwl{+1}} -
    \Iwl{}\bra{\Shl{} \floor{\frac{\nu'}{\Iwl{+1}}}}.
\end{align*}
\end{proof}

{
  \newcommand{\pmu}[2]{{p\ssup{\ell #1}_{\mu #2}\bra{\lambda}}}
\begin{lemma}[Diagonals propagate]
  Consider a layer $\ell$ and two patch indices $\mu$, $\mu'$. For an index
  $\lambda$, $\nu=\pmu{}{}$ and $\nu'=\pmu{}{'}$ are the indices of the
  elements from the $(\ell-1)$th layer that are needed to compute the corresponding
  activations at $\ell-1$. Then, for all $\lambda$ and $\iota$, the pairs
  $p\ssup{\ell-1}_{\pmu{}{}}\bra{\iota}$ and
  $p\ssup{\ell-1}_{\pmu{}{'}}\bra{\iota}$, of the patch covariances needed
  from the layers below are on the same diagonal. That is, 
  $p\ssup{\ell-1}_{\pmu{}{}}\bra{\iota} - p\ssup{\ell-1}_{\pmu{}{'}}\bra{\iota}$
  does not depend on $\iota$ or $\lambda$.
  \label{lemma:diagonal-propagation}
\end{lemma}
\begin{proof}
  From \Fref{lemma:patches-are-diagonal} we know that the result does not depend
  on $\iota$.
  Notice that
  \begin{align}
    \pmu{}{} \bmod \Iwl{} &= \bra{\lambda \bmod \Fwl{}} + \Swl{}\bra{\mu\bmod \Iwl{+1}}\\
    \floor{\frac{\pmu{}{}}{\Iwl{}}} &=
                                          \floor{\frac{\lambda}{\Fwl{}}} + \Shl{} \floor{\frac{\mu}{\Iwl{+1}}}
  \end{align}
  Using this, we can write
  \begin{align*}
    p\ssup{\ell-1}_{\pmu{}{}}\bra{\iota} - p\ssup{\ell-1}_{\pmu{}{'}}\bra{\iota}
    &=\Swl{-1}\bra{\pmu{}{}\bmod \Iwl{}} +
      \Iwl{-1}\bra{\Shl{-1} \floor{\frac{\pmu{}{}}{\Iwl{}}}} \\
      &\hspace{1em}-\Swl{-1}\bra{\pmu{}{'}\bmod \Iwl{}} -
        \Iwl{-1}\bra{\Shl{-1} \floor{\frac{\pmu{}{'}}{\Iwl{}}}} \\
    &= \Swl{-1}\bra{
      \bra{\lambda \bmod \Fwl{}} + \Swl{}\bra{\mu\bmod \Iwl{+1}}
      - 
      \bra{\lambda \bmod \Fwl{}} - \Swl{}\bra{\mu'\bmod \Iwl{+1}}
      } \\
    &= \Iwl{-1}\bra{
      \floor{\frac{\lambda}{\Fwl{-1}}} + \Shl{-1} \floor{\frac{\mu}{\Iwl{}}}
      - \floor{\frac{\lambda}{\Fwl{-1}}} - \Shl{-1} \floor{\frac{\mu'}{\Iwl{}}}
      }
  \end{align*}
  which clearly does not depend on $\lambda$ if we cancel all the equal terms of
  opposite sign.
\end{proof}
}
\begin{remark}
  \Fref{lemma:diagonal-propagation} is not true for general sets of pairs of patches $\nu$ and
  $\nu'$ on the same diagonal in layer $\ell-1$, it is only true if the pair
  comes from a convolution in the layer above.
That is, the elements of one diagonal of the
  covariance matrix of layer $\ell-1$ in general need the elements of more than one
  diagonal of the covariance matrix of layer $\ell-2$ to be computed. It is this
  non-proliferation of diagonals that allows us to compute the kernel in a
  sensible amount of time.
\end{remark}


\bibliography{iclr2020_conference}
\bibliographystyle{iclr2020_conference}

\end{document}
