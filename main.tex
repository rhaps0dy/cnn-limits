\documentclass{article} % For LaTeX2e
\usepackage{iclr2020_conference,times}

\usepackage{amssymb,amsthm}
\usepackage{adria-fancyref}

% Remove for final rendering
\usepackage{geometry}
\geometry{paperwidth=\dimexpr\textwidth+3mm,
  paperheight=\dimexpr\textheight+3mm,
  margin=1.5mm}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\newcommand{\bracket}[3]{{\left#1 #3 \right#2}}
\newcommand{\bra}{\bracket{(}{)}}
\newcommand{\cb}{\bracket{\{}{\}}}
\newcommand{\sqb}{\bracket{[}{]}}
\newcommand{\abs}{\bracket{|}{|}}
\newcommand{\ceil}{\bracket{\lceil}{\rceil}}
\newcommand{\floor}{\bracket{\lfloor}{\rfloor}}

\newcommand{\ssup}[1]{^\bra{#1}}
\newcommand{\GP}[1]{{\text{GP}\bra{#1}}}

\newcommand{\eqdef}{\triangleq}
\newcommand{\tp}{\mathsf{T}}

\newcommand{\vbar}{\,|\,}
\newcommand{\mvbar}{\,\middle |\,}
\newcommand{\patch}[1]{{{#1}\text{th patch}}}

\usepackage{hyperref}
\usepackage{url}
\usepackage{import}
\usepackage{graphicx}

% Theorem environments
\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}

\title{The Baby and the Bathwater: 
Desirable Properties in Infinite Limits of CNNs}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Antiquus S.~Hippocampus, Natalia Cerebro \& Amelie P. Amygdale \thanks{ Use footnote for providing further information
about author (webpage, alternative address)---\emph{not} for acknowledging
funding agencies.  Funding acknowledgements go at the end of the paper.} \\
Department of Computer Science\\
Cranberry-Lemon University\\
Pittsburgh, PA 15213, USA \\
\texttt{\{hippo,brain,jen\}@cs.cranberry-lemon.edu} \\
\And
Ji Q. Ren \& Yevgeny LeNet \\
Department of Computational Neuroscience \\
University of the Witwatersrand \\
Joburg, South Africa \\
\texttt{\{robot,net\}@wits.ac.za} \\
\AND
Coauthor \\
Affiliation \\
Address \\
\texttt{email}
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\begin{abstract}
  We phrase other existing convolutional GPs in terms of NN limits. We try to
  save the baby, yet throw out some of the dirty bathwater.
\end{abstract}

The network takes an arbitrary input image $\mX$ of height $H\ssup{0}$ and
width $D\ssup{0}$, as a $C\ssup{0} \times (H\ssup{0}
D\ssup{0})$ real matrix. 
% Let $\vX \in \reals^{C^{(0)} \times (H^{(0)} \cdot W^{(0)})}$ be an input
% image to the network.
Each row, which we denote $\vx_1,\vx_2,\dotsc,\vx_{C\ssup{0}}$, corresponds to a channel
of the image (e.g. $C\ssup{0} = 3$ for RGB), flattened to form a 
vector.

\section{Convolutional GPs}

In convolutional GPs \citep{markvdw2017convolutional} the prior over functions is
\begin{equation}
  f(\mX) = \sum_{p=1}^P u_p g(\mX^\sqb{p}) \text{ with } g \sim \GP{0, k(\vx^\sqb{p}, \vx^\sqb{p'})}.
\end{equation}
This leads to the kernel 
\begin{equation}
\Cov\sqb{f(\vx), f(\vx')} = \sum_{p=1}^P\sum_{p'=1}^P u_p u_{p'} k(\vx^\sqb{p}, \vx^\sqb{p'})
\end{equation}
Let the postactivations be $\mZ\ssup{0}(\mX) \eqdef \mX$, and
$\mZ\ssup{\ell}(\mX) \eqdef \phi\bra{\mA\ssup{\ell}(\mX)}$. Let
$\mA\ssup{\ell}(\mX)$ be the preactivations for an input $\mX$. Given some
constant weights ${u_1, \dots, u_{D\ssup{L}}}$, where $D\ssup{L}$ is the size of
the convolutional postactivations at the previous to last layer, we can define a
network

\begin{align}
  % \emA_{i,\mu}\ssup{1}(\mX) &\eqdef b_i\ssup{1} + \sum_{j=1}^{C\ssup{0}}\sum_{\nu \in \patch{\mu}}\etW\ssup{1}_{i,j,\mu,\nu}\,\emX_{j,\nu}
  % \label{eq:convgp-network-base} \\
  \emA_{i,\mu}\ssup{\ell+1}(\mX) &\eqdef \evb_i\ssup{\ell+1} +
  \sum_{j=1}^{C\ssup{\ell}} \ \sum_{\nu\in\patch{\mu}} \etW\ssup{\ell+1}_{i,j,\mu,\nu}\, \emZ\ssup{\ell}_{j,\nu}(\mX).
  \label{eq:convgp-network-recursive} \\
  \emA_{i,\mu}\ssup{L+1}(\mX) &\eqdef \evb_i\ssup{L+1} +
                                   \sum_{j=1}^{C\ssup{L}} \ \sum_{\nu=1}^{D\ssup{L}} u_\nu \etW\ssup{L+1}_{i,j,\mu}\, \emZ\ssup{L}_{j,\nu}(\mX).
                                   \label{eq:convgp-network-final}
\end{align}

Note that the tensors $\etW$ in the last layer only have 3 dimensions. Remember that
$\mu=1$ always, because the output is a scalar for each channel,
$\E\sqb{u_\nu\etW\ssup{L+1}_{i,j,\mu}\! u_{\nu'}\etW\ssup{L+1}_{i',j',\mu}} =
\frac{1}{C\ssup{L}}\delta_{ii'}\delta_{jj'}\evu_\nu \evu_{\nu'}$.

But this presents the problem of computing
$\E\sqb{\emZ\ssup{L}_{i,\mu}(\mX)\, \emZ\ssup{L}_{i,\mu'}(\mX)}$ for $\mu\neq
\mu'$.

\begin{equation}
\begin{aligned}
  \E\sqb{\emA\ssup{L}_{i,\mu}(\mX)\, \emA\ssup{L}_{i,\mu'}(\mX')} &= \sigma_\text{b}^2
  + \sum_{j=1}^{C\ssup{L-1}} \sum_{\nu\in \patch{\mu}}\sum_{\nu'\in \patch{\mu'}} \E\sqb{\etW_{i,j,\mu,\nu} \etW_{i,j,\mu',\nu'}}\\
  &\E\sqb{Z\ssup{L-1}_{j,\nu}(\mX)Z\ssup{L-1}_{j,\nu'}(\mX')} \\
  &= \sigma_\text{b}^2 + \sum_{\nu=1}^P \E\sqb{Z\ssup{L-1}_{j,p_\mu(\nu)}(\mX), Z\ssup{L-1}_{j,p_{\mu'}(\nu)}(\mX')}
\end{aligned}
\end{equation}



\begin{figure}%[ht]
  \centerline{\subimport{img/}{convolution.pdf_tex}}
  \caption{The 2D convolution $\emU\ssup{0}_{i,j} * \vx_j$ as the dot product
    $\emW\ssup{0}_{i,j} \vx_j$. The blank elements of $\emW\ssup{0}_{i,j}$ are zeros.
    The $\mu$th row of $\emW\ssup{0}_{i,j}$ corresponds to applying the filter to the
    $\mu$th convolutional patch of the channel $\vx_j$. \label{fig:conv-as-linear}}
\end{figure}

\section{TICK-GPs}
In TICK-GPs \citep{dutordoir2019tick}, which are a generalization of Conv-GPs, the kernel is given by
\begin{equation}
\Cov\sqb{f(\vx), f(\vx')} = \sum_{p=1}^P\sum_{p'=1}^P k\bra{\text{loc}\bra{p}, \text{loc}\bra{p'}} k(\vx^\sqb{p}, \vx^\sqb{p'}).
\end{equation}

We construct the same network as before, but the last layer is
\begin{align}
\emA_{i,\mu}\ssup{L+1}(\mX) &\eqdef \evb_i\ssup{L+1} +
\sum_{j=1}^{C\ssup{L}} \ \sum_{\nu=1}^{D\ssup{L}} \etW\ssup{L+1}_{i,j,\mu,\nu}\, \emZ\ssup{L}_{j,\nu}(\mX).
\label{eq:tickgp-network-final}
\end{align}
where the weights are drawn such that their covariance is
$\Cov\sqb{\etW\ssup{L+1}_{i,j,\mu,\nu},\, \etW\ssup{L+1}_{i,j,\mu,\nu'}} =
k(\text{loc}(\nu), \text{loc}(\nu'))$. Note that this is a generalization of
Conv-GPs, if $k(\text{loc}(\nu), \text{loc}(\nu')) = \evu_\nu \evu_{\nu'}$.


Define the patch function $p\ssup{\ell}_\mu: [0, F\ssup{\ell}_\text{w}F\ssup{\ell}_\text{h}) \cap \mathbb{N} \to [0,
I\ssup{\ell}_\text{w}I\ssup{\ell}_\text{h}) \cap \mathbb{N}$. A patch function $p\ssup{\ell}_\mu(\lambda)$ takes the
$\lambda$th element of the $\mu$th patch into the image space of the activation
below. For 2D convolutions, it is defined as
\begin{equation}
  p\ssup{\ell}_\mu(\lambda) = \bra{\lambda \mod F\ssup{\ell}_\text{w}} + S\ssup{\ell}_\text{w}\bra{\mu\mod I\ssup{\ell+1}_\text{w}}
  + I\ssup{\ell}_\text{w}\bra{\frac{\lambda}{F\ssup{\ell}_\text{w}} + S\ssup{\ell}_\text{h} \frac{\mu}{I\ssup{\ell+1}_\text{w}}}.
\end{equation}

\begin{lemma}
  Consider a layer $\ell$ and two patches $\mu$, $\mu'$. For all $i$, the
  covariance between the corresponding elements $p\ssup{\ell}_\mu(i)$ and
  $p\ssup{\ell}_{\mu'}(i)$ is in the same diagonal of the covariance matrix of
  the filter below. That is, $p\ssup{\ell}_\mu(i) - p\ssup{\ell}_{\mu'}(i) = d$. 
\end{lemma}
\begin{proof}
  \begin{align*}
p\ssup{\ell}_\mu(i) - p\ssup{\ell}_{\mu'}(i) &= 
\bra{\lambda \mod F\ssup{\ell}_\text{w}} + S\ssup{\ell}_\text{w}\bra{\mu\mod I\ssup{\ell+1}_\text{w}} + I\ssup{\ell}_\text{w}\bra{\frac{\lambda}{F\ssup{\ell}_\text{w}} + S\ssup{\ell}_\text{h} \frac{\mu}{I\ssup{\ell+1}_\text{w}}} \\
                                               &\hspace{1em}- 
\bra{\lambda \mod F\ssup{\ell}_\text{w}} - S\ssup{\ell}_\text{w}\bra{\mu'\mod I\ssup{\ell+1}_\text{w}} - I\ssup{\ell}_\text{w}\bra{\frac{\lambda}{F\ssup{\ell}_\text{w}} + S\ssup{\ell}_\text{h} \frac{\mu'}{I\ssup{\ell+1}_\text{w}}} \\
                                             &=  S\ssup{\ell}_\text{w}\bra{\bra{\mu\mod I\ssup{\ell+1}_\text{w}} - \bra{\mu'\mod I\ssup{\ell+1}_\text{w}}} + 
                                               I\ssup{\ell}_\text{w}\bra{S\ssup{\ell}_\text{h} \frac{\mu}{I\ssup{\ell+1}_\text{w}} - S\ssup{\ell}_\text{h} \frac{\mu'}{I\ssup{\ell+1}_\text{w}}}.
    \end{align*}
\end{proof}


\begin{lemma}
  Consider $\ell$. Consider $\nu$, $\nu'$ such that
  $\nu=p\ssup{\ell}_{\mu}(i)$ and $\nu'=p\ssup{\ell}_{\mu'}(\lambda)$ for some
  $\lambda$, $\mu$ and $\mu'$. Then, for all $\lambda$, the pairs
  $p\ssup{\ell-1}_{p\ssup{\ell}_\mu(\lambda)}$ and
  $p\ssup{\ell-1}_{p\ssup{\ell}_{\mu'}(\lambda)}$ are in the same diagonal.
\end{lemma}
\begin{proof}
  Notice that
  \begin{align}
    p\ssup{\ell}_\mu(i) \mod I\ssup{\ell}_\text{w} &= \bra{\lambda \mod F\ssup{\ell}_\text{w}} + S\ssup{\ell}_\text{w}\bra{\mu\mod I\ssup{\ell+1}_\text{w}} \\
    \frac{p\ssup{\ell}_\mu(i)}{I\ssup{\ell}_\text{w}} &= \frac{\lambda}{F\ssup{\ell}_\text{w}} + S\ssup{\ell}_\text{h} \frac{\mu}{I\ssup{\ell+1}_\text{w}}.
  \end{align}
  This implies we can cancel out the $\lambda$ terms and write
{
  \newcommand{\p}[2]{{p\ssup{\ell{#1}}_{\mu{#2}}(\lambda)}}
  \newcommand{\Iw}[1]{{I\ssup{\ell{#1}}_\text{w}}}
  \newcommand{\Ih}[1]{{I\ssup{\ell{#1}}_\text{h}}}
  \newcommand{\Sw}[1]{{S\ssup{\ell{#1}}_\text{w}}}
  \newcommand{\Sh}[1]{{S\ssup{\ell{#1}}_\text{h}}}
  \begin{align*}
    p\ssup{\ell-1}_\p{}{} -   p\ssup{\ell-1}_{\p{}{}} &=
\Sw{-1}\Sw{} \bra{\bra{\mu \mod \Iw{+1}} - \bra{\mu \mod \Iw{+1}}} \\
&+ \Iw{-1}\Sh{-1} \bra{\floor{\frac{\p{}{}}{\Iw{}}} - \floor{\frac{\p{}{'}}{\Iw{}}}}
  \end{align*}
  }
  TO BE FIXED but the lemma is true.
\end{proof}

\bibliography{iclr2020_conference}
\bibliographystyle{iclr2020_conference}

\end{document}
