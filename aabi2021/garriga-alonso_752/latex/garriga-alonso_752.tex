\documentclass[accepted]{uai2021} % for initial submission
% \documentclass[accepted]{uai2021} % after acceptance, for a revised
                                    % version; also before submission to
                                    % see how the non-anonymous paper
                                    % would look like
%% There is a class option to choose the math font
% \documentclass[mathfont=cm]{uai2021} % Computer Modern math instead of
                                       % ptmx, like default for UAI ≤2020
% \documentclass[mathfont=newtx]{uai2021} % newtx fonts (improves upon
                                          % ptmx; less tested, no support)
% NOTE: Only keep *one* line above as appropriate, as it will be replaced
%       automatically for papers to be published. Do not make any other
%       change above this note for an accepted version.

%% Choose your variant of English; be consistent
% \usepackage[american]{babel}
\usepackage[british]{babel}
\DeclareUnicodeCharacter{2212}{-}  % matplotlib outputs this
\usepackage{balance}
%% Some suggested packages, as needed:
\usepackage{natbib} % has a nice set of citation styles and commands
    \bibliographystyle{plainnat}
    \renewcommand{\bibsection}{\subsubsection*{References}}
\usepackage{mathtools} % amsmath with fixes and additions
% \usepackage{siunitx} % for proper typesetting of numbers and units
\usepackage{booktabs} % commands to create good-looking tables
\usepackage{tikz} % nice language for creating drawings and diagrams
\usepackage{xr}
\externaldocument{garriga-alonso_752-supp}

%% Provided macros
% \smaller: Because the class footnote size is essentially LaTeX's \small,
%           redefining \footnotesize, we provide the original \footnotesize
%           using this macro.
%           (Use only sparingly, e.g., in drawings, as it is quite small.)


\title{Correlated Weights in Infinite Limits of Deep Convolutional Neural Networks}

% The standard author block has changed for UAI 2021 to provide
% more space for long author lists and allow for complex affiliations
%
% All author information is authomatically removed by the class for the
% anonymous submission version of your paper, so you can already add your
% information below.
%
\author[1]{\href{mailto:Adrià Garriga-Alonso <ag919@cam.ac.uk>?Subject=Your UAI 2021 paper}{Adrià~Garriga-Alonso}{}}
\author[2]{Mark~van~der~Wilk}
% Add affiliations after the authors
\affil[1]{%
    Department of Engineering\\
    University of Cambridge\\
    UK
}
\affil[2]{%
    Department of Computer Science\\
    Imperial College London\\
    UK
}

%% CUSTOM PACKAGES
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Useful packages
% \usepackage{subcaption}
\usepackage{xcolor}
%\usepackage{amsmath}
\usepackage{amsfonts}


\usepackage{graphicx}
% \usepackage[colorlinks=true, allcolors=blue]{hyperref}
% \usepackage{paralist}
% \usepackage{listings}
% \lstset{
%     showstringspaces=false,
%     basicstyle=\ttfamily,
%     breaklines=true,
%     breakatwhitespace=true,
% }
% \usepackage{booktabs}
\usepackage{cancel}
\usepackage{import}  % Inkscape figures

\usepackage[colorinlistoftodos]{todonotes}
\newcommand{\markcomment}[1]{\todo[color=green]{#1}\xspace}
\newcommand{\adriacomment}[1]{\todo[color=blue]{#1}\xspace}

% Algorithm package from JMLR
\usepackage{algorithm}
\usepackage[algo2e,ruled]{algorithm2e}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{amsthm}
% Cleveref must go after amsthm but before theorem definitions, https://tex.stackexchange.com/a/19106/107809
\usepackage{cleveref}  % may add 'noabbrev'
\creflabelformat{equation}{#2#1#3}
\crefname{assumption}{assumption}{assumptions}
\usepackage{adria-math}  % theorem definitions here

\input{acronyms.tex}

%% Mathematical commands
\usepackage{xspace}
% \creflabelformat{equation}#2#1#3}

%% NETSOR language
\newcommand{\Netsor}{\textsc{Netsor}\xspace}
\newcommand{\Gva}{\mathsf{G}}
\newcommand{\Hva}{\mathsf{H}}
\newcommand{\Ava}{\mathsf{A}}
\newcommand{\Ova}{\mathsf{O}}
\newcommand{\MatMul}{\texttt{MatMul}\xspace}
\newcommand{\LinComb}{\texttt{LinComb}\xspace}
\newcommand{\Nonlin}{\texttt{Nonlin}\xspace}
\newcommand{\SMatMul}[1]{{\MatMul: {#1}}}
\newcommand{\SLinComb}[1]{{\LinComb: {#1}}}
\newcommand{\SNonlin}[1]{{\Nonlin: {#1}}}

\newcommand{\SInput}{\Input}
\newcommand{\SOutput}{\Output}
\newcommand{\CommentC}{\tcc}

\newcommand{\Sigmax}{{\Sigma_{\text{x}}}}
\newcommand{\Sigmay}{{\Sigma_{\text{y}}}}
\newcommand{\Sigmaxy}{{\Sigma_{\text{xy}}}}


% Neural netowrk notations
\newcommand{\layerAchar}{Z}
\newcommand{\layerAd}[2]{\mathbf{Z}^{(#1)}_{#2}(\vX')}
\newcommand{\layerAsd}[2]{Z^{(#1)}_{#2}(\vX')}
\newcommand{\layerAsm}[3]{Z^{(#1)}_{#2}\bra{#3}}
\newcommand{\layerAs}[2]{Z^{(#1)}_{#2}(\vX)}
\newcommand{\layerA}[2]{\mathbf{Z}^{(#1)}_{#2}(\mX)}
\newcommand{\layera}[2]{\mathbf{Z}^{(#1)}_{#2}(\mX)}
\newcommand{\layeram}[3]{\mathbf{Z}^{(#1)}_{#2}\bra{#3}}
\newcommand{\layerAm}[3]{\mathbf{Z}^{(#1)}_{#2}\bra{#3}}
\newcommand{\layerAmmm}[1]{\mathbf{Z}^{(#1)}}
\newcommand{\layerasm}[3]{Z^{(#1)}_{#2}\bra{#3}}

\newcommand{\layerC}[1]{C^{(#1)}}

\newcommand{\layerNLAd}[2]{\vA^{(#1)}_{#2}(\vX')}
\newcommand{\layerNLAsd}[2]{A^{(#1)}_{#2}(\vX')}
\newcommand{\layerNLAsmp}[3]{A^{(#1)}_{#2}\bra{#3}}
\newcommand{\layerNLAsm}[3]{A^{(#1)}_{#2}\bra{#3}}
\newcommand{\layerNLAs}[2]{A^{(#1)}_{#2}(\vX)}
\newcommand{\layerNLA}[2]{\vA^{(#1)}_{#2}(\vX)}
\newcommand{\layerNLAm}[3]{\vA^{(#1)}_{#2}\bra{#3}}
\newcommand{\layernlasm}[3]{\vA^{(#1)}_{#2}\bra{#3}}
\newcommand{\layernla}[2]{\vA^{(#1)}_{#2}(\mX)}

\newcommand{\layerBsp}[2]{\mH\ssup{#1}_{#2}}  % TODO change

\newcommand{\layerUs}[1]{U^{(#1)}}
\newcommand{\layerU}[1]{\vU^{(#1)}}

\newcommand{\layerWs}[1]{W^{(#1)}}
\newcommand{\layerW}[1]{\mW^{(#1)}}
\newcommand{\layerWbase}{\mW}
\newcommand{\layerbs}[1]{b^{(#1)}}
\newcommand{\layerb}[1]{\vb^{(#1)}}

\newcommand{\layersizebase}{\vF}
\newcommand{\layersizebases}{F}
\newcommand{\layersize}[1]{{\layersizebase^{\bra{#1}}}}
\newcommand{\layerw}[1]{F_\text{w}^{(#1)}}
\newcommand{\layerh}[1]{F_\text{h}^{(#1)}}
\newcommand{\layerwbase}{F_\text{w}}
\newcommand{\layerhbase}{F_\text{h}}

\newcommand{\patchidx}{q}
\newcommand{\patchsizebase}{\vP}
\newcommand{\patchsizebases}{P}
\newcommand{\patchsize}[1]{{\patchsizebase^{\bra{#1}}}}
\newcommand{\patchsizes}[1]{\patchsizebases^{\bra{#1}}}
\newcommand{\patchw}[1]{P_\text{w}^{(#1)}}
\newcommand{\patchh}[1]{P_\text{h}^{(#1)}}
\newcommand{\patchwbase}{P_\text{w}}
\newcommand{\patchhbase}{P_\text{h}}
\newcommand{\patchf}[2]{{\tilde#1\bra{#2}}}
\newcommand{\convpatch}[1]{{\text{im}\bra{\tilde#1}}}

\newcommand{\priorLcov}[1]{\vL^{(#1)}}
\newcommand{\priorLscov}[1]{L^{(#1)}}
\newcommand{\priorWcovs}[1]{\Sigma^{(#1)}}
\newcommand{\priorWcov}[1]{\boldsymbol{\Sigma}^{(#1)}}

% These 4 are used for the picture below
\newcommand{\chan}{i}
\newcommand{\prevchan}{j}   % <- Possibly use these for c, p, gamma? I've
\newcommand{\patch}{\vp}               % been using \nu,\mu
\newcommand{\patchs}{p}               % been using \nu,\mu
\newcommand{\nextpatch}{\vq}
\newcommand{\nextpatchs}{q}

\newcommand{\sigmaw}{{\sigma^{2}_{\text{w}}}}

\newcommand{\meanf}[1]{m\ssup{#1}}
\newcommand{\covf}[1]{K\ssup{#1}}
\newcommand{\nlinf}[1]{V\ssup{#1}}
\newcommand{\vmeanf}[1]{\vm\ssup{#1}}
\newcommand{\mcovf}[1]{\mK\ssup{#1}}
\newcommand{\vecf}[1]{\text{vec}\bra{#1}}
\newcommand{\diagf}[1]{\text{diag}\bra{#1}}
\newcommand{\numelf}[1]{\text{ne}\bra{#1}}


% bold numbers
\newcommand{\0}{\boldsymbol{0}}
\newcommand{\1}{\boldsymbol{1}}


\newcommand{\crefp}[1]{(\cref{#1})}
\newcommand{\eqparref}{\crefp}
\newcommand{\eqparreftwo}[2]{\crefp{#1,#2}}


\begin{document}
\maketitle

\begin{abstract}
Infinite width limits of deep neural networks often have tractable forms. They have been used to analyse the behaviour of finite networks, as well as being useful methods in their own right. When investigating infinitely wide convolutional neural networks (CNNs), it was observed that the correlations arising from spatial weight sharing disappear in the infinite limit.
This is undesirable, as spatial correlation is the main motivation behind CNNs. We show that the loss of this property is not a consequence of the infinite limit, but rather of choosing an independent weight prior. Correlating the weights maintains the correlations in the activations. 
Varying the amount of correlation interpolates between independent-weight limits and mean-pooling. Empirical evaluation of the infinitely wide network shows that optimal performance is achieved between the extremes, indicating that correlations can be useful.
\end{abstract}

\section{Introduction}
% Paragraph 1
% - NN theory
% - Infinite limits are often more tractable
% - Infinite limits have given understanding, and new methods
% - However, we lose certain properties
Analysing infinitely wide limits of neural networks has long been used to provide insight into the properties of neural networks.
\citet{neal1996bayesian} first noted such a relationship, through showing that infinitely wide Bayesian neural networks converge in distribution to Gaussian processes (GPs).
The success of GPs raised the question of whether such a comparatively simple model could replace a complex neural network. \citet{mackay1998introgp} noted that taking the infinite limit resulted in a fixed feature representation, a key desirable property of neural networks. Since this property is lost due to the infinite limit, MacKay inquired: ``have we thrown the baby out with the bath water?''

In this work, we follow the recent interest in infinitely wide convolutional neural networks \citep{garriga2018infiniteconv,novak2019infiniteconv}, to investigate another property that is lost when taking the infinite limit: correlation in the activations of patches in different parts of the image.
Given that convolutions were developed to introduce these correlations, and that they improve performance \citep{arora2019exact}, it seems undesirable that they are lost when more filters are added.
Currently, the only way of reintroducing spatial correlations is to change the model architecture by introducing mean-pooling \citep{novak2019infiniteconv}. This raises two questions:
% \begin{enumerate}[\textbf{1)}]
\begin{enumerate}[label=\textbf{\arabic*)}]
\itemsep0em
\item Is the loss of patchwise correlations a necessary consequence of the infinite limit?
\item Is an architectural change the only way of reintroducing patchwise correlations?
\end{enumerate}

We show that the answer to both these questions is ``no''. Correlations between patches can also be
maintained in the limit without pooling by introducing correlations between the weights in the prior. The amount of correlation can be controlled,
which allows us to interpolate between the existing approaches of full
independence and mean-pooling. Our approach allows the discrete architectural choice of mean-pooling to be replaced with a more flexible continuous amount of correlation.

\begin{figure*}[t]
  % Width: 397.9pt
  \centering
{%\renewcommand{\layerAs}[1]{\vZ^{(#1)}}
  % \def\svgwidth{\textwidth}
  \import{fig/}{net_diagram.pdf_tex}}
  \caption{A deep convolutional neural network following our notation. Infinite limits are taken over the number of convolutional filters $\layerC{\ell}$ (vertical), which equals the number of channels in the following layer (horizontal). The network has $L=3$ layers and $D=2$ spatial dimensions. The output is not spatially extended $(\layersize{3}=\1)$ because $\patchsize{3} = \layersize{2}.$}
    % {N.B.: We need to emphasise that the last layer $Z^{(3)}$ is $f(\cdot)$.}
  \label{fig:fancy-cnn}
\end{figure*}


We empirically show that modest performance improvements can be obtained by replacing mean-pooling at the final layer with an intermediate amount of correlation. In addition, we show that in layers before the final one, the discrete architectural choice of mean-pooling can be replaced by an intermediate amount of correlation, without degrading performance. Avoiding discrete design decisions makes architecture search easier, by allowing continuous optimisation.
We speculate that these results from infinite networks could be useful for adapting priors or initialisations in finite networks, leading to better performance, or easier design.

% Empirical evaluation on CIFAR-10 shows that this additional flexibility improves performance. \textbf{emphasise other importance here}: Replacing discrete architecture choices with continuous ones can make optimising them easier, leading to faster neural architecture search.  


Overall, our work illustrates that non-standard choices in the weight prior can significantly influence properties in the infinite limit, and that good choices can lead to improved performance. We hope that this work inspires investigation into correlated weights in finite neural networks, as well as more non-standard priors or initialisations.


\section{Spatial Correlations in Single~Hidden~Layer Networks}
\label{sec:single-layer}

To begin, we will analyse the infinite limit of a single hidden layer
convolutional neural network (CNN). This illustrates the choices that lead to the disappearance of spatial correlation in the activations. We extend \citet{garriga2018infiniteconv} and \citet{novak2019infiniteconv} by considering weight priors with correlations. By adjusting the correlation, we can interpolate between existing independent weight limits and mean-pooling, which previously had to be introduced as a discrete architectural choice. We also discuss how existing convolutional Gaussian processes \citep{vdw2017convgp,dutordoir2020} can be obtained from limits of correlated weight priors.

% We first show how spatial correlations disappear
% when using infinite weights \citep{garriga2018infiniteconv,novak2019infiniteconv}, and how mean pooling reintroduces them \citep{novak2019infiniteconv}. This leads to kernels with the same structure as \Citet{vdw2017convgp}. Next, we show how spatial correlations can be recovered by adding weight correlations instead of making the discrete architectural modification of adding mean pooling. This leads to the 
% ``Translation Insensitive Convolutional Kernel'' (TICK) considered by \citet{dutordoir2020}. In the next section, we will generalise this to deep convolutional networks.

Consider a CNN with $L=2$ layers. \Cref{fig:fancy-cnn} provides a graphical representation of the notation.
The input $\mX$ is a real-valued tensor of shape ${\layerC{0} \times \layersize{0}}$, where $\layerC{0} \in \sN$ is the number of channels and $\layersize{0} \in \sN^{D}$ the spatial size of the input.  Superscripts denote the layer index.
For images, usually $\layerC{0}=3$ (one per colour), and the number of spatial input dimensions is $D=2$, so $\layersize{0} = (\layerh{0}, \layerw{0}).$
The convolution operation at layer $\ell\in\countto{L}$ divides its input into patches of size
$\patchsize{\ell} \preceq \layersize{\ell-1}$. For a given spatial location of the next activation
$\nextpatch\in\countto{\layersize{\ell}}$,\footnote{For some number
  $\patchsizebases\in \sN$, the expression $\countto{\patchsizebases}$ is the
  set $\cb{1,\dots,\patchsizebases}$. For a tuple $\patchsizebase\in\sN^{D}$,
  $\countto{\patchsizebase} = \countto{\patchsizebases_{1}}\times \dots \times \countto{\patchsizebases_{D}}$.}
 the patch function
$\patchf{\nextpatch}{\cdot}: \countto{\patchsize{\ell}} \to \countto{\layersize{\ell-1}}$
iterates over the elements of the patch \crefp{eq:little-patchf}. Weights are applied by taking an inner product with all patches, which we do $\layerC{\ell}$ times to give multiple channels in the next layer. By collecting all weights in the tensor $\layerW{\ell}\in\sR^{\layerC{\ell}\times \layerC{\ell-1}\times \patchsize{\ell}}$ the pre- and post-nonlinearity activations are respectively, for $\ell\in\countto{L},$
\begin{equation}
    % \layerAs{1}{cp} &= \sum_{\gamma=1}^{\layerC0}\sum_{q=1}^{\patchw0\patchh0} \convpatch{X}{p}_{\gamma q} \layerWs1_{c\gamma q} \,,
    \layerAs{\ell}{\chan,\nextpatch} = \sum_{\prevchan=1}^{\layerC{\ell-1}} \sum_{\patch=\1}^{\patchsize{\ell}} \layerWs{\ell}_{\chan,\prevchan,\patch}\, \layerNLAs{\ell-1}{\prevchan,\patchf{\nextpatch}{\patch}}\,,
    \label{eq:deep-recursion}
\end{equation}
\begin{equation}
  \layerNLAs{0}{\chan,\patch} = X_{\chan,\patch},\quad \layerNLAs{\ell}{\chan,\patch} = \phi\bra{\layerAs{\ell}{\chan,\patch}}\,.
  \label{eq:deep-activations}
\end{equation}
\Cref{eq:deep-recursion} is a channel-wise sum of $D$-dimensional convolutions, and $\phi$ denotes the elementwise nonlinearity.

For layer $\ell\in\countto{L}$, stride $s$, dilation $h$, the patch function is $\patchf{\nextpatch}{\patch} \eqdef \bra{\patchf{\nextpatchs_{1}}{\patchs_{1}}, \dots, \patchf{\nextpatchs_{D}}{\patchs_{D}}}$, where
\begin{equation}
  \patchf{\nextpatchs_{d}}{\patchs_{d}} = s\nextpatchs_{d} -  h\bra{\patchs_{d} - \ceil{\patchsizebases_{d}/2}}\,.
  \label{eq:little-patchf}
\end{equation}
Using \cref{eq:little-patchf}, it is possible to verify that \cref{eq:deep-recursion} is the usual deep learning convolution \crefp{app:patchf}.

In a single hidden layer CNN, these activations are followed by a fully-connected layer with weights $\layerW{2} \in \sR^{\layerC 1\times \patchsize{2}}$, where $\patchsize{2} = \layersize{1}$. Our final output is again given by a summation over the activations
\begin{align}
  f(\mX) &= \sum_{\prevchan=1}^{\layerC{1}} \sum_{\patch=1}^{\layersize{1}} \layerWs{2}_{\prevchan,\patch}\, \layerNLAs{1}{\prevchan,\patch}
    % f(\vX) &= \sum_{c=1}^{\layerC1}\sum_{p=1}^{\layersize{}} \phi\big(\layerAs{1}{cp}\big) W^{(2)}_{cp} \nonumber \\
         = \sum_{\patch=\1}^{\layersize{1}} \layerAs{f}{\patch}
        \label{eq:single-layer-f} \,,
\end{align}
where $\layerAs{f}{\patch}$ denotes the result before the summation over spatial locations $\patch$.

We analyse the distribution on function outputs $f(\vX)$ for some Gaussian prior
$p(\gW)$ on the weights of all layers $\gW$. In all the cases
we consider, we take the prior to be independent over layers and channels. Here
we extend earlier work by allowing spatial correlation in the final layer's
weights (we will consider all layers later) through the covariance tensor
$\priorWcov{\ell} \in \sR^{\layersize{\ell}\times\layersize{\ell}}$. This gives
the prior
\begin{align}
    p(\layerW1) &= \prod_{\chan=1}^{\layerC1}\prod_{\prevchan=1}^{\layerC0}\Normal{\layerW1_{\chan,\prevchan}\,;\, 0, \eye} \,, \\
     p(\layerW2) &= \prod_{\chan=1}^{\layerC1} \Normal{\layerW2_{\chan}\,;\, 0, \frac{1}{\layerC1}\priorWcov2} \,,
\end{align}
with independence between different layers' weights.
Here, a tensor-valued covariance $\priorWcov2$ expresses arbitrary covariance over the spatial dimensions of the tensor $\layerW{2}_{\chan}$: $\Cov\sqb{\layerWs{2}_{\chan,\patch},\layerWs{2}_{\chan,\patch'}} = \priorWcovs{2}_{\patch,\patch'}/\layerC{\ell-1}.$

Since $\layerW1,\layerW2$ are i.i.d. over channels $\chan\in\layerC{1}$, the random variables
$\sum_{\patch=\1}^{\layersize{1}}  \layerWs{2}_{\chan,\patch}\, \layerNLAs{1}{\chan,\patch}$ are identically distributed and independent for each $\chan \in \countto{\layerC{1}}$.
This allows us to apply the central limit theorem (CLT) to their sum $f(\mX)$, showing that $f(\mX)$ converges in distribution to a Gaussian process as $\layerC1\to\infty$ \citep{neal1996bayesian}.

The covariance between the final-layer activations for two inputs $\vX,\vX'$ becomes
\begin{align}
  &\Cov_{\vW}\left[\layerAs{f}{\patch},\layerAsd{f}{\patch'}\right] = \nonumber\\ &=\Expect{\vW}{\sum_{\prevchan=1}^{\layerC1}\sum_{\prevchan'=1}^{\layerC1} \layerWs{2}_{\prevchan,\patch}\layerNLAs{1}{\prevchan,\patch}\; \layerWs{2}_{\prevchan',\patch'}\layerNLAsd{1}{\prevchan',\patch'}}\,, \nonumber \\
  \intertext{use independences to split the expectations, and substitute the weight covariance,}
    &= \sum_{\prevchan=1}^{\layerC1}\sum_{\prevchan'=1}^{\layerC1} \Expect{\layerW1}{\layerNLAs{1}{\prevchan,\patch}\,\layerNLAsd{1}{\prevchan',\patch'}}\Expect{\layerW2}{\layerWs{2}_{\prevchan,\patch} \layerWs{2}_{\prevchan',\patch'}} \nonumber \\
  &= \sum_{\prevchan=1}^{\layerC1}\sum_{\prevchan'=1}^{\layerC1} \Expect{\layerW1}{\layerNLAs{1}{\prevchan,\patch}\,\layerNLAsd{1}{\prevchan',\patch'}}\;\delta_{\prevchan,\prevchan'}
    \frac{\priorWcovs{2}_{\patch,\patch'}}{\layerC{1}}\,,\nonumber \\
  \intertext{eliminate one of the sums over $\prevchan$ using $\delta_{\prevchan,\prevchan'}$, and rearrange}
  &= \Expect{\layerW1}{
\frac{1}{\layerC1}\sum_{\prevchan=1}^{\layerC1} \layerNLAs{1}{\prevchan,\patch}\,\layerNLAsd{1}{\prevchan,\patch'}}
    \priorWcovs{2}_{\patch,\patch'} \nonumber \\
  &=  \nlinf{1}_{\patch,\patch'}\bra{\mX,\mX'}  \;\priorWcovs{2}_{\patch,\patch'} \,.
\end{align}
The limit of the sum of the final expectation over $\layerW1$ can be found in closed form for many activations (see \cref{sec:exp-nonlin}) and is denoted $\nlinf{1}_{\patch,\patch'}\bra{\mX,\mX'}.$
Note in \cref{eq:deep-recursion} that the activations for some location $\patch\in\countto{\layersize{1}}$ only depend on the input patch at $\patch$, that is, on the elements of $\mX$ that are in the image $\convpatch{\patch}$ of the patch function $\patchf{\patch}{\cdot}$. Thus, the kernel acts locally on patches:
$\nlinf{1}_{\patch,\patch'}\bra{\mX,\mX'} = k\ssup{1}\bra{\mX_{:,\,\convpatch{\patch}},\mX'_{:,\,\convpatch{\patch'}}}$.

We find the final kernel for the GP by taking the covariance between function values $f(\vX)$ and $f(\vX')$ and performing the final sum in \cref{eq:single-layer-f}:
\begin{align}
    K(\vX, \vX') &= \Cov\left[f(\vX), f(\vX')\right] \nonumber\\
    &= \sum_{\patch,\patch'} k\ssup{1}\bra{\mX_{:,\,\convpatch{\patch}},\mX'_{:,\,\convpatch{\patch'}}}\, \priorWcovs{2}_{\patch,\patch'} \label{eq:cov-kernel} \,.
\end{align}
We can now see how different choices for $\priorWcov2$ give different forms of spatial correlation.

\paragraph{Independence.} \Citet{garriga2018infiniteconv} and \citet{novak2019infiniteconv} consider $\priorWcovs{2}_{\patch,\patch'} = \delta_{\patch,\patch'}\sigmaw$,
i.e.~the case where all weights are independent. The resulting kernel simply sums components over patches, which implies an \emph{additive model} \citep{stone1985}, where a \emph{different} function is applied to each patch, after which they are all summed together: $f(\vX) = \sum_\patch f_\patch(\vX_{:,\convpatch{\patch}})$.
This structure has commonly been applied to improve GP performance in high-dimensional settings \citep[e.g.][]{duvenaud2011additive,durrande2012additive}. \citet{novak2019infiniteconv} point out that the same kernel can be obtained by taking an infinite limit of a \emph{locally connected network} (LCN) \citep{lecun1989generalization} where connectivity is the same as in a CNN, but without weight sharing, indicating that a key desirable feature of CNNs is lost.

\paragraph{Mean-pooling.} By taking $\priorWcovs{2}_{\patch,\patch'} = 1/\abs{\layersize{2}}^{2}$ we make the weights fully correlated over all locations\footnote{For a size $\layersizebase \in \sN^{D}$, its number of elements is $\abs{\layersizebase} \eqdef \prod_{d=1}^{D}\layersizebases_{d}.$}, leading to identical weights for all $\patch$, i.e.~$\layerWs{2}_{\chan,\patch} = \layerWs{2}_{\chan}$. This is equivalent to taking the mean response over all spatial locations (see \cref{eq:single-layer-f}), or global average pooling. As \citet{novak2019infiniteconv} discuss, this reintroduces the spatial correlation that is the intended result of weight sharing. The ``translation invariant'' convolutional GP
of \citet{vdw2017convgp} can be obtained by this single-layer limit using Gaussian activation functions \citep{vdw2019thesis}. Since this mean-pooling was shown to be too restrictive in this single-layer case, \Citet{vdw2017convgp} considered pooling with constant weights $\alpha_\patch$ (i.e.~without a prior on them). In this framework, this is equivalent to placing a rank 1 prior on the final-layer weights by taking $\priorWcovs{2}_{\patch\patch'} = \alpha_\patch \alpha_{\patch'}$. This maintains the spatial correlations, but requires the $\alpha_\patch$ parameters to be learned by maximum \textit{marginal} likelihood (ML-II, empirical Bayes).

% In the single-layer case with Gaussian activations \citep{vdw2019thesis}, this limit becomes the ``translation invariant'' convolutional GP of \Citep{vdw2017convgp}, who showed that that mean-pooling is too restrictive in this single-layer case. Pooling with a learned weights ${\alpha_p}$ for each patch was introduced as a remedy for this, which can be recovered by taking $\priorWcovs{2}_{pp'} = \alpha_p \alpha_{p'}$. These weights need to be learned by maximum marginal likelihood (ML-II), which departs slightly from the strict Bayesian framework.

\paragraph{Spatially correlated weights.} In the pooling examples above, the spatial covariance of weights is taken to be a rank-1 matrix. We can add more flexibility to the model by varying the strength of correlation between weights based on their distance in the image. We consider an exponential decay depending on the distance between two patches: $\priorWcovs{2}_{\patch\patch'} = \exp\bra{-d\bra{\patch, \patch'}/l}$. We recover full independence by taking $l\to 0$, and mean-pooling with $l\to\infty$. Intermediate values of $l$ allow the rigid assumption of complete weight sharing to be relaxed, while still retaining spatial correlations between similar patches. This construction gives the same kernel as investigated by \citet{mairal2014ckn} and \citet{dutordoir2020}, who named this property ``translation insensitivity'', as opposed to the stricter invariance that mean-pooling gives. The additional flexibility improved performance without needing to add many parameters that are learned in a non-Bayesian fashion.

Our construction shows that spatial correlation can be retained in infinite limits without needing to resort to architectural changes. A simple change to the prior on the weights is all that is needed. This property is retained in wide limits of deep networks, which we investigate next.


% \subsection{Notes}

% Give a high-level commentary on the infinite-width line of work. Summarise commentary on this line of work. As far as I see, the main point here is MacKay's point on the baby and the bathwater, and who else has discussed it (I know Matthews has).
% \begin{itemize}
%     \item Note MacKay's point about disappearing correlations. Have we thrown out the baby with the bathwater?
%     \item Note that both \citet{garriga2018infiniteconv} and \citet{novak2019infiniteconv} show that correlations disappeard in infinite limit.
%     \item Ask question: Is this really what we want? The whole point of CNNs was to have spatial correlations in different areas of the image!
%     \item Compare and contrast to additive models in GPs and convolutional models in GPs, which were introduce specifically to gain these correlations.
%     \item Discuss mean-pooling as a way to get back correlations. Raise point that this is a discrete non-learnable architectural choice!
%     \item Note that \citet{vdw2019thesis} briefly discusses how ConvGP is an infinite limit.
%     \item Can we construct an infinite limit of a CNN that has the correlations like the ConvGP, but also works for full deep networks?
% \end{itemize}




\section{Spatial Correlations in Deep Networks}
\label{sec:deep-correlations}
Here, we provide an informal extension of the previous section's results to deep networks.
In deep networks, correlated weights also retain spatial correlation in the activations.
\Cref{app:netsor} provides a formal justification for this section, using the framework by \citet{yang2019wide}.

The procedure for computing the kernel has a recursive form similar to existing analyses \citep{garriga2018infiniteconv,novak2019infiniteconv}. Negligible additional computation is needed to consider arbitrary correlations, compared to only considering mean-pooling \citep{novak2019infiniteconv,arora2019exact}. The main bottleneck is the need for computing covariances for all pairs of patches in the image, as in \cref{eq:cov-kernel}. For a $D$-dimensional convolutional layer, the corresponding kernel computation is a convolution of the activations' second moment with the
$2D$-dimensional covariance tensor of the weights.


The setup for the case of a deep neural network follows that of \cref{sec:single-layer}, but with the number of layers $L>2$. The outputs of the network are simply the pre-nonlinearity activations of the $L$th layer, $\layerAs{L}{\chan,\patch}$. If we need several outputs, for example in $K$-class classification, we may set $\layerC{L}=K.$
If the output of the network should not be spatially extended, we set the spatial size to $\layersize{L}=\1$. This can be achieved by making the weights $\layerW{L}$ (and their corresponding convolutional patch) have the same size as $\layersize{L-1}$ (see \cref{fig:fancy-cnn}).

As pointed out by \citet{matthews2018dnnlimit}, a straightforward application of the central limit theorem is not possible for deep networks. Fortunately, \citet{yang2019wide} developed a general framework for expressing neural network architectures and finding their corresponding Gaussian process infinite limits. The resulting kernel is given by the recursion that can be derived from a more informal argument which takes the infinite width limit in a sequential layer-by-layer fashion, as was used in \citet{garriga2018infiniteconv}. We follow this informal derivation, as this more naturally illustrates the procedure for computing the kernel. A formal justification can be found in \cref{app:netsor}.

\subsection{Recursive Computation of the Kernel}
In our weight prior, we correlate weights \emph{within} a convolutional filter. The weights remain independent over layers and channels. For each $\ell \in \countto{L}$,
\begin{equation}
  p\bra{\layerW{\ell}} = \prod_{\chan=1}^{\layerC{\ell}}\prod_{\prevchan=1}^{\layerC{\ell-1}}
  \Normal{\layerW{\ell}_{\chan,\prevchan}; 0, \frac{1}{\layerC{\ell-1}} \priorWcov{\ell}}.
    \label{eq:correlated-weight-prior}
\end{equation}
As in \cref{sec:single-layer}, $\layerW{\ell} \in \sR^{\layerC{\ell}\times\layerC{\ell-1}\times\layersize{\ell}}$, and the covariance tensor $\priorWcov{\ell} \in \sR^{\layersize{\ell}\times\layersize{\ell}}$ is positive semi-definite.
Our derivation is general for any weight covariance, so layers with correlated weights can be interspersed with the usual layers.
% Following \citet{dutordoir2020}, we let the correlation decay exponentially with the distance between weights within the convolutional filter:
% \begin{align}
% \priorWcovs{2}_{pp'} = \exp(-d(p, p')/l) \,.
% \end{align}

A Gaussian process is determined by the mean and covariance of function values for pairs of inputs $\vX,\vX'$. The mean is zero. Using the recursion in \cref{eq:deep-recursion}, we can find the covariance between any two pre-nonlinearity activations from a pair of inputs $\vX,\vX'$ and the covariance of the previous layer. For $\chan,\chan'\in\countto{\layerC{\ell}}$ and $\nextpatch,\nextpatch'\in\countto{\layersize{\ell}}$,
\begin{align}
  &\Cov_{\gW}\left[\layerAs{\ell}{\chan,\nextpatch},\layerAsd{\ell}{\chan',\nextpatch'}\right] = \nonumber\\
  &=\sum_{\prevchan,\prevchan',\;\patch,\patch'} \Expect{\gW}{
    \layerWs{\ell}_{\chan,\prevchan,\patch}\layerNLAs{\ell-1}{\prevchan,\patchf{\nextpatch}{\patch}}\; \layerWs{\ell}_{\chan',\prevchan',\patch'}\layerNLAsd{\ell-1}{\prevchan',\patchf{\nextpatch'}{\patch'}}}\,, \nonumber \\
  \intertext{substituting the expression for the weight covariance,}
  &= \delta_{\chan,\chan'} \sum_{\patch,\patch'}
    \priorWcovs{\ell}_{\patch,\patch'}
    \E_{\gW} \bigg[\nonumber\\
  &\hspace{4em}
 \frac{1}{\layerC{\ell-1}}\sum_{\prevchan=1}^{\layerC{\ell-1}}
    \layerNLAs{\ell-1}{\prevchan,\patchf{\nextpatch}{\patch}}\,
    \layerNLAsd{\ell-1}{\prevchan,\patchf{\nextpatch'}{\patch'}}
    \bigg] \nonumber \\
  &= \delta_{\chan,\chan'} \covf{\ell}_{\nextpatch,\nextpatch'}\bra{\mX,\mX'}\,.\label{eq:kernel-recursion}
\end{align}
We can see that the covariance of activations in different channels
$(\chan\ne\chan')$ is zero. Otherwise, to calculate
$\covf{\ell}\bra{\mX,\mX'}$, we need to calculate the
expectation over $\gW$, which we term $\nlinf{\ell-1}\bra{\mX,\mX'}.$ The resulting kernel expression is
\begin{equation}
  \covf{\ell}_{\nextpatch,\nextpatch'}\bra{\mX,\mX'} = \sum_{\patch=\1}^{{\patchsize{\ell}}}
  \sum_{\patch'=\1}^{{\patchsize{\ell}}} \priorWcovs{\ell}_{\patch,\patch'}
  \nlinf{\ell-1}_{\patchf{\nextpatch}{\patch},\patchf{\nextpatch'}{\patch'}}\bra{\mX,\mX'}\,,
  \label{eq:cnn-kernel-recursive}
\end{equation}
which, because the concatenation of patch functions is a patch function \crefp{rem:concat-patchf}, is equivalent to a $2D$-dimensional convolution. This kernel does not correspond to a locally-connected network, because it uses off-diagonal elements of the previous layer's kernel.

\subsection{Expectation of the Nonlinearities}
\label{sec:exp-nonlin}
For $\ell=0$, the activations in the previous layer are the image inputs, i.e.~$\layerNLA{0}{} = \vX$ (\cref{eq:deep-activations}), making $\nlinf{0}_{\nextpatch,\nextpatch'}\bra{\mX,\mX'}$ an inner product between image patches.

For $\ell\ge 1$, the expression inside the expectation in \cref{eq:kernel-recursion} is a random variable, an average over $\prevchan\in\countto{\layerC{\ell-1}}$. From \cref{eq:deep-recursion} we see that all its terms have the same expectation, i.e.
\begin{multline}
\nlinf{\ell}_{\patch,\patch'}\bra{\mX,\mX'}
= \Expect{\gW}{\frac{1}{\layerC{\ell}} \sum_{\prevchan=1}^{\layerC{\ell}} \layerNLAs{\ell}{\prevchan,\patch} \layerNLAsd{\ell}{\prevchan,\patch'}} \\
= \Expect{ \layerA{\ell}{}, \layerAd{\ell}{} }{\phi\bra{\layerAs{\ell}{1,\patch}} \phi\bra{\layerAsd{\ell}{1,\patch'}}}\,.
\label{eq:avg-to-gaussian}
\end{multline}
For the purposes of \cref{eq:avg-to-gaussian}, in the infinite width limit, the
pre-nonlinearities $\layerA{\ell}{},\layerAd{\ell}{}$ converge in distribution to a joint Gaussian
\crefp{thm:netsor-master}. Accordingly, the value of the expectation above depends only on the entries of their $2\times 2$ covariance matrix and the form of $\phi$. Here we represent this dependence through the function $F_{\phi}(\Sigmax, \Sigmay, \Sigmaxy)$,
\begin{multline}
\nlinf{\ell}_{\patch,\patch'}\bra{\mX,\mX'}
=F_{\phi}\big(\\
\covf{\ell}_{\patch,\patch'}\bra{\mX, \mX},\,\covf{\ell}_{\patch,\patch'}\bra{\mX', \mX'},
\covf{\ell}_{\patch,\patch'}\bra{\mX, \mX'}\big)\,.
\label{eq:post-nonlin}
\end{multline}

Combining \cref{eq:cnn-kernel-recursive},~\ref{eq:post-nonlin} and the input inner product
provides us with a recursive procedure to compute the covariances all the way up to the final layer.

For the balanced ReLU nonlinearity ($\phi(x) = \sqrt{2}\max(0, x)$), which we
use in all the experiments in this paper, we can use the expression by \citet{cho2009mkm}:
\begin{multline}
  F_{\phi}\bra{\Sigmax, \Sigmay, \Sigmaxy} = \frac{1}{\pi} \sqrt{\Sigmax\Sigmay - \Sigmaxy^{2}} \\+ \bra{1 - \frac{1}{\pi}\cos^{-1}\bra{\frac{\Sigmaxy^{2}}{\Sigmax\Sigmay}}} \Sigmaxy.
\end{multline}

This expression implies that
$\nlinf{\ell}_{\patch,\patch}\bra{\mX,\mX} = \covf{\ell}_{\patch,\patch}\bra{\mX,\mX}$,
for all $\mX$ and $\patch$ \citep{lee2018dnnlimit,matthews2018dnnlimit}.


% We can now apply \cref{eq:kernel-recursion} to find the covariance of the pre-nonlinearity activations at layer $\ell=2$, given the covariance of the post-nonlinarity activations in layer $\ell=1$ (\cref{eq:post-nonlin}). For finite $\layerC{1}$, $\layerA{2}{}$ will not be Gaussian, and so \cref{eq:post-nonlin} can not be applied. However, if we take $\layerC1\to\infty$, $\layerA{2}{}$ will converge to a Gaussian by the central limit theorem, all while keeping the covariance constant. After taking the limit, we can then apply \cref{eq:post-nonlin}. This provides us with a recursive procedure to compute the covariances all the way up to the final layer, by sequentially taking limits of $\layerC\ell\to\infty$.


% \begin{algorithm}[tb]
% \begin{algorithmic}[1]
% %\STATE \emph{Input}: two images, $\vX,\vX' \in \mathbb{R}^{C^{(0)} \times (H^{(0)}W^{(0)})}$.
% %\STATE Compute $\bdiagcov_{\vX\vX}^{(1)}$, $\bdiagcov_{\vX\vX'}^{(1)}$, and
% %$\bdiagcov_{\vX'\vX'}^{(1)}$; using \cref{eq:kernel-base}.
% %\FOR{$\ell=1,2,\dots,L$}
% %\STATE Compute $\E[\phi\phi]^{(\ell)}$, $\E[\phi\phi']^{(\ell)}$ and
% %$\E[\phi'\phi']^{(\ell)}$ using equation (\ref{eq:nlin-relu}),
% %(\ref{eq:nlin-erf}), or some other nonlinearity.
% %\STATE Compute $\bdiagcov_{\vX\vX}^{(\ell+1)}$, $\bdiagcov_{\vX\vX'}^{(\ell+1)}$, and
% %$\bdiagcov_{\vX'\vX'}^{(\ell+1)}$; using \cref{eq:kernel-recursive}.
% %\ENDFOR
% %\STATE Output $\diagcov_{\vX\vX'}^{(L+1)}$, which is a nonnegative scalar.
% \State \emph{Input}: two images, $\vX,\vX' \in \mathbb{R}^{\layerC0 \times \layersize{}}$.
% % \State Compute $K_\mu\ssup{1}(\vX, \vX)$, $K_\mu\ssup{1}(\vX, \vX')$, and
% % $K_\mu\ssup{1}(\vX', \vX')$ \\\hspace{2em}for $\mu \in \{1,\dotsc,H\ssup{1}D\ssup{1}\}$; using Eq.~kernel-base.
% \State Compute $K_{pp}^{(1)}(\vX, \vX)$ and $K_{pp}^{(1)}(\vX', \vX')$ for all patch locations $p$ in the input image $\vX$.
% \State Compute $K_{pp'}^{(1)}(\vX, \vX')$ for all pairs of patch locations $p, p'$ in the input images $\vX$ and $\vX'$.
% \For{$\ell=1,2,\dots,L$}
% \State Compute $V_{pp'}^{(\ell)}(\vX, \vX')$ for all pairs of patch locations $p, p'$ in layer $\ell$.
% \State Compute $K_{pp}^{(\ell)}(\vX, \vX)$ and $K_{pp}^{(1)}(\vX', \vX')$ for all patch locations $p$ in the activations of layer $\ell$.
% \State Compute $K_{pp'}^{(\ell)}(\vX, \vX')$ for all pairs of patch locations $p, p'$ in the activations of layer $\ell$.
% % \State Compute $V_\mu\ssup{\ell}(\vX, \vX')$, $V_\mu\ssup{\ell}(\vX, \vX')$ and
% % $V_\mu\ssup{\ell}(\vX,\vX')$ \\\hspace{2em}for $\mu \in \{1,\dotsc,H\ssup{\ell}D\ssup{\ell}\}$; using Eq.~nlin-relu, or some other nonlinearity.
% % \State Compute $K_\mu\ssup{\ell+1}(\vX,\vX)$, $K_\mu\ssup{\ell+1}(\vX,\vX')$, and
% % $K_\mu\ssup{\ell+1}(\vX',\vX')$ \\\hspace{2em}for $\mu \in \{1,\dotsc,H\ssup{\ell+1}D\ssup{\ell+1}\}$; using Eq.~kernel-recursive.
% \EndFor
% \State Output the scalar $K_{11}\ssup{L}(\vX,\vX')$ (the only covariance left after the last layer).
% \end{algorithmic}
% \caption{Computation of correlated weight ConvNet kernel $k(\vX, \vX')$}
% \label{alg:kernel}
% \end{algorithm}

\subsection{Computational complexity, diagonal propagation}
To handle the covariance tensor for $\covf{\ell}\bra{\mX,\mX'}$, we need to compute and represent $\absshort{\layersize{\ell}}^{2}$ entries. This can be considerably more expensive than the forward pass of the corresponding CNN, where the activations have size $\absshort{\layersize{\ell}}$.
In special cases, the computation or memory costs can be reduced, compared to the $2D$-dimensional convolution in \cref{eq:cnn-kernel-recursive}, which is a generalisation of previous algorithms. These cases do not include layers with mean-pooling, for which our algorithm is equally expensive to previous ones \citep{arora2019exact}.

If weights are independent, only the diagonal of $\priorWcov{\ell}$ has nonzero entries, so $\priorWcovs{\ell}_{\patch,\patch'} = \delta_{\patch,\patch'}\priorWcovs{\ell}_{\patch,\patch'}$. One of the sums in the \cref{eq:cnn-kernel-recursive} can then be removed,
\begin{equation}
\covf{\ell}_{\nextpatch,\nextpatch'}\bra{\mX, \mX'} =
\sum_{\patch=\1}^{\patchsize{\ell-1}}
\priorWcovs{\ell}_{\patch,\patch} \,
\nlinf{\ell-1}_{\patchf{\nextpatch}{\patch},\patchf{\nextpatch'}{\patch}}\bra{\mX, \mX'}.
\label{eq:cnn-kernel-independent-weights}
\end{equation}
The patch functions that access $\nlinf{\ell-1}\bra{\mX,\mX'}$ are still
different ($\patchf{\nextpatch}{\cdot}$ and $\patchf{\nextpatch'}{\cdot}$), but
their argument $\patch$ is the same.

Patch functions \crefp{def:patchf} subtract their argument multiplied by the dilation. Consequently, the difference  of two patch functions with the same argument is constant: $\patchf{\nextpatch}{\patch} - \patchf{\nextpatch'}{\patch} = s\cdot\bra{\nextpatch - \nextpatch'}$. This means that the terms of the sum are on the same diagonal. Thus, to calculate the covariance for a given location pair $\nextpatch,\nextpatch'$, we need to do a single sum \emph{over a diagonal} of the second moment tensors $\nlinf{\ell-1}\bra{\mX, \mX'}$ and $\priorWcov{\ell}.$

This results in exact same algorithm as \citet{arora2019exact}, which convolves over the diagonals, for layers with independent weights. Its memory cost is still $O(\absshort{\layersize{\ell}}^{2})$, but the computational cost is reduced to $O(\absshort{\layersize{\ell}}^{2} \absshort{\patchsize{\ell}})$, compared to $O(\absshort{\layersize{\ell}}^{2} \absshort{\patchsize{\ell}}^{2})$ for non-diagonal covariance.

\paragraph{Diagonal propagation with independent weights.}
Exactly \emph{which} diagonal of $\nlinf{\ell-1}\bra{\mX, \mX'}$ do we need to sum over? Clearly, it is the one indexed by $s\cdot\bra{\nextpatch - \nextpatch'}$, i.e.~the one that contains the position $\bra{s\nextpatch, s\nextpatch'}$. Thus, the number of diagonals of $\nlinf{\ell-1}\bra{\mX, \mX'}$ that we will need to access is exactly the number of possible values that $\nextpatch-\nextpatch'$ can take. That number is determined by the size $\layersize{\ell}$ of layer $\ell$, but is completely unrelated to the size $\layersize{\ell-1}$ of layer $\ell-1$.

Fix some layer $\ell\in\countto{L}$.
We can iterate this argument from layer $\ell$ to layer $1$ to show that, for all $m \le \ell$, the number of diagonals of $\covf{m}\bra{\mX,\mX'}$ that one needs to calculate depends only on $\layersize{\ell}$. This can yield significant computational savings when the stride is $s\ge 2$ for one or more layers.

\paragraph{Last layer not spatially extended.} When the last layer is not spatially extended, its size is $\layersize{L}=\1$, so it only has one diagonal. If all the weights of the CNN are independent, this implies that we only need to calculate one diagonal of the covariance for every layer. That is:
\begin{equation}
\covf{\ell}_{\nextpatch,\nextpatch}\bra{\mX, \mX'} =
\sum_{\patch=\1}^{\patchsize{\ell-1}}
\priorWcovs{\ell}_{\patch,\patch} \,
\nlinf{\ell-1}_{\patchf{\nextpatch}{\patch},\patchf{\nextpatch}{\patch}}\bra{\mX, \mX'}.
\label{eq:cnn-kernel-independent-diagonal}
\end{equation}
With this simplification, the convolutions required to calculate the kernel are
$D$-dimensional, bringing the memory cost to $O(\absshort{\layersize{\ell}})$ and computational cost to
$O(\absshort{\layersize{\ell}}\absshort{\patchsize{\ell}})$, same as the finite CNN \citep{garriga2018infiniteconv}. The resulting kernel is equivalent to that of a
locally connected network.

\subsection{Implementation}
We extend the \texttt{neural-tangents} \citep{neuraltangents2020} library with a convolution layer and a fully connected layer, that admit a 4-dimensional covariance tensor for the weights. This allows interoperation with existing layers.
% 4-dimensional convolutions are expressible in most common deep learning frameworks (Jax, Pytorch, Tensorflow). However, these libraries only implement 4-d convolutions in the CPU, because they are a very uncommon operation. We express the 4-d convolution with a size $k \times k \times k \times k$ convolutional kernel as a sum over $d$ 3-d convolutions \citep{funke4dconv}, which can be accelerated using the GPU. Because convolutional kernels are usually small ($\le 5 \times 5$), this still provides a speedup over the CPU.

Since 4d convolutions are uncommon in deep learning, our implementation uses a sum over $\patchh{\ell}$ 3-d convolutions, where $\patchh{\ell}=3$ is the spatial height of the convolutional filter. While this enables GPU acceleration, computing the kernel is a costly operation. Reproducing our results takes around 10 days using an nVidia RTX 2070 GPU. Access to computational resources limited our experiments to subsets of data on CIFAR-10.

\section{Experiments}
By considering different amounts of correlation, we can interpolate between existing architectures that use independent weights or full mean-pooling. We consider two possible benefits of using this larger, continuously parameterised space of models:
\begin{enumerate}[label=\textbf{\arabic*)}]
    \item Decreased reliance on discrete architectural choices like mean-pooling.
    \item Improved performance by finding a better model in the expanded search space.
\end{enumerate}

Discrete choices pose a challenge for architecture search, as a separate network needs to be trained to evaluate the effect of each choice, which is computationally expensive. Continuous choices are preferable, as gradients can often be used to adjust many choices simultaneously. We investigate whether the discrete choice of mean-pooling can instead be replaced by a suitable selection of the continuous correlation parameter in a larger convolutional filter. While searching in this larger space of kernels, we also hope to observe improved performance. We investigate these two questions by performing parameter search in the next two sections.

% Given that this generalisation  continuously parameterised generalisation 
% We seek to investigate two questions. 
% \begin{enumerate}[label=\textbf{\arabic*)}]
%     \item 
% \end{enumerate}

% We seek to test two hypotheses. 1) Can we eliminate architectural choices, and
% recover their effect using continuous hyperparameters instead? 
% 2) In the additional search space we have uncovered, can we find a kernel that
% performs better than the existing 

\subsection{Experimental setup}
We evaluate various models on class-balanced subsets of CIFAR-10 of size $2^i \cdot 10$, following \citet{arora2020small}. As is
standard practice in the wide network literature, we reframe classification as
regression to one-hot targets $\vY$. %$Y_{nk} = \indicator{t_n = k} - C$.
We subtract $C=0.1$ from $\vY$ to make its mean zero, but we observed that this affects the results very little. The prediction is the class $k$ with highest mean of the posterior Gaussian process
\begin{align}
\text{label}(x_*) &= \text{argmax}_k \,f_k(x_*) \nonumber \\
&= \text{argmax}_k \,\vK_{x_* \vX}\bra{\sigma^2\eye + \vK_{\vX\vX}}^{-1}\vY_{:,k}\,,
\label{eq:mean-gp}
\end{align}
where $\sigma^2$ is a hyperparameter, the variance of the observation noise of the GP regression. We perform cross-validation to find a setting for $\sigma^2$. We use the eigendecomposition of $\vK_{xx}$ to avoid the need to recompute the inverse for each value of $\sigma^2$.

In the next two experiments we investigate the cross-validation performance on subsets of CIFAR-10 for a sweep of correlation parameters on two different neural network architectures.
We consider two architectures used in the neural network kernel literature, the CNN-GP \citep{novak2019infiniteconv,arora2019exact} with 14 layers, and the Myrtle network \citep{shankar2020without} with 10 layers. The CNNGP-14 architecture $((\texttt{conv}, \texttt{relu})\times 14, \texttt{pool})$ has a $32 \times 32$-sized layer at the end, which is usually transformed into the $1 \times 1$ output using global average pooling. The Myrtle10 architecture $(((\texttt{conv},\texttt{relu})\times 2, \texttt{pool}_{2\times 2}) \times 3, \texttt{pool})$ has a $8\times 8$ pooling layer at the end.


% \paragraph{Implementation}
% To calculate the least-squares regression solution, common practice is to use the Cholesky decomposition of $\bra{\sigma^2\eye + \vK_{xx}}$ and a triangular solve. Since we need to cross-validate over many settings of $\sigma$, this would be costly, as it would require recomputation.
% For our purposes, this has two disadvantages. First, it needs to be solved once for every different $\sigma^2$ that we try during cross-validation. Additionally, because of floating point rounding errors, $\vK_{xx}$ often has small negative eigenvalues, in which case the Cholesky decomposition needs to be attempted several times, usually with exponentially increasing values of $\sigma^2$.
% Instead we use the eigendecomposition $\vK_{xx} = \vQ\vLambda\vQ^\tp$, calculated once, we solve both of these problems. The least-squares regression solution (GP mean) is $\vK_{*x}\vQ\bra{\sigma^2\eye + \vLambda}^{-1}\vQ^\tp\vY$, which only requires solving for a diagonal matrix, and an inner product. This allows us to search over many values of $\sigma$ very quickly. This also tells us what the smallest value of $\sigma^2$ has to be to make the kernel matrix positive definite.

% For all experiments, we find the maximum cross-validation accuracy over 1000 values of $\log \sigma^2$ linearly spaced in $[\max(c, -36), 5]$, where $e^c$ is the smallest value such that $e^c + \lambda_i > 0$ in floating-point arithmetic, and $\lambda_i$ is the smallest eigenvalue.

\begin{figure*}
\centering
 \scalebox{1.0}{\input{fig/myrtle10.pgf}}
  \caption{Cross-validation accuracy of the CNNGP-14 and Myrtle10 networks on subsets of CIFAR10, with varying lengthscale of the Matérn-$3/2$ kernel that determines the weight correlation in the last layer. With larger data set sizes $N$, the improvement is larger, and the optimal lengthscale $\lambda$ converges to a similar value $(\lambda \approx 17)$. For all data sets except the largest, the values are averaged over several runs, and the thin lines represent the $\pm 2\sigma_n$, the estimated standard deviation of the mean. We can improve the performance of the classifier by choosing an intermediate $\lambda$. \label{fig:last-layer}}
\end{figure*}

\begin{figure*}[htpb]
  \centering
  \scalebox{1.0}{\input{fig/internal_correlation.pgf}}.
  \caption{Correlated weights in intermediate layers. We replace pooling layers in the Myrtle10 architecture with larger convolutional filters with correlated weights. The lengthscale, and thus the amount of correlation, is varied along the x-axis. By adding correlations to a convolutional layer, we can recover (but not, in this case, exceed) the performance of the hand-selected architecture with mean-pooling. \label{fig:all-layers}}
\end{figure*}

\subsection{Correlated weights in the last layer}
We begin by investigating the addition of correlations in the weights of the final layer, since this is sufficient to prevent the disappearance of spatially correlated activations.  Following \citet{dutordoir2020}, the covariance $\vSigma_{pp'}$ of the weights is given by the Matérn-3/2 kernel with lengthscale $\lambda$:
\begin{equation}
    \priorWcovs{L}_{\patch,\patch'} = \bra{1 + \frac{\sqrt{3}\bracket{\|}{\|}{\patch - \patch'}_2}{\lambda}} \exp\bra{-\frac{\sqrt{3}\bracket{\|}{\|}{\patch - \patch'}_2}{\lambda}}.
\end{equation}
where we see the patch locations $\patch,\patch'$ as vectors. The ``extremes''
of independent weights and mean pooling are represented by
$\priorWcovs{L}_{\patch,\patch'} = \delta_{\patch,\patch'}$ and
$\priorWcovs{L}_{\patch,\patch'} = 1$, respectively. \footnote{Strictly
  speaking, $\priorWcov{L} = {\boldsymbol 1\boldsymbol 1}^\tp$ corresponds to
  sum-pooling, but the missing constant $\absshort{\layersizebase}^{-2}$ does
  not affect the maximum in \cref{eq:mean-gp}.}

\Cref{fig:last-layer}, shows how the 4-fold cross-validation accuracy on the training sets varies with the lengthscale $\lambda$ of the Matérn-3/2 kernel, which controls the ``amount'' of spatial correlation in the weights of the last layer. For each data point in each line, we split the data set into 4 folds, and we calculate the test accuracy on 1 fold using the other 3 as training set, for each value of $\sigma$ that we try. We take the maximum accuracy over $\sigma$.

We investigate how the effect above varies with data set size. The results in \cref{fig:last-layer} show that particularly for the CNNGP-14 architecture, correlated weights in the final layer lead to a modest but consistent improvement in performance, with the effect becoming larger with increasing dataset size. We can also see the optimal lengthscale $\lambda$  converging to a similar value for both architectures, of about $\lambda \approx 17$, which is evidence that the improvement holds for larger data sets. The optimal lengthscale is the same for both networks, so we speculate it may be a property of the {CIFAR10} data set.

\paragraph{Data partitioning.} The largest data set size in each part of the plot was run only once because of computational constraints. We transform one data set of size $N$ into two data sets of size $N/2$ by taking block diagonals of the stored kernel matrix, so we have more runs for the smallest sizes. This is an unbiased \acl{MC} estimate of the true accuracy under the data distribution, since the individual data points are uniformly distributed (but not independent, since they are sampled without replacement). It also has less variance than independent data sets, because the data sets taken are anti-correlated; they have no points in common. Accordingly, the error bars in \cref{fig:last-layer,fig:all-layers} are an estimate of the standard error: the square root of an upwards-biased estimator of the variance of the mean.

\paragraph{Implementation.} We use the \texttt{neural-tangents} \citep{neuraltangents2020} library to calculate the spatial kernel at the previous-to-last layer, $\covf{L-1}\bra{\mX,\mX'}$, once.
Since only the lengthscale of the last layer changes, we can cheaply obtain the final layer kernel matrix $\vK\ssup{L}_{\vX\vX'}$ for all lengthscales.

\subsection{Correlated weights in intermediate layers}
We take the same approach to the experiment in \cref{fig:all-layers}. To investigate whether correlated weights can replace mean-pooling, we replace the $2\times 2$ intermediate mean-pooling layer, together with the next $3\times 3$ convolution layer, in the Myrtle10 architecture with correlated weights. We change them to a $6 \times 6$ weight-correlated convolution. We vary the lengthscale for the covariance of all the newly correlated layers, setting them to the same value.

We observe that for independent weights (lengthscale is 0) the performance of the network is significantly below the optimum. Correlating the weights improves performance, although after adding small amounts of correlation, performance stays roughly constant. This indicates that for intermediate layers mean-pooling is not a sub-optimal choice, as it is for the last layer. However, the amount of correlation is a continuous parameter, which could lead to avoiding this discrete choice in model architecture.

\paragraph{Implementation.}
In this experiment, the lengthscales vary across the whole network, so we need to calculate $\covf{L-1}\bra{\mX,\mX'}$ every time. For a given data set size, this makes each point in \cref{fig:all-layers} considerably more expensive. For each data point, we optimise over the lengthscale of the last layer like in \cref{fig:last-layer}, picking the one with highest cross-validation accuracy.


\section{Related work}
Infinitely wide limits of neural networks are currently an important tool for creating approximations and analyses. Here we provide a background on the different infinite limits that have been developed, together with a brief overview of where they have been applied.

Interest in infinite limits first started with research into properties of Bayesian priors on the weights of neural networks. \citet{neal1996bayesian} noted that prior function draws from a single hidden layer neural network with appropriate Gaussian priors on the weights tended to a Gaussian process as the width grew to infinity. The simplicity of performing Bayesian inference in Gaussian process models led to their widespread adoption soon after \citep{williams1996gpr,gpml}. Over the years, the wide limits of networks with different weight priors and activation functions have been analysed, leading to various \emph{kernels} which specify the properties of the limiting Gaussian processes \citep{williams1997inf,cho2009mkm}.

With the increasing prominence of deep learning, recursive kernels were introduced in an attempt to obtain similar properties. \citet{cho2009mkm,mairal2014ckn} investigated such methods for fully-connected and convolutional architectures respectively. Despite similarities between recursive kernels and neural networks, the derivation did not provide clear relationships, or any equivalence in a limit. \citet{hazan2015} took initial steps to showing the wide limit equivalence of a neural network beyond the single layer case. Recently, \citet{matthews2018dnnlimit,lee2018dnnlimit} simultaneously provided general results for the convergence of the prior of deep fully-connected networks to a GP.\footnote{The derivation of the limiting kernel differs between the two papers, with the results being consistent. \citet{matthews2018dnnlimit} carefully take limits of realisable networks, while \citet{lee2018dnnlimit} take the infinite limit of each layer sequentially.%Follow-on work \citep{matthews2018dnnlimit2,novak2019infiniteconv} further relax technical restrictions.
} 
A different class of limiting kernels, the Neural Tangent Kernel (NTK), originated from analysis of the function implied by a neural network during optimisation \citep{jacot2018ntk}, rather than the prior implied by the weight initialisation. Just like the Bayesian prior limit, this kernel sheds light on certain properties of neural networks, as well as providing a method with predictive capabilities of its own. 
The two approaches end up with subtly different kernels, which both can be computed as a recursive kernel. Both such infinite limits have recently been used for predicting and analysing training properties of finite neural networks \citep{poole2016,schoenholz2017deepinformationpropagation,hayou2019activation}, as well as for (Bayesian) training of infinitely wide networks.

With the general tools in place, \citet{garriga2018infiniteconv,novak2019infiniteconv} derived limits of the prior of convolutional neural networks with infinite filters. %\footnote{This time with \citet{garriga2018infiniteconv} using the simpler sequential infinite limits, and \citet{novak2019infiniteconv} blah blah}
These two papers directly motivated this work by noting that spatial correlations disappeared in the infinite limit. Spatial mean pooling at the last layer was suggested as one way to recover correlations, with \citet{novak2019infiniteconv} providing initial evidence of its importance. Due to computational constraints, they were limited to using a Monte Carlo approximation to the limiting kernel, while \citet{arora2019exact} performed the computation with the exact NTK. Very recent preprints provide follow-on work that pushes the performance of limit kernels \citep{shankar2020without} and demonstrated the utility of limit kernels for small data tasks \citep{arora2020small}. Extending on the results for convolutional architectures, \citet{yang2019wide} showed how infinite limits could be derived for a much wider range of network architectures.

In the kernel and Gaussian process community, kernels with convolutional structure have also been proposed. Notably, these retained spatial correlation in either a fixed \citep{vdw2017convgp} or adjustable \citep{mairal2014ckn,dutordoir2020} way. While these methods were not derived using an infinite limit, \Citet{vdw2019thesis} provided an initial construction from an infinitely wide neural network limit. Inspired by these results, we propose limits of deep convolutional neural networks which retain spatial correlation in a similar way.

\section{Conclusion}
The disappearance of spatial correlations in infinitely wide limits of deep convolutional neural networks could be seen as another example of how Gaussian processes lose favourable properties of neural networks. While other work sought to remedy this problem by changing the architecture (mean-pooling), we showed that changing the weight prior could achieve the same effect. Our work has three main consequences:
\begin{enumerate}
    \item Weight correlation shows that locally connected models (without spatial correlation) and mean-pooling architectures (with spatial correlation) actually exist at ends of a spectrum. This unifies the two views in the neural network domain. We also unify two known convolutional architectures that were introduced from the Gaussian process community.
    \item We show empirically that performance improvements can be gained by using weight correlations \emph{between} the extremes of locally connected networks or mean-pooling. We also show that mean-pooling in intermediate layers can be replaced by weight correlation in infinitely wide architectures.
    \item Using weight correlation may provide advantages during hyperparameter tuning. Discrete architectural choices need to be searched through simple evaluation, while continuous parameters can use gradient-based optimisation. While we have not taken advantage of this in our current work, this may be a fruitful direction for future research.
\end{enumerate}
% Firstly, 
% Secondly, 
% Finally, 

% Implications
% - Continuous parameter
% - Bayesian priors

\begin{acknowledgements}
The authors would like to thank the reviewers for helpful comments.
AGA was supported by a UK Engineering and Physical Sciences Research Council studentship [1950008].
\end{acknowledgements}

\bibliography{garriga-alonso_752}
\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
