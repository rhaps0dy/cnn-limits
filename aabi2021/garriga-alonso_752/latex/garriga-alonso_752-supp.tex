\documentclass[accepted]{uai2021} % for initial submission
% \documentclass[accepted]{uai2021} % after acceptance, for a revised
                                    % version; also before submission to
                                    % see how the non-anonymous paper
                                    % would look like
%% There is a class option to choose the math font
% \documentclass[mathfont=cm]{uai2021} % Computer Modern math instead of
                                       % ptmx, like default for UAI ≤2020
% \documentclass[mathfont=newtx]{uai2021} % newtx fonts (improves upon
                                          % ptmx; less tested, no support)
% NOTE: Only keep *one* line above as appropriate, as it will be replaced
%       automatically for papers to be published. Do not make any other
%       change above this note for an accepted version.

%% Choose your variant of English; be consistent
% \usepackage[american]{babel}
\usepackage[british]{babel}
\DeclareUnicodeCharacter{2212}{-}  % matplotlib outputs this
\usepackage{balance}
%% Some suggested packages, as needed:
\usepackage{natbib} % has a nice set of citation styles and commands
    \bibliographystyle{plainnat}
    \renewcommand{\bibsection}{\subsubsection*{References}}
\usepackage{mathtools} % amsmath with fixes and additions
% \usepackage{siunitx} % for proper typesetting of numbers and units
\usepackage{booktabs} % commands to create good-looking tables
\usepackage{tikz} % nice language for creating drawings and diagrams
\usepackage{xr}
\externaldocument{garriga-alonso_752}

%% Provided macros
% \smaller: Because the class footnote size is essentially LaTeX's \small,
%           redefining \footnotesize, we provide the original \footnotesize
%           using this macro.
%           (Use only sparingly, e.g., in drawings, as it is quite small.)


\title{Correlated Weights in Infinite Limits of Deep Convolutional Neural Networks (Supplementary material)}

% The standard author block has changed for UAI 2021 to provide
% more space for long author lists and allow for complex affiliations
%
% All author information is authomatically removed by the class for the
% anonymous submission version of your paper, so you can already add your
% information below.
%
\author[1]{\href{mailto:Adrià Garriga-Alonso <ag919@cam.ac.uk>?Subject=Your UAI 2021 paper}{Adrià~Garriga-Alonso}{}}
\author[2]{Mark~van~der~Wilk}
% Add affiliations after the authors
\affil[1]{%
    Department of Engineering\\
    University of Cambridge\\
    UK
}
\affil[2]{%
    Department of Computer Science\\
    Imperial College London\\
    UK
}

%% CUSTOM PACKAGES
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Useful packages
% \usepackage{subcaption}
\usepackage{xcolor}
%\usepackage{amsmath}
\usepackage{amsfonts}


\usepackage{graphicx}
% \usepackage[colorlinks=true, allcolors=blue]{hyperref}
% \usepackage{paralist}
% \usepackage{listings}
% \lstset{
%     showstringspaces=false,
%     basicstyle=\ttfamily,
%     breaklines=true,
%     breakatwhitespace=true,
% }
% \usepackage{booktabs}
\usepackage{cancel}
\usepackage{import}  % Inkscape figures

\usepackage[colorinlistoftodos]{todonotes}
\newcommand{\markcomment}[1]{\todo[color=green]{#1}\xspace}
\newcommand{\adriacomment}[1]{\todo[color=blue]{#1}\xspace}

% Algorithm package from JMLR
\usepackage{algorithm}
\usepackage[algo2e,ruled]{algorithm2e}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{amsthm}
% Cleveref must go after amsthm but before theorem definitions, https://tex.stackexchange.com/a/19106/107809
\usepackage{cleveref}  % may add 'noabbrev'
\creflabelformat{equation}{#2#1#3}
\crefname{assumption}{assumption}{assumptions}
\usepackage{adria-math}  % theorem definitions here

\input{acronyms.tex}

%% Mathematical commands
\usepackage{xspace}
% \creflabelformat{equation}#2#1#3}

%% NETSOR language
\newcommand{\Netsor}{\textsc{Netsor}\xspace}
\newcommand{\Gva}{\mathsf{G}}
\newcommand{\Hva}{\mathsf{H}}
\newcommand{\Ava}{\mathsf{A}}
\newcommand{\Ova}{\mathsf{O}}
\newcommand{\MatMul}{\texttt{MatMul}\xspace}
\newcommand{\LinComb}{\texttt{LinComb}\xspace}
\newcommand{\Nonlin}{\texttt{Nonlin}\xspace}
\newcommand{\SMatMul}[1]{{\MatMul: {#1}}}
\newcommand{\SLinComb}[1]{{\LinComb: {#1}}}
\newcommand{\SNonlin}[1]{{\Nonlin: {#1}}}

\newcommand{\SInput}{\Input}
\newcommand{\SOutput}{\Output}
\newcommand{\CommentC}{\tcc}

\newcommand{\Sigmax}{{\Sigma_{\text{x}}}}
\newcommand{\Sigmay}{{\Sigma_{\text{y}}}}
\newcommand{\Sigmaxy}{{\Sigma_{\text{xy}}}}


% Neural netowrk notations
\newcommand{\layerAchar}{Z}
\newcommand{\layerAd}[2]{\mathbf{Z}^{(#1)}_{#2}(\vX')}
\newcommand{\layerAsd}[2]{Z^{(#1)}_{#2}(\vX')}
\newcommand{\layerAsm}[3]{Z^{(#1)}_{#2}\bra{#3}}
\newcommand{\layerAs}[2]{Z^{(#1)}_{#2}(\vX)}
\newcommand{\layerA}[2]{\mathbf{Z}^{(#1)}_{#2}(\mX)}
\newcommand{\layera}[2]{\mathbf{Z}^{(#1)}_{#2}(\mX)}
\newcommand{\layeram}[3]{\mathbf{Z}^{(#1)}_{#2}\bra{#3}}
\newcommand{\layerAm}[3]{\mathbf{Z}^{(#1)}_{#2}\bra{#3}}
\newcommand{\layerAmmm}[1]{\mathbf{Z}^{(#1)}}
\newcommand{\layerasm}[3]{Z^{(#1)}_{#2}\bra{#3}}

\newcommand{\layerC}[1]{C^{(#1)}}

\newcommand{\layerNLAd}[2]{\vA^{(#1)}_{#2}(\vX')}
\newcommand{\layerNLAsd}[2]{A^{(#1)}_{#2}(\vX')}
\newcommand{\layerNLAsmp}[3]{A^{(#1)}_{#2}\bra{#3}}
\newcommand{\layerNLAsm}[3]{A^{(#1)}_{#2}\bra{#3}}
\newcommand{\layerNLAs}[2]{A^{(#1)}_{#2}(\vX)}
\newcommand{\layerNLA}[2]{\vA^{(#1)}_{#2}(\vX)}
\newcommand{\layerNLAm}[3]{\vA^{(#1)}_{#2}\bra{#3}}
\newcommand{\layernlasm}[3]{\vA^{(#1)}_{#2}\bra{#3}}
\newcommand{\layernla}[2]{\vA^{(#1)}_{#2}(\mX)}

\newcommand{\layerBsp}[2]{\mH\ssup{#1}_{#2}}  % TODO change

\newcommand{\layerUs}[1]{U^{(#1)}}
\newcommand{\layerU}[1]{\vU^{(#1)}}

\newcommand{\layerWs}[1]{W^{(#1)}}
\newcommand{\layerW}[1]{\mW^{(#1)}}
\newcommand{\layerWbase}{\mW}
\newcommand{\layerbs}[1]{b^{(#1)}}
\newcommand{\layerb}[1]{\vb^{(#1)}}

\newcommand{\layersizebase}{\vF}
\newcommand{\layersizebases}{F}
\newcommand{\layersize}[1]{{\layersizebase^{\bra{#1}}}}
\newcommand{\layerw}[1]{F_\text{w}^{(#1)}}
\newcommand{\layerh}[1]{F_\text{h}^{(#1)}}
\newcommand{\layerwbase}{F_\text{w}}
\newcommand{\layerhbase}{F_\text{h}}

\newcommand{\patchidx}{q}
\newcommand{\patchsizebase}{\vP}
\newcommand{\patchsizebases}{P}
\newcommand{\patchsize}[1]{{\patchsizebase^{\bra{#1}}}}
\newcommand{\patchsizes}[1]{\patchsizebases^{\bra{#1}}}
\newcommand{\patchw}[1]{P_\text{w}^{(#1)}}
\newcommand{\patchh}[1]{P_\text{h}^{(#1)}}
\newcommand{\patchwbase}{P_\text{w}}
\newcommand{\patchhbase}{P_\text{h}}
\newcommand{\patchf}[2]{{\tilde#1\bra{#2}}}
\newcommand{\convpatch}[1]{{\text{im}\bra{\tilde#1}}}

\newcommand{\priorLcov}[1]{\vL^{(#1)}}
\newcommand{\priorLscov}[1]{L^{(#1)}}
\newcommand{\priorWcovs}[1]{\Sigma^{(#1)}}
\newcommand{\priorWcov}[1]{\boldsymbol{\Sigma}^{(#1)}}

% These 4 are used for the picture below
\newcommand{\chan}{i}
\newcommand{\prevchan}{j}   % <- Possibly use these for c, p, gamma? I've
\newcommand{\patch}{\vp}               % been using \nu,\mu
\newcommand{\patchs}{p}               % been using \nu,\mu
\newcommand{\nextpatch}{\vq}
\newcommand{\nextpatchs}{q}

\newcommand{\sigmaw}{{\sigma^{2}_{\text{w}}}}

\newcommand{\meanf}[1]{m\ssup{#1}}
\newcommand{\covf}[1]{K\ssup{#1}}
\newcommand{\nlinf}[1]{V\ssup{#1}}
\newcommand{\vmeanf}[1]{\vm\ssup{#1}}
\newcommand{\mcovf}[1]{\mK\ssup{#1}}
\newcommand{\vecf}[1]{\text{vec}\bra{#1}}
\newcommand{\diagf}[1]{\text{diag}\bra{#1}}
\newcommand{\numelf}[1]{\text{ne}\bra{#1}}


% bold numbers
\newcommand{\0}{\boldsymbol{0}}
\newcommand{\1}{\boldsymbol{1}}


\newcommand{\crefp}[1]{(\cref{#1})}
\newcommand{\eqparref}{\crefp}
\newcommand{\eqparreftwo}[2]{\crefp{#1,#2}}


\begin{document}
\maketitle

\appendix % UAI wants the appendices in the same pdf
\section{Patch functions and discrete convolutions} \label{app:patchf}
Usually, convolutions are defined explicitly by subtracting the indices of one input tensor from the other one, and not using patch functions. To make this paper clearer, it is convenient to abstract the details of a convolution, so we introduced the patch function.

\begin{definition}[Discrete convolution]
  \label{def:convolution}
  Let $D \in \sN$ be a number of spatial dimensions, the tensor-valued weights
  $\mW\in \sR^{\patchsizebase}$, input $\mX\in \sR^{\vF}$, and output $\mY \in \sR^{\vF'}$.
  The tensor sizes $\patchsizebase$ (patch size) and $\layersizebase,\layersizebase'$ (feature sizes) are each a
  $D$-tuple, $\patchsizebase,\layersizebase,\layersizebase' \in \sN^{D}$.
  We say that $\mY$ is the result of the convolution operation $\mY = \mW*\mX$, if
  \begin{equation}
    Y_{\nextpatchs_{1},\dots,\nextpatchs_{D}} = \sum_{\patchs_{1}=1}^{P_{1}} \dots \sum_{\patchs_{D}=1}^{P_{D}} W_{\patchs_{1},\dots,\patchs_{D}} X_{\patchf{\nextpatchs_{1}}{\patchs_{1}}, \dots, \patchf{\nextpatchs_{D}}{\patchs_{D}}}.
    \label{eq:def-convolution-no-tuples}
  \end{equation}
  Here, $\patchf{\nextpatchs_{d}}{\cdot} : \countto{P_{d}} \to \countto{F'_{d}}$ are the \emph{patch functions} for a given output location $\nextpatch$. Using $D$-tuples $\patch,\nextpatch$ as indices, we may also write
  \begin{equation}
    Y_{\nextpatch} = \sum_{\patch=\1}^{\vP} W_{\patch} X_{\patchf{\nextpatch}{\patch}}.
    \label{eq:def-convolution-tuples}
  \end{equation}
  Counting from $\1$ to $\patchsizebase$ is done in such a way that $\patch$ takes all the values in $\countto{\patchsizebase}.$
  \end{definition}

  \begin{definition}[Patch function]
    \label{def:patchf}
    For each dimension $d \in \countto{D}$, layer $\ell\in\countto{L}$, fix a
    stride $s\in\sN$,
    and dilation $h \in \sN$.
   For output position $\nextpatch$, the patch function of the $d$th dimension $\patchf{\nextpatchs_{d}}{\cdot} : \countto{P_{d}} \to \countto{F'_{d}}$ is
\begin{equation}
  \patchf{\nextpatchs_{d}}{\patchs_{d}} = s\nextpatchs_{d} -  h\bra{\patchs_{d} - \ceil{\frac{\patchsizebases_{d}}{2}}} .
  \label{eq:def-patchf}
\end{equation}
For a $D$-tuple index $\patch$, we may compactly write $\patchf{\nextpatch}{\patch} \eqdef \bra{\patchf{\nextpatchs_{1}}{\patchs_{1}}, \dots, \patchf{\nextpatchs_{D}}{\patchs_{D}}}$.
\end{definition}
It is possible to verify that \cref{def:patchf} overall yields the usual definition of a convolution in deep learning \citep[Section~9.1]{deeplearningbook}.

\begin{remark}
  The concatenation of two patch functions $\patchf{\nextpatch}{\cdot},\patchf{\nextpatch'}{\cdot}$ is also a patch function, with argument in $\countto{\patchsizebase}^{2}$. That is, for $[\patch,\patch'] = \vs \in \sN^{\patchsizebase \times \patchsizebase}$ and $[\nextpatch,\nextpatch'] = \vr$,
  \begin{align}
    (&\patchf{\nextpatch}{\patch},\patchf{\nextpatch'}{\patch'}) \nonumber\\&=
    \bra{\patchf{\nextpatchs_{1}}{\patchs_{1}}, \dots, \patchf{\nextpatchs_{D}}{\patchs_{D}},
    \patchf{\nextpatchs'_{1}}{\patchs'_{1}}, \dots, \patchf{\nextpatchs'_{D}}{\patchs'_{D}}
    }\nonumber\\&= \patchf{\vr}{\vs}\,.
  \end{align}
  \label{rem:concat-patchf}
  \end{remark}

\section{Proof that a CNN with correlations in the weights converges to a GP}\label{app:netsor}

In this section, we formally prove that a CNN with correlated weights converges in distribution
to a Gaussian process in the limit of infinite width. Using the \Netsor
programming language due to \citet{yang2019wide}, most of the work in the proof
is done by
one step: describe a CNN with correlated weights in \Netsor.

For the reader's convenience, we informally recall the \Netsor programming language
\citep{yang2019wide} and key properties of its programs \crefp{thm:netsor-master,corollary:netsor-gp}.
The outline of our presentation here also closely follows
\citet{yang2019wide}. Readers familiar with \Netsor should skip to
\cref{sec:netsor-cnn-program}, where we show the program that proves \cref{thm:correlated-cnn-gp}.


\subsection{Definition of a \Netsor program}\label{sec:netsor-programming}
A \Netsor program expresses numerical computations, such as those used to define the output of a neural network. Each line of a \Netsor program is simply the definition of a new variable, in terms of previously defined variables.

There are three types of variables: $\Gva(n)$-vars, $\Ava(n_1, n_2)$-vars, and
$\Hva(n)$-vars (henceforth called ``\Netsor variables''). Each of these have one
or two parameters, which are the widths we will take to infinity. For a given
index in $\countto{n}$ (or $\countto{n_1} \times \countto{n_2}$), each \Netsor
variable is a random \emph{scalar}. To represent vectors that do not grow to
infinity, we need to use collections of \Netsor variables.

$\Gva$-vars, $\Ava$-vars and $\Hva$-vars are all random when the program is run. To accomodate non-random variables that may change (like the input $\mX$ to a \ac{NN}) we must define a different \Netsor program, defining $\mX$ either as a constant or a $\Gva$-var with variance zero.

What follows is an explanation of the three kinds of \Netsor variables, and
example uses of them. The program indicates the type of a variable using
``$\text{var} : \text{Type}$''.
\begin{itemize}
\item[$\Gva$-vars] (Gaussian-vars) are $n$-wise \emph{approximately}
\ac{iid} and Gaussian. By ``$n$-wise (approximately) independent'' we mean that there can be
correlations between $\Gva$-vars, but only within a single index $i \in 1,\dots,n$.
$\Gva$-vars will converge in
distribution to an $n$-wise independent, identically distributed Gaussian in the limit of $n \to \infty$, if
all widths are $n$. They are used, for example, to define the biases of a \ac{FCNN}.

\item[$\Ava$-vars] represent matrices,
like the weight matrices of a dense neural network. Their entries are always \ac{iid}
Gaussian with with zero mean, even for finite instantiations of the program
(finite $n$).
There are no correlations between different $\Ava$-vars, or elements of the same $\Ava$-var. They may be used to define the weight matrices of a \ac{FCNN}.

\item[$\Hva$-vars] represent variables that become $n$-wise \ac{iid} (not necessarily
Gaussian) in the
infinite limit. $\Gva$ is a subtype of $\Hva$, so all $\Gva$-vars are also $\Hva$-vars. Post-nonlinearity activations are $\Hva$-vars.

\item[$\Ova$-vars] (Output-vars) are used to define the output of the \Netsor
program. A $\Ova(n)$-var behaves like you would expect a hypothetical $\Ava(n, 1)$-var to behave: its elements are \ac{iid} Gaussian with mean zero, and it is independent of all other variables in the program.
\end{itemize}

\citet{yang2019wide} does not define $\Ova$-vars, instead choosing to consider them part of the $\Gva$-vars, since they both converge to \ac{iid} Gaussians.

\begin{definition}[Netsor program]
A \textsc{Netsor} program consists of:

\textbf{Input:}
 A set that may contain $\Gva$-vars, $\Ava$-vars, and $\Ova$-vars.

\textbf{Body:}
Each line of the program defines a new variable in terms of existing ones.
New variables may be defined using the following rules:
\begin{itemize}
  \item\MatMul: $\Ava(n_1, n_2) \times \Hva(n_2) \to \Gva(n_1)$. Given
        an $\Ava(n_{1}, n_{2})$-var (\ac{iid} Gaussian matrix) and an $\Hva(n_{2})$-var (\ac{iid} vector), its multiplication is a $\Gva(n_{1})$-var (that is, it
        converges to a Gaussian vector in the limit $n_2 \to \infty$).
  \item\LinComb: Given constants $\alpha_1,\dots,\alpha_K$, and $\Gva$-vars
    $x_1,\dots,x_K$ of type $\Gva(n_1)$, their linear combination $\sum_{k=1}^K \alpha_k
    x_k$ is a $\Gva$-var.
  \item\Nonlin: applying an elementwise nonlinear function $\phi : \sR^K \to
    \sR$, we map
    several $\Gva$-vars $x_1,\dots,x_K$ to one $\Hva$-var.
\end{itemize}

\textbf{Output:}
A tuple of scalars $(o_1^\tp x_1, \;\dots, \; o_K^\tp
x_K/\sqrt{n_K})$. The variables $o_k : \Ova(n_{k})$ are $\Ova$-vars. It may be the case that $o_j = o_k$ for
different $j, k$ (that is, the list $[v_{1},\dots,v_{K}]$ has repeated entries). Each $x_k : \Hva(n_k)$ is a $\Hva$-var.
\end{definition}

Outside of these rules, \Netsor does not have conditionals or loops\footnote{Of course, a nonlinearity $\phi$ may be internally defined using loops and conditionals, so long as it satisfies \cref{asm:controlled}.}. In
practice, we may use loops and conditionals to write a \Netsor program, so long
as they do not access the values of \Netsor variables. Conceptually, these
behave like a LISP-style ``macro'' that generates a \Netsor program.

% \begin{remark} The \texttt{LinComb} rule is redundant, since a linear combination may be expressed as a \texttt{Nonlin} rule, and \texttt{MatMul} takes as input a $\Hva$-var. \citet{yang}
% \end{remark}

\subsection{The output of a \Netsor program converges to a Gaussian process}\label{sec:netsor-converges}
For simplicity, we assume that the width of all the \Netsor variables is $n$.
\Citet{yang2019wide} also considers the case where each $n_{k}$ is different. First, the necessary assumptions.

\begin{definition}[Controlled function \citep{yang2019wide}]
  A function $\phi: \sR^k \to \sR$ is \emph{controlled} if it is measurable and
  \[ \abs{\phi(\vx)} \le \text{exp} \bra{C\bracket{\|}{\|}{\vx}_2^\bra{2-\epsilon}}
    + c \]
  for some
  $C,c,\epsilon > 0$, where $\|\cdot\|_2$ is the L2 norm.
  \label{def:controlled-function}
\end{definition}

If a function is controlled, it is L2 integrable with a Gaussian. That is, if
the argument $x$ of the function is Gaussian, the variance of $\phi(x)$ is
finite. This in turn ensures that the \ac{NN} function has finite variance. All
common nonlinearities (ReLU, tanh, SiLU, \dots) are controlled. This is a very
weak assumption, it is vanishingly unlikely that future nonlinearities will grow as fast
as $O\bra{e^{x^{2}}}$.

\begin{assumption}
  All nonlinear functions $\phi(\cdot)$ in the \Netsor program are controlled.
  \label{asm:controlled}
\end{assumption}

\begin{assumption}[Distribution of $\Ava$-var inputs] Consider each $\Ava(n, n)$-var in the program, $\mW$. Each of its elements
  $W_{\chan,\prevchan}$, where $\chan,\prevchan \in \countto{n}$, is  sampled from the zero-mean, \ac{iid} Gaussian, $W_{\chan,\prevchan} \simiid \Normal{0,
    \sigma_{\text{w}}^2/n}.$
  \label{asm:avar-inputs}
\end{assumption}
\begin{assumption}[Distribution of $\Gva$-var inputs]
  Consider the input vector of all $\Gva(n)$-vars for each channel $\chan \in \countto{n}$,
  that is the vector $\vz_\chan \eqdef [x_\chan : x\text{ is input
  }\Gva\text{-var}]$. It is drawn from a Gaussian, $\vz_\chan \simiid
  \Normal{\vmu^\text{in}, \vSigma^\text{in}}$.
  The covariance $\vSigma^\text{in}$ may be
  singular.\label{asm:gvar-inputs}
\end{assumption}
\begin{assumption}[Distribution of $\Ova$-vars]
  Each $\Ova(n_{k})$-var $v_{k}$ in the program is an independent Gaussian for each channel. Different $\Ova$-vars may have different variances.
  That is, for each $k\in\countto{K}, \chan\in\countto{n}$, $v_{k,\chan} \simiid \Normal{0, \sigma_{k}^{2}/n_{k}}$.
  \label{asm:ovar-inputs}
\end{assumption}

\begin{theorem}[\Netsor master theorem, \citealp{yang2019wide}]
  Fix any \Netsor program satisfying \cref{asm:controlled,asm:avar-inputs,asm:gvar-inputs,asm:ovar-inputs}. If $g\ssup{1},\dots,g\ssup{M}$ are all the $\Gva$-vars in the entire program, then for any controlled $\psi : \sR^{M} \to \sR$, as $n\to\infty$,
  \begin{multline}
    \frac{1}{n}\sum_{\chan=1}^{n} \psi\bra{g\ssup{1}_{\chan},\dots,g\ssup{M}_{\chan}}\\\asconverges \Expect{\vz \sim \Normal{\vm, \mK}}{\psi\bra{z\ssup{1}, \dots, z\ssup{M}}}.
    \label{eq:strong-lln-psi}
  \end{multline}
  Here $\asconverges$ is almost sure convergence \citep[sec.~5.2]{probability-theory-intro}. The mean $\vm$ and covariance $\mK$ are calculated under the assumption that all the $\Gva$-vars are jointly Gaussian, like in \cref{sec:deep-correlations}.
  \label{thm:netsor-master}
\end{theorem}
\begin{proof}[Proof sketch]
  The proof is by induction on the number of $\Gva$-vars included in the output,
  added in order of definition. The induction invariant is that, for some $m < M$, \cref{eq:strong-lln-psi} holds; and that a subset of $\Gva$-vars in $\countto{m}$ which form a basis have a non-singular distribution. The detailed proof is in \citet[Appendix~H]{yang2019wide}.
\end{proof}

The following corollary is a consequence of the Master theorem
(\ref{thm:netsor-master}) and the \acl{CLT}.

\begin{corollary}[Corollary~5.5, abridged, \citealp{yang2019wide}]
Fix any \Netsor program which satisfies \cref{asm:controlled,asm:avar-inputs,asm:gvar-inputs,asm:ovar-inputs}.
For simplicity, fix
  the widths of all the variables to $n$. The program outputs are $(o_{1}^\tp x_1, \;\dots, \; v_K^\tp
  x_K)$, where
  each $x_k$ is an $\Hva$-var, and each
    $o_k$ is a $\Ova$-var.
  Then, as $n \to \infty$, the output tuple
  converges in distribution to a Gaussian $\Normal{{\boldsymbol{0}}, \vK}$.
  The covariance $\vK$ is given by doing calculations like \cref{sec:deep-correlations}, assuming that $\Gva$-vars are jointly Gaussian.
  \label{corollary:netsor-gp}
\end{corollary}

\subsection{\Netsor program and GP behaviour: CNN with correlated weights}\label{sec:netsor-cnn-program}
\Netsor only has native support for matrix-vector multiplications and linear
combinations with constants. How can we represent a convolution operation for \ac{CNN}? Consider the convolutional layer definition \eqparref{eq:deep-recursion}. Changing the sum order, we obtain
\begin{align}
  \intertext{Expanding the convolution into a sum, and changing the sum order, we obtain}
 \layerAsm{\ell}{\chan,\nextpatch}{\mX} &= \sum_{\patch=\1}^{\patchsize{\ell}} \sum_{\prevchan=1}^{\layerC{\ell-1}}\layerWs{\ell}_{\chan,\prevchan,\patch}
                         \;{\layernlasm{\ell-1}{\prevchan,\patchf{\nextpatch}{\patch}}{\mX}},
\intertext{which is just a spatial sum of matrix multiplications}
\layera{\ell}{:,\nextpatch} &= \sum_{\patch=\1}^{\patchsize{\ell}} \layerW{\ell}_{:,:,\patch} \; \layernla{\ell-1}{:,\patchf{\nextpatch}{\patch}}.
                              \label{eq:conv-as-matmul}
\end{align}
Thus, we may express a convolution with multiple filters as a sum of matrix-vector multiplications. This is
the canonical way to represent
convolutional filters in \Netsor \citep[\Netsor program~4]{yang2019wide}.

Here we run into a problem. \Cref{eq:correlated-weight-prior} states that \ac{CNN}
filters are spatially correlated, but \cref{asm:avar-inputs} states that
$\Ava$-vars have to be independent. To solve this, we will use the the following
well-known lemma, which is the $\mR\epsilon$ expression of a Gaussian random
variable with mean zero. The tensor $\mR$ is a square root of the covariance.

\begin{lemma}
  Let $\mSigma \in \sR^{\patchsizebase^{2}}$ be an arbitrary real-valued covariance tensor.
  Then there exists another real-valued tensor $\mR \in \sR^{\patchsizebase^{2}}$ such that $\Sigma_{\nextpatch,\nextpatch'} = \sum_{\patch=1}^{\patchsizebase}R_{\nextpatch,\patch} R_{\nextpatch',\patch}$.
  Next, let $\vu,\vw \in \sR^{\patchsizebase}$ be real-valued tensors, such that $\vw = \mR \vu$. Suppose the elements of $\vu$ are \ac{iid} standard Gaussian variables, $\cb{u_\patch}_{\patch \in \countto{\patchsizebase}} \simiid \Normal{0, 1}$. Then, $\vw$ has a multivariate Gaussian distribution with mean zero and covariance tensor $\vSigma$.
  \label{lemma:R}
\end{lemma}
\begin{proof}
  Let $K = \abs{\patchsizebase}$, and $\tilde\mSigma$ be $K \times K$ matrices, obtained by flattening the dimensions of $\mSigma$ respectively.
  Then $\tilde\mSigma$ is a real-valued covariance matrix, so it is positive semi-definite and
  a square matrix $\tilde\mR$ s.t. $\tilde\mR{\tilde\mR}^\tp = \tilde\mSigma$ always exists. Un-flattening $\tilde\mR$ we obtain $\mR$. The variable $\vw$ is Gaussian because it is
  a linear transformation of the Gaussian $\vu$. Calculating the second moment of $\vw$ finishes the proof.
\end{proof}
Thus, to express convolution in \Netsor with correlated weights $\vw$, we can use the
following strategy. First, express several convolutions with uncorrelated
weights $\vu$, using \cref{eq:conv-as-matmul}. Then, combine the output of the convolutions using \LinComb and coefficients of
the tensor $\mR$.

Given a collection of $\Ava$-vars
$\cb{\layerU{\ell}_{:,:,\patch}}_{\patch \in \countto{\patchsize{\ell}}}$, we
can express the convolutional weights $\layerW{\ell}$ which have covariance
$\priorWcov{\ell} = \mR\ssup{\ell} \bra{\mR\ssup{\ell}}^{\tp}$ as
\begin{equation}
  \layerW{\ell}_{:,:,\patch} = \sum_{\vs=\1}^{\patchsize{\ell}}
  R\ssup{\ell}_{\patch,\vs}\layerU{\ell}_{:,:,\vs}\,.
\end{equation}

Substituting this into \cref{eq:conv-as-matmul} we obtain
\begin{equation}
\layera{\ell}{:,\nextpatch} = \sum_{\patch=\1}^{\patchsize{\ell}} \sum_{\vs=\1}^{\patchsize{\ell}}R\ssup{\ell}_{\patch,\vs}\layerU{\ell}_{:,:,\vs} \; \layernla{\ell-1}{:,\patchf{\nextpatch}{\patch}}.
\label{eq:correlated-conv-as-matmul}
\end{equation}
To express this computation with \Netsor rules we may write
\begin{align}
  \texttt{MatMul: }& \mH\ssup{\ell}_{:,\vs,\patch}\bra{\mX} \eqdef \layerU{\ell}_{:,:,\vs}\layernla{\ell-1}{:,\patch} \nonumber\\
  &\text{for }\vs\in \countto{\patchsize{\ell}},\patch \in \countto{\layersize{\ell-1}}, \label{eq:cnn-corr-matmul} \\
  \texttt{LinComb: }& \layera{\ell}{:,\nextpatch} \eqdef +
\sum_{\patch=\1}^{\patchsize{\ell}} \sum_{\vs=\1}^{\patchsize{\ell}}
R\ssup{\ell}_{\patch,\vs}\;
\mH\ssup{\ell}_{:,\vs,\patchf{\nextpatch}{\patch}}\bra{\mX}  \nonumber\\
  &\text{for }\nextpatch\in \countto{\layersize{\ell}}.
\label{eq:cnn-corr-lincomb}
\end{align}
\Cref{alg:netsor-cnn} uses this construction for every layer to express an
$L$-layer \ac{CNN} with correlated weights, applied to an input data set
$\gX\eqdef\sqb{\mX_{1},\dots,\mX_{M}}$.

\begin{algorithm}[tp]
\SetKwInOut{Input}{Input}
\SetKwInOut{Output}{Output}
% \SetKwInOut{MatMul}{MatMul}
% \SetKwInOut{LinComb}{LinComb}
% \SetKwInOut{Nonlin}{Nonlin}
  \caption{\Netsor description of an $L$-layer CNN with
    correlated weights, with input $\gX$.}\label{alg:netsor-cnn}
  \CommentC{$\Gva$-vars for layer 1 activations, for all spatial locations
     $\patch$ and input points $\mX_m$.}
  \SInput{$\layerAsm{1}{\patch}{\mX_m} : \Gva(\layerC{1})$ \newline for $\patch \in
  \countto{\layersize{1}}$ and $m =1,\dots,M.$}
  \CommentC{$\Ava$-vars for the independent convolutional weights}
  \SInput{$\layerU{\ell}_{\patch} : \Ava\bra{\layerC{\ell}, \layerC{\ell-1}}$\newline
      for $\patch \in [\patchsize{\ell}]$ and $\ell=2,\dots,L-1$.}
  \CommentC{$\Ova$-vars for the output, for every patch location $\vs$ and channel $\chan$}
  \SInput{$\vo_{\chan,\vs} : \Ova\bra{\layerC{L-1}}$ \newline for $\vs \in \countto{\patchsize{L}}$ and $\chan =1,\dots,\layerC{L}.$}
  \vspace{1ex}

  \For{$m = 1, \dots, M$ (data points of $m$)}{
  \For{$\ell = 2,\dots,L-1$ (layer $\ell$)}{
  \For{$\patch = \1,\dots,\layersize{\ell-1}$}{
  \SNonlin{$\Hva\bra{\layerC{\ell-1}}$ \newline $\layernlasm{\ell-1}{:,\patch}{\mX_{m}} \eqdef \phi\bra{\layeram{\ell-1}{:,\patch}{\mX_{m}}}$}

  \For{$\vs = \1,\dots,\patchsize{\ell}$ (patch location $\vs$)}{
  \SMatMul{$\Gva\bra{\layerC{\ell}}$ \newline $\mH\ssup{\ell}_{:,\vs,\patch}\bra{\mX_{m}} \eqdef \layerU{\ell}_{\vs}\layernlasm{\ell-1}{\patch}{\mX_{m}}$}
  } }
  \For{$\nextpatch=\1,\dots,\layersize{\ell}$ (spatial location $\nextpatch$)}{
  \SLinComb{$ \Gva\bra{\layerC{\ell}}: \layeram{\ell}{:,\nextpatch}{\mX_{m}}$ \newline $\eqdef
\sum_{\patch=\1}^{\patchsize{\ell}} \sum_{\vs=\1}^{\patchsize{\ell}}
R\ssup{\ell}_{\patch,\vs}\;
\mH\ssup{\ell}_{:,\vs,\patchf{\nextpatch}{\patch}}\bra{\mX_{m}} $}
  } }
  \vspace{1ex}
  \For{$\patch \in \sqb{\layersize{L-1}}$ (spatial location $\patch$)}{
  \SNonlin{$ \Hva\bra{\layerC{L-1}}$ \newline $\layernlasm{L-1}{:,\patch}{\mX_{m}} \eqdef \phi\bra{\layeram{L-1}{:,\patch}{\mX_{m}}}$}
  }
  }

  \CommentC{One output for every spatial location $\patch$, patch location $\vs$, channel $i$ and data point $m$.}
  \SOutput{$\big(\vo_{\chan,\vs}^\tp \layernlasm{L-1}{:,\patch}{\vX_m} : \text{ for
    }\patch \in \countto{\layersize{L}}, \vs\in\countto{\patchsize{L}},$\newline
    $\chan \in \countto{\layerC{L}}\text{ and }m \in \countto{M}\big)$}
  \vspace{1ex}
  {\textbf{Output postprocessing: } correlate the outputs and add biases (not part of \Netsor)}

  \For{$m \in \countto{M}$, $\chan \in \countto{\layerC{L}}$ and $\nextpatch \in \layersize{L}$}{
  {$\layerAsm{L}{\chan,\nextpatch}{\vX_m} \eqdef
      \sum_{\patch=\1}^{\patchsize{L}} \sum_{\vs=\1}^{\patchsize{L}} R\ssup{L}_{\patch,\vs} \;
      \bra{\vo_{\chan,\vs}^\tp \layernlasm{L-1}{:,\patchf{\nextpatch}{\patch}}{\vX_m} }$}
    }
\end{algorithm}

\begin{theorem}[Correlated CNN behaves like a GP]
  Consider a countable set of input points $\tilde{\gX}$, and a fixed number of
  layers $L$. Apply the $L$-layer convolutional neural network \crefp{eq:deep-recursion,eq:deep-activations}
  with correlated weights \crefp{eq:correlated-weight-prior} to $\gX$. Assume its
  nonlinearities are controlled \crefp{asm:controlled}.
  For simplicity, fix all layers to have
  the same number of channels: $C = \layerC{1} = \dots = \layerC{L}$. Then, as the number of channels $C \to \infty$, the activations $\layerAm{L}{}{\tilde\gX}$ converge in distribution to a Gaussian process with mean function $\E\sqb{\layerAsm{L}{\chan}{\tilde\gX}} = \meanf{L}\bra{\tilde{\gX}}$ and covariance function $\Cov\sqb{\layerAsm{L}{\chan,\nextpatch}{\tilde\gX}, \layerAsm{L}{\chan',\nextpatch'}{\tilde{\gX}}} =
  \delta_{\chan,\chan'}\covf{L}_{\nextpatch,\nextpatch'}\bra{\tilde\gX,\tilde\gX}$ \crefp{sec:deep-correlations}.
\label{thm:correlated-cnn-gp}
\end{theorem}
\begin{proof}
  We need to show
  \begin{enumerate}
    \item that \cref{alg:netsor-cnn}, including the postprocessing part, implements a correlated-weight CNN correctly,
    \item that the full program converges weakly to a \ac{GP} on $\tilde\gX$,
    \item that the moments of this \ac{GP} match the ones in \cref{sec:deep-correlations}.
  \end{enumerate}

  For the first claim, the key is the equivalence between a convolutional layer
  with correlated weights \eqparref{eq:deep-recursion}, and a spatial outer
  product followed by linear combination
  \eqparreftwo{eq:cnn-corr-matmul}{eq:cnn-corr-lincomb}. Keeping this in mind, we
  can verify by inspection that the steps of \cref{alg:netsor-cnn}, including
  the output postprocessing, implement the recursive CNN equations
  \crefp{eq:deep-recursion,eq:deep-activations}.

  The second claim is somewhat more involved. Invoking the Kolmogorov extension
  theorem \citep[Thm.~2.4.3]{measure-theory-intro} we restrict our attention to finite
  subsets $\gX \subseteq \tilde\gX$, which are going to be compatible
  distributions if claim~3 is true. Since $\mX \in \gX$ are tensors, we may use the Euclidean metric.
  We then show by \cref{thm:netsor-master} that the output tuple of the CNN
  \Netsor program converges weakly to a GP as $C\to\infty$. Each activation in
  the postprocessing is defined as a linear combination of a Gaussian \ac{RV}
  (the bias) and a \ac{RV} that converges weakly to a Gaussian, and thus the resulting distribution on $\gX$ converges weakly to a Gaussian too.

  Finally, we have to show that the postprocessed output has the correct kernel.
  The output tuple and the activations have mean zero, which is correct. We thus compute the covariance of the output tuple in \cref{alg:netsor-cnn}, for $\mX,\mX' \in \tilde\gX$:
  \begin{multline}
    \Cov\sqb{
      \vo_{\chan,\vs}^\tp \layernlasm{L-1}{:,\patch}{\mX},
      \vo_{\chan',\vs'}^\tp \layernlasm{L-1}{:,\patch'}{\mX'}
    } \\= \delta_{\chan,\chan'} \delta_{\vs,\vs'} \nlinf{L-1}_{\patch,\patch'}\bra{\mX,\mX'}.
  \end{multline}
  The delta functions appear because the $\Ova$-vars and their elements are all independent. Using this, we can calculate the covariance function of the activations
  \begin{multline}
\Cov\sqb{
  \layerAsm{L}{\chan,\nextpatch}{\vX}, \layerAsm{L}{\chan',\nextpatch'}{\vX'}}
= \sum_{\patch,\patch'}^{\patchsize{L}^{2}}\sum_{\vs,\vs'}^{\patchsize{L}^{2}}\\
R\ssup{L}_{\patch,\vs} R\ssup{L}_{\patch',\vs'}
\Cov\sqb{
      \vo_{\chan,\vs}^\tp \layernlasm{L-1}{:,\patchf{\nextpatch}{\patch}}{\mX},
      \vo_{\chan',\vs'}^\tp \layernlasm{L-1}{:,\patchf{\nextpatch'}{\patch'}}{\mX'}
    }.
\end{multline}
Substitute the value of the expectations and eliminate one of the sums due to $\delta_{\vs,\vs'}$,
\begin{align}
&\Cov\sqb{
  \layerAsm{L}{\chan,\nextpatch}{\vX}, \layerAsm{L}{\chan',\nextpatch'}{\vX'}} \nonumber\\
&=\delta_{\chan,\chan'} \big(\sum_{\patch,\patch'}^{\patchsize{L}^{2}}\sum_{\vs=\1}^{\patchsize{L}}
   R\ssup{L}_{\patch,\vs} R\ssup{L}_{\patch',\vs} \nlinf{L-1}_{\patchf{\nextpatch}{\patch},\patchf{\nextpatch'}{\patch'}}\bra{\mX,\mX'} \big).
\intertext{
Finally, noting that $\priorWcov{L}_{\patch,\patch'} = \sum_{\vs=\1}^{\patchsize{L}} R\ssup{L}_{\patch,\vs} R\ssup{L}_{\patch',\vs} $ \crefp{lemma:R}, and using the definition of $\covf{L}_{\nextpatch,\nextpatch'}\bra{\mX,\mX'}$ \eqparref{eq:cnn-kernel-recursive}, we obtain the claim.
}
  &= \delta_{\chan,\chan'} \big(
\sum_{\patch,\patch'}^{\patchsize{L}^{2}} \priorWcov{L}_{\patch,\patch'} \nlinf{L-1}_{\patchf{\nextpatch}{\patch},\patchf{\nextpatch'}{\patch'}}\bra{\mX,\mX'}
\big)  \nonumber\\
&= \delta_{\chan,\chan'} \covf{L}_{\nextpatch,\nextpatch'}\bra{\mX,\mX'}.
\end{align}
\end{proof}

\bibliography{garriga-alonso_752}
\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
